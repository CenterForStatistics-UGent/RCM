---
title: "RC2NB"
author: "Stijn"
date: "June 13, 2016"
output: pdf_document
---

```{r check-install-load-packages, warning=FALSE, message=FALSE, echo=FALSE}
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE,  warning=FALSE, message=FALSE, echo=FALSE, eval=FALSE, tidy = TRUE)
setwd("/home/stijn/PhD/Biplots")
# The required package list:
reqpkg <- c("phyloseq","MASS", "parallel","nleqslv", "edgeR")
# Load all required packages and show version
for(i in reqpkg)
{
#   print(i) 
#   print(packageVersion(i))
  library(i, quietly=TRUE, verbose=FALSE, warn.conflicts=FALSE, character.only=TRUE)
}
#options(digits=4)
#rm(list=ls())
#nCores <- 4
```

#Log-linear model

Suppose we have a $nxp$ data matrix $\mathbf{X}$

## Reconsitution formula of Correspondence Analysis (CA)

One approach is to use log-linear modelling an thereby introduce the negative binomial as error term.

Under independence we model the count as 

$$E(X_{ij}) = a_i b_j$$

whereby usually $a_i=x_{i.}=\sum_{j=1}^px_{ij}$ and $b_j=\frac{x_{.j}}{x_{..}}=\frac{\sum_{i=1}^nx_{ij}}{\sum_{j=1}^p\sum_{i=1}^nx_{ij}}$.

A more extended model is

$$E(X_{ij}) = a_i  b_j + c_i d_j \sum_{m=1}^k \omega_m v_{mi} w_{jm}$$.

For $a_i = c_i = x_{i.}$ and $b_j = d_j = \frac{x_{.j}}{x_{..}}$ this is regular correspondence analysis, usually with truncation at $k^*=2$. 

$$E(X_{ij}) = \frac{x_{i.}x_{.j}}{x_{..}} \big(1 + \sum_{m=1}^k \omega_m v_{mi} w_{jm}\big)$$.

This is called the *reconstitution formula* since it decomposes the observed count into its expectation and the residual and the residual further into $k$ pieces. In matrix notation this becomes

$$X = E_{independence} + RU\Sigma VC$$.

## Log-linear analysis

In log-linear analysis the logged count $l_{ij}$ is modelled as

$$log(E(x_{ij})) = l_{ij} = u + u_i + u_j + u_{ij}$$

## Relationship with CA

Accorinding to Escoufier, 1985 if $x =\sum_{m=1}^k \omega_m v_{mi} w_{jm}$ is small (i.e. the deviation from independence is small) then $log(1+x) \approx x$ and

$$l_{ij} \approx u + u_i + u_j + \sum_{m=1}^k \psi_m r_{mi} s_{jm}$$

with $u=-log(x_{..})$, $u_i=log(x_{i.})$, $u_j=log(x_{.j})$ and $\psi_m \approx \omega_m$. However, the assumption that the deviation from independence is small may not be valid for our purpose.

## The RC(2)-association model

For the purpose of making biplots, we might consider the following model (proposed by Leo Goodman, 1985. See also VanDerHeijden 1994)

$$log(E(x_{ij})) = l_{ij} = u + u_i + u_j + \psi_1r_{1i}s_{1j} + \psi_2r_{2i}s_{2j}$$.

Constraints are needed to render this model identifiable, Goodman proposes

$$\sum_{i=1}^nx_{i.}r_{ki} = 0$$

with k=1,2 and

$$\sum_{i=1}^nx_{i.}r_{ki}r_{k'i} = I(k=k')$$

$$\sum_{j=1}^px_{.j}s_{kj} = 0$$

$$\sum_{j=1}^px_{.j}s_{kj}s_{k'j} = I(k=k')$$

The first and third restriction have to be applied with every iteration, the second and fourth normalizations can occur afterwards (Goodman 1985, appendix). However, this may put undue emphasis on samples with large library sizes , so we use

$$\sum_{i=1}^nr_{ki} = 0$$

with k=1,2 and

$$\sum_{i=1}^nr_{ki}r_{k'i} = I(k=k')$$

$$\sum_{j=1}^ps_{kj} \frac{x_{.j}}{x_{..}} = 0$$

$$\sum_{j=1}^ps_{kj}s_{k'j} \frac{x_{.j}}{x_{..}} = I(k=k')$$

The models are usually fitted by ML. See page 55 of Goodman 1985 for details. Initial values can be obtained from the singular value decomposition of the saturated model (CA). Goodman assumed a Poisson distribution, but it might be possible to use a negative binomial model with unique dispersions for each taxon $j$ as well.

We will use $u=-log(x_{..}$, $u_i = log(x_{i.})$ and $u_j = log(x{.j})$, and then we'll have to iterate between fitting the NB and estimating the imporance parameters $\psi$, the $r's$ and the $s's$. This is implemented in the _VGAM_ package but the fitting method does not converge and crashes.

The orthogonality of the row and column scores is enforced through Lagrange multipliers. This makes the score equations much harder to solve but assures independence of the dimensions.

The centering and scaling of the scores is done afterwards, we may enforce this also through the Lagrangian but this complicates the fitting process even further and it doesn't hinder the convergence anyway. The equations are weighted such that all score equations together get the same weight as one orthogonality constraint.

### Fitting algorithm for the RC(2) association model with a NB error structure

See Goodman 1985 (appendix), Becker 1988 for information on estimation. The code below is generic but we take k=2 for simplicity.

Parameters of interest to estimate are $u_j$,$\psi$, $r_1$, $r_2$, $c_1$, $c_2$. In the process we'll also need the nuisance parameters $\mathbf{\theta}$. Assume $E(X_{ij}) = \mu_{ij}$ and  $Var(X_{ij})=\mu_{ij} + \frac{\mu_{ij}^2}{\theta}$

1. Do a regular correspondence analysis (based on the Pearson resdiuals) and obtain the $R^{-1/2}(X-E)C^{-1/2} = U\Sigma V$, the singular value decomposition. This gives us initial values $[r_{11}^{init}, r_{12}^{init},..., r_{1i}^{init}] = U_{(1.)}$, the first row of $U$, and correspondingly $\Sigma$ and $V$ give initial values for $\mathbf{\psi}$ and $\mathbf{s}$.

2. Fit a NB for every taxon (column) $j$ with 

$$log(E(x_{ij})) = l_{ij} = offset\big( log(\frac{x_{i.}x_{.j}}{x_{..}}  \big) + \psi_1 (s_{1i}^{init}r_{1j}^{init}) + \psi_2 (s_{2i}^{init}r_{2j}^{init})$$

to obtain estimates for the $\psi$'s and the overdispersions $\theta_j$. This is a bit tricky, since all taxa share the same $\psi$ parameters but not the same dispersion, which is a nuisance parameter. As a result we cannot use the regular glm.nb() function from the _MASS_ package but have to write a new algorithm iterate between

 - Mean estimation: $\psi$'s
 - Dispersion estimation: $\theta_j$'s
 
 I will later refer to this as the **inner iteration**.
 
  i. Mean estimation
 
 Solve the system of score equations (see Lawless 1987) for $\psi_1$ and $\psi_2$, assuming $\theta_j$'s given
 
 $$\sum_{i=1}^n \sum_{j=1}^p r_{im}s_{mj} \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}} = 0$$ for all m=1,...,k
 
 These are non-linear equations for which we use the _nleqslv_ package, with its default the Broyden method.
 
  ii. Dispersion estimation
  
Solve the score equations for every taxon $j$, assuming the means $\mu_{ij}$ are known

 $$\sum_{i=1}^n \sum^{y_i^*}_{t=0} \big( \frac{t}{1+\frac{t}{\theta_j} } \big) + \theta_j^2 log(1+\frac{\mu_{ij}}{\theta_j})- \mu_{ij}\frac{y_{ij}+\theta_j}{1+\frac{\mu_{ij}}{\theta_j} }$$
 
 For this we use the theta.ml() function in the _MASS_ package
 

3. To optimize the $r_{i}$'s we would really like to use weighted least squares to minimize

$$\sum_{j=1}^p\Big(log(x_{ij}) - \big(u + u_i + u_j +\sum_{m=1}^2 r_{im} \big( \hat{\psi_ms_{jm}} \big)\Big)^2$$

for every $i$. Unfortunately we cannot calculate $log(x_{ij})$ because of the zeroes.

We'll have to find another way to estimate them, e.g. maximum likelihood or weighted maximum likelihood.

The weights will then be inversely proportional to

$$Var(x_{ij}) = \mu_{ij} + \mu_{ij}^2/\theta_j$$.

with 

$$\mu_{ij} = exp\big(log(\frac{x_{i.}x{.j}}{x_{..}}) + \psi_1^{MLE} s_{1j}^{init}r_{1i}^{init} + \psi_2^{MLE} s_{2j}^{init}r_{2i}^{init} \big)$$.

Also for $k>1$ we need to assure that 

$$\sum_{i=1}^n r_{ki}r_{k'i}=0$$

for $k \neq k'$ (the orthogonality constraint). To enforce this we use Lagrange multipliers and thus look for the maximum of the Lagrangian function

$$Lag(\mathbf{r}, \lambda) = L_{NB}(\mathbf{r})  +  \lambda \sum_{i=1}^n r_{1i}r_{2i} $$

with $L_{NB}(\mathbf{r})$ the log-lieklihood function of the negative binomial regression. The derivatives of this function are

$$\frac{\delta Lag(\mathbf{r}, \lambda)}{\delta r_{ik}} = \sum_{j=1}^p \hat{s_{jk}} \hat{\psi_k} \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}} +  \sum_{k' \neq k} r_{ki} \lambda_{kk'} = \mathbf{0}$$

and 

$$\frac{\delta Lag(\mathbf{r}, \lambda)}{\delta \lambda_{kk'}} = \sum_{i=1}^n r_{1i}r_{2i} = \mathbf{0}$$

(the original constraint).

There is an extra equation now ($\sum_{i=1}^n r_{ki}r_{k'i}=0$) but also the extra $\lambda$ parameter to optimize. With the numerical procedure in the back of my head I prefer a weighted derivative of the  Lagrangian to find the approximate solutions of

$$(\sum_{j=1}^p \hat{s_{jk}} \hat{\psi_k} \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}} +  \sum_{k' \neq k} r_{ki} \lambda_{kk'})*\frac{n+k*(k-1)/2}{n(1+k*(k-1)/2)} = 0$$

and 

$$\frac{n+k*(k-1)/2}{1+k*(k-1)/2} * \sum_{i=1}^n r_{1i}r_{2i} = 0$$

Roughly speaking this means that a deviation from 0 for the orthogonality constraint is weighted $n$ times more than a deviation in a single score equation. I do this because I think the orthogonality will be important for the biplot. Note that this is only a numerical issue and we're still looking for true MLEs, i.e. for which the score equations are zero. Note also that the sum of weights is still the total number of equations.

We now truly have a system of equations to solve, which we do with the _nleqslv_ package. It is straightforward but a bit cumbersome to also provide the analytical Jacobian, which greatly speeds up the iterations (otherwise it has to be approximated numerically). The Jacobian is sparse

TO BE ADDED: analytical description of the Jacobian

After solving the system (or reaching the quorum of iteration runs) the estimates for $\mathbf{r_{i}} = (r_{1i},r_{2i})$ are then normalized to fulfil

$$\sum_{i=1}^n r_{ki} = 0$$

by setting 

$$r_{ki}^{new} = r_{ki} - \sum_{i=1}^n r_{ki}/n$$

and to fulfil

$$\sum_{i=1}^n r_{ki}r_{k'i} = I(k=k')$$

by setting

$$r_{ki}^{final} = \big(\frac{r_{ki}}{\sum_{i=1}^nr_{ki}^2}\big)^{1/2}$$.

4. Repeat step 2 with updated $\mathbf{r_i}$ and obtain new $\psi^{MLE}$'s and $theta_j^{MLE}$'s

5. Repeat step 3 but now estimate $\mathbf{s_j}$

This is very similar as the row score estimation except that now the overdispersions are constant for every row score and we use weights such that e.g.

$$\sum_{j=1}^p\frac{x_{.j}}{x_{..}}s_{kj}s_{k'j} = I(k=k')$$

6. Repeat step 2 with updated $\mathbf{s_j}$ and obtain new $\psi^{MLE}$'s and $theta_j^{MLE}$'s

Steps 4-6 are referred to as the **outer iteration**.

7. Repeat until convergence

We use a relative convergence tolerance for the $\psi$'s, the $r$'s and the $s$'s. We don't look at the overdispersion for convergence, since it is only a nuisance parameter and its estimator is very variable.

8. Plot $\psi_1r_{1i}$ vs $\psi_2r_{2i}$ and add $s_{1j}*$ vs $s_{2j}$ to make a biplot.

In the end we'll have estimated p (abundances) + p (dispersions) + kxp (column scores) + kxn (row scores) + n (library sizes) + k (importance parameters) = (2+k)p + (1+k)n + k parameters out of np entries. We have imposed 4*k + k(k-1) restrictions, so the final model is still very parsimonious for n and p in the hundreds.

Can we get a measure of how much deviation from indepencence we've explained as in CA? Perhaps compare with saturated and indendence models and look at $G^2$?


# Implementation

```{r Auxfuns}
#All matrices X are considered to be nxp, i.e. samples are rows and taxa are columns

## A function to perform the initial SVD

initSVD = function(X){
  
# @param X: the nxp count matrix
  
# @return: the singular value decomposition of the weigthed matrix of residuals
  
  C = colSums(X)
  R = rowSums(X)
  onesn =rep(1, nrow(X))
  onesp = rep(1, ncol(X))
  E = diag(R) %*% outer(onesn ,onesp) %*% diag(C)/sum(C)
  Goal = diag(1/sqrt(R)) %*% (X-E) %*% diag(1/sqrt(C))
  dimnames(Goal) = dimnames(X)
  svd(Goal)
}
#-------------------------------------------------#

#A wrapper function for psi estimation

glm.nb.mat = function (X, reg, init.theta, psiInit, abunds,  libSizes, global = "dbldog", nleqslv.control,k=k){
  # @param X: the nxp data matrix
  # @param reg: a nxpxk regressor matrix
  # @param init.theta: a vector of length p with the current dispersion parameters
  # @param psiInit: a vector of length k with the initial psi parameters
  # @param abunds: a vector of length p with abundance parameters
  # @param libSizes: a vector of length n with (known) library sizes

  # @return Psis: estimated importance estimates, sorted from large to small and positive
  
require(nleqslv)
k = length(psiInit)
# psiVec = psiInit
# Psi parameters are estimated one by one, given the previous ones
# for (K in 1:k){
# nleqslv.control$trace=FALSE
psiVec = try(nleqslv(fn = dNBll, x = psiInit, theta = init.theta, y = X, reg = reg, 
                 abunds =abunds, libSizes = libSizes, k=k, global=global, control = nleqslv.control, jac=NBjacobianPsi)$x, silent=TRUE)
#, psiVec=psiVec
if(class(psiVec)=="try-error"){
  psiVec = psiInit
  warning("Could not update psi values")
}

return(Psis = sort(abs(psiVec), decreasing=TRUE)) #enforce positive psis and sort
}
  
#--------------------------------------#

dNBll = function(beta, y, reg, theta, abunds, k, libSizes){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param logAbund: a vector of length p with the LOGS OF THE abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
# 
#   logMu = sapply(1:k,simplify="array", function(K){
#     reg[,,K]*beta[K]
#   })
  mu = exp(arrayprod(reg, beta)) * outer(libSizes, abunds)
  
  vapply(1:k,FUN.VALUE=0, function(z){
  sum(reg[,,z]*(y-mu)/(1+t(t(mu)/theta)))
  })

}
#--------------------------------------#
#A jacobian for the psi parameters
NBjacobianPsi = function(beta, y, reg, theta, abunds, libSizes, k){
  
  mu = exp(arrayprod(reg, beta)) * outer(libSizes, abunds)
  sapply(1:k, function(K){
    vapply(1:k,FUN.VALUE=0, function(Kinner){
    -sum(reg[,,K]*reg[,,Kinner]*(1+t(t(y)/theta))*mu/(1+t(t(mu)/theta))^2)
  })})
}

#--------------------------------------#
dNBll2 = function(beta, y, reg, theta, abunds, K, libSizes, psiVec)  {
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param abunds: a vector of length p with the abundances
  # @param K: The dimension of the RC solution
  # @param psiVec: A vector of length r with the old psi estimates
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  logMu = array(0,dim=c(dim(reg)[1:2], K+1))
  for (i in 1:(K-1)){
    logMu[,,i] = reg[,,i]*psiVec[i]
  }
  logMu[,,K] = reg[,,K]*beta
  logMu[,,K+1] = log(outer(libSizes, abunds)) #log(Libsize * abundance)
  mu = exp(apply(logMu,c(1,2),sum))
  
  sum(reg[,,K]*(y-mu)/(1+t(t(mu)/theta)))
}

#---------------------------------------#
# A function to estimate the overdispersions
estDisp = function(X, cMat, rMat, libSizes, abunds, psis, prior.df=10, k=k){
# @param X: the nxp named count matrix
# @param rMat: A nxk matrix with the row weights of every dimension in the columns 
# @param cMat: A kxp matrix with the column weights of every dimension in the rows 
# @param libSizes: a vector of length n with library sizes
# @param abunds: a vector of length p with the abundances
# @param psis: A vector of length k with importance estimates
# @param prior.df (optional): prior degrees of freedom for dispersion estimation, see edgeR documentation

  require(edgeR)
    # A matrix of means
  logMeansMat = t(rMat %*% (cMat*psis) + log(outer(libSizes, abunds)))
#Use the edgeR machinery to estimate the dispersions
  trended.dispersion  <- estimateGLMTrendedDisp(y = t(X), design = NULL, method = "bin.loess",offset=logMeansMat)
  thetaEsts <- 1/estimateGLMTagwiseDisp(y = t(X), design = NULL, prior.df = prior.df, offset=logMeansMat, dispersion = trended.dispersion)
  if(anyNA(thetaEsts)){
    idNA = is.na(thetaEsts)
    thetaEsts[idNA] = mean(thetaEsts[!idNA])
    warning(paste(sum(idNA), "dispersion estimations did not converge!"))
  }
  return(thetas=thetaEsts)
}
#---------------------------------------#

#A function to estimate the psis given all the other parameters
estPsis = function(X, rMat, cMat, psiInit, abunds, libSizes, thetas, nleqslv.control,k,...){
    
# @param X: the nxp named count matrix
# @param rMat: A nxk matrix with the row weights of every dimension in the columns 
# @param cMat: A kxp matrix with the column weights of every dimension in the rows 
# @param psiInit: A vector of length k with initial importance estimates
# @param thetas: A vector of length k with overdispersions
# @param abunds: a vector of length p with the abundances
# @param libSizes: a vector of length n with (known) library sizes
# @param nleqslv.control: a list with control options for the nleqslv function of the same package

# @return psis: A vector of length k with the "importance" parameters of the RC decompositions

  ## Step 3 estimate the dimension scores psi by ML, assuming overdispersions known
#   reg = array(0, dim = c(dim(X),k))
#   for (i in 1:k){
#     reg[,,i] = outer(rMat[,i], cMat[i,])
#   }
  
  reg = sapply(1:k, simplify="array", function(K){
    outer(rMat[,K], cMat[K,])
  })
  return(glm.nb.mat(X = X, reg = reg, init.theta = thetas, psiInit = psiInit,
                       abunds = abunds, libSizes = libSizes, nleqslv.control = nleqslv.control,k=k))

}
#-------------------------------------------#
#An auxiliary R function, kindly provided by Joris
arrayprod <- function(x,y){
  xdim <- dim(x)
  outdim <- xdim[1:2]
  outn <- prod(outdim)
 
  yexpand <- rep(y, each = outn)
  outid <- seq_len(outn)
 
  tmp <- x * yexpand
 
  dim(tmp) <- c(outn, xdim[3])
  out <- rowSums(tmp)
 
  dim(out) <- outdim
 
  out
}

#-------------------------------------------#
## A function to calculate row or column scores when keeping the other one constant, given overdispersion and importance parameters psi.

scoreCalc = function(X, psis, thetas, abunds, toEstimate = c("rows","columns"), rMat, cMat, lambda, method,  nleqslv.control, libSizes = NULL, global="dbldog",  k, n ,p){
  
# @param X: the nxp data matrix
# @param psis: A vector of length k with the importance parameters psi
# @param thetas: a vector of length p with dispersion estimates
# @param abunds: a vector of length p with relative abundances
# @param toEstimate: a character string, either "rows" or "columns", indicating which scores to estimate
# @param rMat: a nxk matrix with current row scores
# @param cMat: a kxp with matrix with current column scores
# @param libSizes(optional): a vector of length n with estimates for the library sizes
# @param lambda: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers
# @param method: Method for jacobian estimation , see nleqslv
# @param global(optional): global strategy for solving non-linear systems , see nleqslv
# @param nleqslv.control: a list with control options, see nleqslv

# @return rMat: a nxk matrix with row scores: The same ones as provided or new estimates
# @return cMat: a pxk with matrix with column scores: The same ones as provided or new estimates
# @return converged: a boolean indicating if the roots of the system were found
  
   if(length(psis) != NCOL(rMat) || length(psis) != NROW(cMat)){
    stop("Dimensions of psis, rows or columns don't match")
  } else{
    }
  
  if(is.null(libSizes)){
    libSizes = rowSums(X)
  }
  if(toEstimate == "columns"){

    cMatNewList = glm.nb.col(X = X, reg = t(psis*t(rMat)), current.theta = thetas, abunds = abunds, libSizes = libSizes, cMatInit = cMat, global = global, nleqslv.control = nleqslv.control, k=k, lambdaCol = lambda, method=method, n=n, p=p)

    cMatNew = cMatNewList$cMat
    # cMatNew=cMatNew/cMatSE #Bad idea!
#       #Renormalize
  colScoresNorm = t(apply(cMatNew,1, function(colS){
    colS - sum(colS * abunds)/sum(abunds) #mean(colS)
  }))
  colScoresNorm = t(apply(colScoresNorm, 1, function(y){
    y/sqrt(sum(abunds * y^2)/sum(abunds))
  }))

  return(list(rMat=rMat, cMat=colScoresNorm, lambdaCol = cMatNewList$lambdaCol, converged  = cMatNewList$converged))
  } else{

    rMatNewList = glm.nb.row(X = X, reg = psis*cMat, current.theta = thetas, abunds = abunds, libSizes = libSizes, global =global, nleqslv.control = nleqslv.control, rMatInit = rMat, lambdaRow=lambda, method=method,  n=n, p=p, k=k)
    
  rMatNew = rMatNewList$rMat
  #Renormalize
  rowScoresNorm = apply(rMatNew,2, function(rowS){
    rowS - mean(rowS)#
  })
  rowScoresNorm = apply(rowScoresNorm, 2, function(y){
    y/sqrt(sum( y^2))
  })
   
  # rownames(rowScoresNorm) = rownames(X)
  return(list(rMat=rowScoresNorm, cMat=cMat, lambdaRow = rMatNewList$lambdaRow, converged = rMatNewList$converged))
    }
  }

#-------------------------------------------#
#A function to calculate the column scores for one column through weighted ML
  
glm.nb.col = function (X, reg, current.theta, abunds, libSizes, cMatInit, nleqslv.control, k , p,n,lambdaCol, global = "dbldog", method){
  # @param X: the data vector of length n
  # @param reg: a nxpxk regressor matrix: outer product of rowScores and psis
  # @param current.theta: a scalar,  the current dispersion parameters
  # @param abunds: a scalar with abundance parameter
  # @param libSizes : a vector of length n with library sizes
  # @param cVecInit: a vector of length k with initial column scores
  # @param k: a scalar, the dimension of the RC solution
# @param lambdaCol: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers
# @param method: Method for jacobian estimation , see nleqslv
# @param global(optional): global strategy for solving non-linear systems , see nleqslv
# @param nleqslv.control: a list with control options, see nleqslv

  # @return cMat: a vector length k with estimated column scores
  # @return lambdaCol: a vector of length 2*k+k*(k-1)/2 with estimates or the lagrange multipliers
  # @return converged: a boolean indicating if the roots of the system were found

require(nleqslv)
# cMat = cMatInit
beta = c(c(t(cMatInit)), lambdaCol)

  if(length(beta) != (k*p  + k*(k-1)/2 + 2*k)){ # + k + k
    stop("Dimensions of regressor matrix and parameters don't match")
  }
# nleqslv.control$trace=TRUE
keepGoing = TRUE
globalInd = methodInd = 1
while(keepGoing){
tmp = try(nleqslv(fn = dNBllcol, x = beta, current.theta = current.theta, y = X, reg = reg, abunds =abunds, libSizes = libSizes, k=k,  global = global[globalInd], control = nleqslv.control, n=n, p=p, jac = NBjacobianCol, method=method[methodInd]), silent=TRUE)
if(class(tmp)=="try-error" || tmp$termcd %in% c(2,3,4,5,6)){
 globalInd = globalInd + 1
 if(globalInd > length(global)){
   globalInd = 1
 nleqslv.control$maxit=200
    methodInd = methodInd +1
}
  if(methodInd > length(method)){
   keepGoing = FALSE
 }
 if(class(tmp)=="list" && tmp$termcd %in% c(2,3,4,5,6)){
  cMat = matrix(tmp$x[1:(k*p)], byrow=TRUE, ncol=p, nrow=k)
  beta = tmp$x
    }
} else if(tmp$termcd==1){
    keepGoing=FALSE
  }
}
if(class(tmp)=="try-error"){
  warning("Could not finish iteration of row values!")
} else{
  cMat = matrix(tmp$x[1:(k*p)], byrow=TRUE, ncol=p, nrow=k)
}

return(list(cMat = cMat, lambdaCol = tmp$x[(k*p+1):length(tmp$x)], converged = tmp$termcd ==1))
}
  
#--------------------------------------#
#A score function of the NB for the column scores

dNBllcol = function(beta, y, reg, current.theta, abunds, libSizes, k, p, n) {
  # @param beta: a vector of k*p column scores + k within dimensional centering lambda parameters + k within dimensional normalization parameters +  k(k-1)/2 between dimension lambda parameters
  # @param y: the data vector of length n
  # @param reg: a nxk regressor matrix: outer product of rowScores and psis
  # @param current.theta: a scalar,  the current dispersion parameters
  # @param logAbund: a scalar with the LOG OF THE abundance parameter
  # @param libSizes : a vector of length n with (known) library sizes
  # @param k: a scalar, the dimension of the RC solution
  # @param p: a scalar, the number of taxa
  # @param n: a scalar, the number of samples

  # @return A vector of length p*k+2*k+ k(k-1)/2 evaluations of the lagrangian
  
  cMat = matrix(beta[1:(k*p)], byrow=TRUE, ncol=p, nrow=k)
  # logMu[,K] = reg[,K]*beta
  mu = exp(reg %*% cMat) * outer(libSizes, abunds)
  
  lambda1 = beta[(k*p+1):(k*p+k)] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[(k*p+k+1):(k*p+2*k)] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  # lambda3 = beta[(k*p+1):length(beta)] #Orthogonality restrictions sum(abunds*r_{ik}*r_{ik'}) = 0. First all of dimension 1, then the remaining of dimension 2 etc. #2*k+
  lambda3Mat = matrix(0,ncol=k*(k-1)/2+1, nrow=k*(k-1)/2+1)
  lambda3Mat[lower.tri(lambda3Mat)] = beta[(k*(p+2)+1):length(beta)]
  lambda3Mat = lambda3Mat + t(lambda3Mat)
#   score = as.vector(sapply(1:k, function(K){
#     sapply(1:p, function(P){
#       sum(reg[,K]*(y[,P]-mu[,P])/(1+t(t(mu[,P])/current.theta[P])))  + 
#             sum(lambda3Mat[K,]*cMat[,P]*abunds[P]) + lambda1[K]*abunds[P] + lambda2[K]*abunds[P]*
#         if(LASSO) ifelse(cMat[K,P]>0,1,-1) else 2*cMat[K,P]*abunds[P]
#           })})) 
  
  score = c(t(
    crossprod(reg,((y-mu)/(1+t(t(mu)/current.theta)))) + 
                        t(abunds*t(lambda1 + lambda2*2*cMat + (lambda3Mat %*% cMat)))
    ))
  
  centers = colSums(abunds*t(cMat))
  unitSums = colSums(abunds*t(cMat^2))-1
  orthogons = sapply(1:(k-1), function(K){
    vapply((K+1):k,FUN.VALUE=0, function(Kinner){
      sum(cMat[K,]*cMat[Kinner,]*abunds)
    })
  })
  
  return(c(score,centers, unitSums, orthogons))
}

#-------------------------------------------#
#A function to calculate the row scores for one row through weighted ML
  
glm.nb.row = function (X, reg, current.theta, abunds, libSizes,  rMatInit, nleqslv.control, lambdaRow,k, n, p, global ="dbldog", method){
  # @param X: the data vector of length p
  # @param reg: a kxp regressor matrix: outer product of colScores and psis
  # @param current.theta: a vector of length p,  the current dispersion parameters
  # @param abunds: a vector of length p with abundance parameters
  # @param libSizes : a scalar, the library size
  # @param rVecInit: a vector of length k with initial row scores
  # @param lambdaRow: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers
# @param method: Method for jacobian estimation , see nleqslv
# @param global(optional): global strategy for solving non-linear systems , see nleqslv
# @param nleqslv.control: a list with control options, see nleqslv

  # @return rMat: a vector length k with estimated row scores
  # @return lambdaRow: a vector of length 2*k+k*(k-1)/2 with estimates or the lagrange multipliers
  # @return converged: a boolean indicating if the roots of the system were found
  
require(nleqslv)
# rMat = rMatInit
keepGoing = TRUE
globalInd = methodInd = 1
beta = c(c(rMatInit), lambdaRow)
  
if(length(beta) != (k*(n+2)  + k*(k-1)/2)){ #+ k + k
    stop("Dimensions of regressor matrix and parameters don't match")
}
while(keepGoing){
tmp = try(nleqslv(fn = dNBllrow, x = beta, current.theta = current.theta, y = X, reg = reg, abunds = abunds, libSizes = libSizes, k=k, global=global[globalInd], control= nleqslv.control, p=p, n=n, jac= NBjacobianRow, method=method[methodInd]), silent=TRUE)
if(class(tmp)=="try-error" || tmp$termcd %in% c(2,3,4,5,6)){
 globalInd = globalInd + 1
 if(globalInd > length(global)){
   globalInd = 1
  nleqslv.control$maxit=200
   methodInd = methodInd +1

}
  if(methodInd > length(method)){
   keepGoing = FALSE
 }
 if(class(tmp)=="list" && tmp$termcd %in% c(2,3,4,5,6)){
   rMat = matrix(tmp$x[1:(k*n)], byrow=FALSE, ncol = k, nrow=n) #
beta = tmp$x
    }
} else if(tmp$termcd==1){
    keepGoing=FALSE
  }
}
if(class(tmp)=="try-error"){
  warning("Could not finish iteration of row values!")
} else{
   rMat = matrix(tmp$x[1:(k*n)], byrow=FALSE, ncol = k, nrow=n) #
}
return(list(rMat = rMat, lambdaRow=tmp$x[(k*n+1):length(tmp$x)], converged = tmp$termcd ==1))
}
  
#--------------------------------------#
#A score function of the NB for the row scores

dNBllrow= function(beta, y, reg, current.theta, abunds, libSizes, k, n ,p) {
  # @param beta: a vector of k regression parameters to optimize: the k column scores
  # @param y: the data vector of length p
  # @param reg: a kxp regressor matrix: outer product of rowScores and psis
  # @param current.theta: a scalar,  the current dispersion parameters
  # @param abunds: a vector of length p with the abundances
  # @param libSizes : a vector of length n with (known) library sizes
  # @param k: a scalar, the dimension of the RC solution
  # @param p: a scalar, the number of taxa
  # @param n: a scalar, the number of samples

    # @return A vector of length p*k+2*k+ k(k-1)/2 evaluations of the lagrangian
  rMat = matrix(beta[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)
#   logMu = vapply(FUN.VALUE= array(0,dim=c(n,p)),1:k, function(i){
#     outer(rMat[,i],reg[i,])
#   })
  # logMu[,K] = reg[,K]*beta
  mu = exp(rMat %*% reg)* outer(libSizes, abunds)
  
  lambda1 = beta[(k*n+1):(k*n+k)] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[(k*(n+1)+1):(k*(n+2))] #normalization restrictions sum(abunds*r^2_{ik}) = 1
#Orthogonality restrictions sum(abunds*r_{ik}*r_{ik'}) = 0. First all of dimension 1, then the remaining of dimension 2 etc. #+2*k
  lambda3Mat = matrix(0,ncol=k*(k-1)/2+1, nrow=k*(k-1)/2+1)
  lambda3Mat[lower.tri(lambda3Mat)] = beta[(k*(n+2)+1):length(beta)]
  lambda3Mat = lambda3Mat + t(lambda3Mat)
#   score = as.vector(sapply(1:k, function(K){
#     sapply(1:n, function(N){
#       sum(reg[K,]*(y[N,]-mu[N,])/(1+t(t(mu[N,])/current.theta)))  + 
#        sum(lambda3Mat[K,]*rMat[N,])+ lambda1[K] + lambda2[K]*
#         (if (LASSO) ifelse(rMat[N,K]>0,1,-1) else 2*rMat[N,K])
#           })}))
  
  score = c(t(tcrossprod(reg, (y-mu)/(1+t(t(mu)/current.theta))) + lambda1 + lambda2* 2*t(rMat) + t(rMat %*% lambda3Mat)))
  #
  centers = colSums(rMat)
  unitSums = colSums(rMat^2)-1
  orthogons = sapply(1:(k-1), function(K){
    vapply((K+1):k,FUN.VALUE=0, function(Kinner){
      sum(rMat[,K]*rMat[,Kinner])
    })
  })

  return(c(score,centers, unitSums, orthogons))
}
#-------------------------------------------#
# A function to provide an analytical jacobian in the hope to speed up the calculations

#beta consists of 
#   - n*k parameters forming rMat
#   - k parameters forming lambda1, these are the Lagrange multipliers for the centering restrictions (row scores of one dimensions must sum to zero)
#   - k parameters forming lambda 2 these are the Lagrange multipliers for the normalization restrictions (squares of row scores of one dimensions must sum to one)
#   - k*(k-1)/2 parameters forming lambda3, the lagrange multiplier of the orthogonality restriction

NBjacobianRow = function(beta, y, reg, current.theta, abunds, libSizes, k, n ,p){
  #@return a symmetric jacobian matrix of size (n+2) *k + k(k-1)/2
  rMat = matrix(beta[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)
  nLambda = k*(k-1)/2+2*k #Number of lambda parameters
  lambda3Mat = matrix(0,ncol=k*(k-1)/2+1, nrow=k*(k-1)/2+1) #Reorder to lower triangular matrix
  lambda3Mat[lower.tri(lambda3Mat, diag=FALSE)] = beta[((n+2)*k+1):length(beta)]
  lambda3Mat = lambda3Mat + t(lambda3Mat)
  #lambdaWeights = (n+nLambda)/(1+nLambda) #Weigths of the restircting equations
  
#   #Calculate the mean
#   logMu = vapply(FUN.VALUE= array(0,dim=c(n,p)),1:k, function(i){
#     outer(rMat[,i],reg[i,])
#   })
    mu = exp(rMat %*% reg)* outer(libSizes, abunds)

  Jac = matrix(0, nrow= n*k + nLambda, ncol=n*k + nLambda)
  #The suymmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle
  
#   for (K in 1:k){
#       #dLag²/dr_{ik}dlambda_{1k}
#       # Jac[1:(n*k),(n*k+K)] = c(rep(0,(K-1)*n),rep(1,n),rep(0,(k-K)*n))
#       
#        #dLag²/dr_{ik}dlambda_{2k}
#       Jac[1:(n*k),((n+1)*k+K)] = if(LASSO) c(rep(0,(K-1)*n),ifelse(rMat[,K]>0,1,-1),rep(0,(k-K)*n)) else c(rep(0,(K-1)*n),2 *rMat[,K],rep(0,(k-K)*n))
#   
#   } #END for loop 1:k
  
  #dLag²/dr_{ik}dlambda_{1k}
  #Jac[1:(n*k),(n*k+1):((n+1)*k)] = sapply(1:k, function(K){c(rep(0,(K-1)*n),rep(1,n),rep(0,(k-K)*n))})
  Jac[1:(n*k),(n*k+1):((n+1)*k)] = c(rep(c(rep.int(1,n), rep.int(0,n*k)),k-1), rep.int(1,n)) 
  #dLag²/dr_{ik}dlambda_{2k}
  Jac[1:(n*k),((n+1)*k+1):((n+2)*k)] = vapply(1:k,FUN.VALUE = vector("numeric",n*k), function(K){c(rep(0,(K-1)*n),2 *rMat[,K],rep(0,(k-K)*n))})
     
  for (K in 1:(k-1)){
      for(Kinner in (K+1):k){
         #dLag²/dr_{ik}dlambda_{3kk'}
      # Jac[1:(n*k),((n+2)*k+(K*(2*k-1-K)/2-k+Kinner))] = c(rep(0, n*(K-1)), rMat[,Kinner], rep(0, n*(Kinner-K-1)), rMat[,K],rep(0, n*(k-Kinner)))
        
#       for(N in 1:n){
#           #dLag²/dr_{ik}dr_{ik'}
#       Jac[N+(n*(K-1)),N+(n*(Kinner-1))] = -sum(reg[K,]*reg[Kinner,]*(1+y[N,]/current.theta)*mu[N,]/(1+mu[N,]/current.theta)^2) + lambda3Mat[Kinner, K]
#       }
      
        #dLag²/dr_{ik}dr_{ik'}
      diag(Jac[(n*(K-1)+1):(n*K),(n*(Kinner-1)+1):(n*Kinner)]) = -t(tcrossprod(reg[K,]*reg[Kinner,],(1+t(t(y)/current.theta))*mu/(1+t(t(mu)/current.theta))^2)) + lambda3Mat[Kinner, K]
      }
  }
  #dLag²/dr_{ik}dlambda_{3kk'}
    Jac[1:(n*k),((n+2)*k+1):(n*k+nLambda)] = sapply(1:(k-1), function(K){
        vapply((K+1):k,FUN.VALUE = vector("numeric",n*k), function(Kinner){
          c(rep(0, n*(K-1)), rMat[,Kinner], rep(0, n*(Kinner-K-1)), rMat[,K],rep(0, n*(k-Kinner)))
        })
      })
  
  #Symmetrize
  Jac = Jac + t(Jac)
#dLag²/dr_{ik}²
#   diag(Jac[1:(n*k),1:(n*k)]) = as.vector(unlist(sapply(1:k, function(K){
#     sapply(1:n, function(N){
#       -sum(reg[K,]^2*(1+y[N,]/current.theta)*mu[N,]/(1+mu[N,]/current.theta)^2) + 2*beta[(n+1)*k+K]
#       })
#   })))
  
    diag(Jac[1:(n*k),1:(n*k)]) = c(t(-tcrossprod(reg^2 ,(1+t(t(y)/current.theta))*mu/(1+t(t(mu)/current.theta))^2) + 2*rep(beta[((n+1)*k+1):((n+2)*k)], each = n)))
    
#         Jac[1:(n*k),1:(n*k)] = t(-outer(t(reg) %*% reg , (1+t(t(y)/current.theta))*mu/(1+t(t(mu)/current.theta))^2) + 2*rep(beta[((n+1)*k+1):((n+2)*k)], each = n)) * 
#       do.call(cbind, replicate(k,matrix(rep(t(diag(1,ncol=n, nrow=n)),k), ncol=n, byrow=TRUE),simplify=FALSE))
  
  Jac
#   Jac[1:(n*k),1:(n*k)] = Jac[1:(n*k),1:(n*k)]/n
#   Jac*lambdaWeights
}
#-------------------------------------------#
# A function to provide an analytical jacobian in the hope to speed up the calculations
NBjacobianCol = function(beta, y, reg, current.theta, abunds, libSizes, k, n ,p){
  #@return a symmetric jacobian matrix of size p*k + k(k-1)/2
  cMat = matrix(beta[1:(k*p)], byrow=TRUE, nrow=k, ncol=p)
  nLambda = k*(k-1)/2+2*k #Number of lambda parameters
  lambda3Mat = matrix(0,ncol=k*(k-1)/2+1, nrow=k*(k-1)/2+1) #Reorder to lower triangular matrix
  lambda3Mat[lower.tri(lambda3Mat, diag=FALSE)] = beta[((p+2)*k+1):length(beta)]
  lambda3Mat = lambda3Mat + t(lambda3Mat)
  #lambdaWeights = (p+nLambda)/(1+nLambda) #Weigths of the restircting equations
  
  #Calculate the mean
  mu = exp(reg %*% cMat)* outer(libSizes, abunds)

  Jac = matrix(0, nrow= p*k + nLambda, ncol=p*k + nLambda)
  #The suymmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle
#   
#   for (K in 1:k){
#       #dLag²/dr_{ik}dlambda_{1k}
#       Jac[1:(p*k),(p*k+K)] = c(rep(0,(K-1)*p),abunds,rep(0,(k-K)*p))
#       
#        #dLag²/dr_{ik}dlambda_{2k}
#       Jac[1:(p*k),((p+1)*k+K)] = if(LASSO) c(rep(0,(K-1)*p),ifelse(cMat[K,]>0,1,-1)*abunds,rep(0,(k-K)*p)) else c(rep(0,(K-1)*p),2 *cMat[K,]*abunds,rep(0,(k-K)*p))
#   
#   } #END for loop 1:k
  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:(p*k),(p*k+1):((p+1)*k)] = c(rep(c(abunds, rep.int(0,p*k)),k-1), abunds)
  #Jac[1:(p*k),(p*k+1):((p+1)*k)] = sapply(1:k, function(K){c(rep(0,(K-1)*p),abunds,rep(0,(k-K)*p))})
  Jac[1:(p*k),((p+1)*k+1):((p+2)*k)] = vapply(1:k,FUN.VALUE = vector("numeric", p*k), function(K){c(rep(0,(K-1)*p),abunds*2 *cMat[K,],rep(0,(k-K)*p))})
     
  for (K in 1:(k-1)){
      for(Kinner in (K+1):k){
         #dLag²/dr_{ik}dlambda_{3kk'}
      # Jac[1:(p*k),((p+2)*k+(K*(2*k-1-K)/2-k+Kinner))] = c(rep(0, p*(K-1)), cMat[Kinner,]*abunds, rep(0, p*(Kinner-K-1)), cMat[K,]*abunds,rep(0, p*(k-Kinner)))
        
#         for(P in 1:p){
#           #dLag²/dr_{ik}dr_{ik'}
#       Jac[P+(p*(K-1)),P+(p*(Kinner-1))] = -sum(reg[,K]*reg[,Kinner]*(1+y[,P]/current.theta[P])*mu[,P]/(1+mu[,P]/current.theta[P])^2) + lambda3Mat[Kinner, K]*abunds[P]
#         }
  #dLag²/dr_{ik}dr_{ik'}
            diag(Jac[(p*(K-1)+1):(p*K),(p*(Kinner-1)+1):(p*Kinner)]) =  crossprod( -(1+t(t(y)/current.theta))*mu/(1+t(t(mu)/current.theta))^2 , (reg[,K]*reg[,Kinner])) + lambda3Mat[Kinner, K]*abunds
    }
  }
  
    #dLag²/ds_{ik}dlambda_{3kk'}
    Jac[1:(p*k),((p+2)*k+1):(p*k+nLambda)] = sapply(1:(k-1), function(K){
        vapply((K+1):k,FUN.VALUE = vector("numeric",p*k), function(Kinner){
          c(rep(0, p*(K-1)), abunds*cMat[Kinner,], rep(0, p*(Kinner-K-1)), abunds*cMat[K,],rep(0, p*(k-Kinner)))
        })
      })
  
  #Symmetrize
  Jac = Jac + t(Jac)
#dLag²/dr_{ik}²
#   diag(Jac[1:(p*k),1:(p*k)]) = as.vector(unlist(sapply(1:k, function(K){
#     sapply(1:p, function(P){
#       -sum(reg[,K]^2*(1+y[,P]/current.theta[P])*mu[,P]/(1+mu[,P]/current.theta[P])^2) + 2*beta[(p+1)*k+K]*abunds[P]
#       })
#   })))
  
    diag(Jac[1:(p*k),1:(p*k)]) = c(- crossprod((1+t(t(y)/current.theta))*mu/(1+t(t(mu)/current.theta))^2, reg^2)) + 2*rep(beta[((p+1)*k+1):((p+2)*k)], each = p)*abunds
  Jac
  # Jac[1:(p*k),1:(p*k)] = Jac[1:(p*k),1:(p*k)]/p
  # Jac*lambdaWeights
}
#-------------------------------------------#
## A wrapper function to calculate the overdispersion, abundance and importance parameters

outerLoop = function(X, k, tol = 5e-3, maxItOut = 500, Psitol = 1e-3, verbose = TRUE, libSizes = NULL, rMatInit = NULL, cMatInit = NULL, psiInit = NULL,  global ="dbldog", nleqslv.control=list(),method=c("Broyden", "Newton"), lambdaRow=NULL, lambdaCol = NULL, quant=0.99,  dispFrec=5){
  
# @param X: a nxp data matrix
# @param k: a scalar, number of dimensions in the RC(M) model
# @param tol(optional): a scalar, the relative convergende tolerance for the row scores and column scores parameters, defaults to 1e-3
# @param Psitol(optional): a scalar, the relative convergence tolerance for the psi parameters, defaults to 1e-4
# @param maxItOut(optional): an integer, the maximum number of iteration in the outer loop, defaults to 50
# @param libSizes(optional) : a vector of length n with (known) library sizes. If not provided, rowSums of x are used
# @param verbose(optional): a boolean, should information on iterations be printed? Defaults to TRUE
# @param method(optional): Method for jacobian estimation , see nleqslv. Defaults to Broyden. The difference with the newton method is that the Jacobian is not recalculated at every iteration
# @param global(optional): global strategy for solving non-linear systems , see nleqslv
# @param nleqslv.control: a list with control options, see nleqslv
# @param lambdaRow: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers for the row scores
# @param lambdaCol: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers for the column scores
# @param rMatInit(optional): a nxk matrix with initial row scores. If not provided values from the singular value decomposition will be used as starting values
# @param cMatInit(optional): a pxk matrix with initial column scores. If not provided values from the singular value decomposition will be used as starting values
# @param psisInit(optional): a vector of length k with inital values for the importance parameters psi. If not provided values from the singular value decomposition will be used as starting values

## @return A list with elements:
# @return psis: a vector of length k with estimates for the importance parameters psi
# @return thetas: a vector of length p with estimates for the overdispersion
# @return rMat: a nxk matrix with estimated row scores
# @return cMat: a pxk matrix with estimated column scores
# @return converged: a boolean indicating if the algorithm converged
# @return rowRec: a n x k x maxItOut array with a record of all rMat estimates through the iterations
# @return colRec: a k x p x maxItOut array with a record of all cMat estimates through the iterations
# @return psiRec.: a k x maxItOut array with a record of all psi estimates through the iterations
  
 ## 1) Initialization
  svdX = initSVD(X)
  #assoc = logmult::rc(X, nd=min(dim(X)-1), weighting = "marginal", se="none", family= "poisson")$assoc
  n=NROW(X)
  p=NCOL(X)
  
  if(is.null(rMatInit)){
  rMatInit = svdX$u[,1:k]
    # rMatInit = assoc$row[,,1]
  }
  if(is.null(cMatInit)){
  cMatInit = t(svdX$v[,1:k])
    # cMatInit = t(assoc$col[,,1])
  }
  if(is.null(psiInit)){
   psiInit = svdX$d[1:k]*sqrt(colSums(rMatInit^2)*rowSums(cMatInit^2))
    # psiInit = assoc$phi[1,]
  }
  if(is.null(libSizes)){
    libSizes = rowSums(X)
  }
  abunds = (colSums(X)/sum(X))
  
  rMatInit = apply(rMatInit, 2,function(rowS){
    rowS - mean(rowS)
  })
  rMatInit = apply(rMatInit, 2,function(rowS){
      rowS/sqrt(sum(rowS^2))
  })
  cMatInit = t(apply(cMatInit, 1,function(colS){
    colS - sum(colS * abunds)/sum(abunds)
  }))
  cMatInit = t(apply(cMatInit, 1,function(colS){
    colS/sqrt(sum(colS^2 * abunds)/sum(abunds))
  }))
  #thetaInit = thetas
  psi = psiOld = psiInit
  rMat = rMatOld = rMatInit
  cMat = cMatOld = cMatInit
  if(is.null(lambdaRow)) lambdaRow =  rep(0,2*k+k*(k-1)/2)
  if (is.null(lambdaCol)) lambdaCol =  rep(0,2*k+k*(k-1)/2)

  iterOut = 1
  rowRec = array(0,dim=c(NROW(X),k, maxItOut))
  colRec = array(0,dim=c(k,NCOL(X), maxItOut))
  thetaRec = matrix(0,ncol=maxItOut, nrow=length(abunds))
  psiRec = matrix(0,ncol=maxItOut, nrow=k)

  ## 2) Propagation
  
  while((iterOut ==1) || ((iterOut <= maxItOut) && ((any(abs(1-psi/psiOld) > Psitol)) || 
                          (quantile(abs(1-rMat/rMatOld),quant) > tol) || (quantile(abs(1-cMat/cMatOld),quant) > tol) )))
    {
    
  if(verbose && iterOut%%1 == 0){
  cat("\n","Outer Iteration", iterOut, "\n","\n")
    if(iterOut!=1){
  cat("Old psi-estimates: ", psiOld, "\n")
  cat("New psi-estimates: ", psi, "\n")
    }
  }
  ## 2)a. Store old parameters
  psiOld = psi
  rMatOld = rMat
  cMatOld = cMat
    
#   ## 2)b. ML estimation of dispersion
#   while(iterIn ==1 || (iterIn <= maxItIn) && (any(abs(psi-psiOldIn) > tol))){
# #     
#     psiOldIn = psi
#     cat("Inner iteration ", iterIn, "\n")
  if((iterOut %% dispFrec) ==0  || iterOut==1){
  if (verbose) cat("\n Estimating overdispersions \n")
 thetas = estDisp(X = X, rMat = rMat,cMat =  cMat,  abunds=abunds, libSizes = libSizes, psis = psi, k=k)
  }
  ## 2)c. ML estimation of psis
 if(verbose) cat("\n Estimating psis \n")
  psi =  estPsis(X = X, rMat = rMat, cMat = cMat, psiInit = psi, abunds = abunds, libSizes = libSizes, thetas = thetas, nleqslv.control =nleqslv.control, k=k)
#   iterIn = iterIn+1 
#   }

   # iterIn = 1
  
              if(verbose){
    cat("\n Estimating column scores \n")
              }
    cMatList = scoreCalc(X = X, psis = psi, thetas = thetas, abunds = abunds, toEstimate = "columns", rMat = rMat, cMat = cMat, libSizes = libSizes, global =global, nleqslv.control = nleqslv.control, lambda =lambdaCol, method=method,  k=k, n=n, p=p)
  lambdaCol = cMatList$lambdaCol
  cMat = cMatList$cMat   
  
     if(verbose) cat("\n Estimating psis \n")
  psi =  estPsis(X = X, rMat = rMat, cMat = cMat, psiInit = psi, abunds = abunds, libSizes = libSizes, thetas = thetas, nleqslv.control =nleqslv.control, k=k)
  
      if(verbose){
    cat("\n Estimating row scores \n")
  }
  rMatFit = scoreCalc(X = X, psis = psi, thetas = thetas, abunds = abunds, toEstimate = "rows", rMat = rMat, cMat = cMat, libSizes = libSizes, global = global, nleqslv.control =nleqslv.control, lambda = lambdaRow, method=method,  k=k, n=n, p=p)
  #Renormalize to unit sum
  lambdaRow = rMatFit$lambdaRow
  rMat = rMatFit$rMat
#   ## 2)b. ML estimation of dispersion
#   while(iterIn ==1 || (iterIn <= maxItIn) && (any(abs(psi-psiOldIn) > tol))){
# #     
#     psiOldIn = psi
#     cat("Inner iteration ", iterIn, "\n")
#    if (verbose) cat("\n Estimating overdispersions \n")
#  thetas = estDisp(X = X, rMat = rMat,cMat =  cMat, thetaInit = thetaInit, abunds=abunds, libSizes = libSizes, maxItDisp = maxItNB, psis = psi)$thetas
  
#   ## 2)c. ML estimation of psis
#   if(verbose) cat("\n Estimating psis \n")
#   psi =  estPsis(X = X, rMat = rMat, cMat = cMat, psiInit = psi, abunds = abunds, libSizes = libSizes, thetas = thetas, nleqslv.control =nleqslv.control)$psi
#   iterIn = iterIn+1 
#   }
#         if(verbose){
#     cat("\n Estimating row scores \n")
#   }
#   rMatFit = scoreCalc(X = X, psis = psi, thetas = thetas, abunds = abunds, toEstimate = "rows", rMat = rMat, cMat = cMat, libSizes = libSizes, global = global, nleqslv.control =nleqslv.control, lambda = lambdaRow, method=method, LASSO = LASSO)
#   #Renormalize to unit sum
#   lambdaRow = rMatFit$lambdaRow
#   rMat = rMatFit$rMat

  #cMatSE = cMatList$cMatSE
  rowRec[,, iterOut] = rMat
  colRec[,, iterOut] = cMat
  thetaRec [, iterOut] = thetas
  psiRec[, iterOut] = psi
  
  ## 2)f. Change iterator
    iterOut = iterOut + 1
} # END while-loop
  
  ## 3) Termination
  convergence = !(((iterOut <= maxItOut) && (any(abs(1-psi/psiOld) > Psitol)) || 
                          (quantile(abs(1-rMat/rMatOld),quant) > tol) || (quantile(abs(1-cMat/cMatOld),quant) > tol) ))
  
  rownames(rMat) = rownames(X)
  colnames(cMat) = colnames(X)
  rownames(cMat) = colnames(rMat) = paste0("Dim",1:k)
  
  if(!convergence ){
    warning("Algorithm did not converge! Check for errors or consider changing tolerances or number of iterations")
  }
    return(list(rMat=rMat, cMat=cMat, thetas = thetas, psis = psi, 
                converged = convergence, rowRec= rowRec, colRec = colRec, psiRec = psiRec, thetaRec = thetaRec, lambdaRow=lambdaRow, lambdaCol=lambdaCol))
}
#-------------------------------------------#
#A simple wrapper function for phyloseq objects, passes all argument sonto outerLoop()

phyloWrapper = function(physeq, ...){
  dat = if (taxa_are_rows(physeq)) t(otu_table(physeq)@.Data) else otu_table(physeq)@.Data
  outerLoop(dat, ...)
}

#-------------------------------------------#
## A plotting function that plots the samples as dots and the species as arrows

plotRCM = function(psis, rMat, cMat, Dim = c(1,2), X = NULL, thetas = NULL, 
                   abunds = NULL, arrowFrac = 0.04, biplot = TRUE,
                   libLoc ="topleft",libLegend=TRUE, libInset = c(0,-0.3), dispInset = c(0,-0.4), abInset = c(0,-0.4),stressSpecies=NULL, ...){
# @param psis: vector of length k with psi estimates
# @param rMat: a nxk matrix with final row scores
# @param cMat: a pxk with matrix with final column scores
# @param Dim (optional): a vector of length 2, indicating which dimensions to fit, defaults to the first two
# @param X (optional): the nxp data matrix
# @param thetas (optional): a vector of length p with estimates for the overdispersion
# @param abunds(optional): a vector of length p with abundance estimates (otherwise it is estimated from the data)
# @param arrowFrac(optional): Fraction of largest species to plot. defaults to 0.1
# @param biplot(optional): A boolean indicating if species should be added to the plot, defaults to TRUE
# @param libLoc(optional): a string, location of the library size legend. Defaults to "topleft"
# @param libLegend(optional): a boolean, should library size legend be displayed? defaults to TRUE
# @param libInset, dispInset, abInset(optional): numeric vectors of length 2, insets for library size, dispersion and abundance legends
# param stressSpecies(optional): names of species to be highlighted
  
# @param ... additional arguments, passed on to the plot() function
# @return: NONE,  plots the result in the plotting window
  # tmp = par(no.readonly = TRUE)
  par(mar=c(4,5,5,5), pty="s")
  if(!(length(psis)== NCOL(rMat) && length(psis) == NROW(cMat))){
    stop("Dimensions don't match")
  }
  #Reorder dimensions according to importance
  Dim = Dim[order(psis, decreasing = TRUE)]
  
  a = Dim[1]
  b = Dim[2]
  
  #Add colours for the library sizes
  if(!is.null(X)){
  Colour = ifelse(rowSums(X) < median(rowSums(X)), "blue","red")
  } else {Colour = 1}
  
  ## Add linetypes for the dispersions
  if(!is.null(thetas)){
    LineType = rowSums(sapply(quantile(1/thetas, c(0.25,0.5,0.75,1)), function(x){
      1/thetas > x
    })) 
  } else {LineType=rep(1, ncol(cMat))}
  
  ##Add colours for the abundances
  if(!is.null(abunds)){
    lineColour = rowSums(sapply(quantile(abunds, c(0.25,0.5,0.75,1)), function(x){
      abunds < x
    })) 
  } else {lineColour = rep(1, ncol(cMat))}
  
  plot(x = rMat[,a] * psis[a],
  y = rMat[,b] *psis[b],
  xlab="Dim1",
  ylab="Dim2",
  col = Colour,
  asp=1,
  ...)
  if(!is.null(X) & libLegend){
  legend(libLoc,legend=c("Small library size","Large library size"),
         pch=c(1,1), col=c("blue","red"), inset=libInset, xpd=TRUE)
  }
  
    if (biplot){
      #Arrows
        arrowLengths = apply(cMat[Dim,],2,function(x){sqrt(sum(x^2))})
    id = arrowLengths >= quantile(arrowLengths,1-arrowFrac)
        scalingFactor = min(abs(apply(t(t(rMat[,Dim])*psis[Dim]),2, range)))/
        max(abs(cMat[Dim,id]))*0.99
    arrows(x0=0,y0=0,x1=cMat[a,id]*scalingFactor,y1=cMat[b, id]*scalingFactor, 
           lty=LineType[id], col = lineColour[id])
    if(!is.null(thetas)){
      legend("top", legend=paste0(">",seq(0,75,25), "th quantile"), 
             lty=1:4, title="Dispersion", xpd=TRUE, inset = dispInset, cex= 0.75)
    }
    if(!is.null(abunds)){
      legend("topright", legend=paste0(">",seq(0,75,25), "th quantile"), 
             col = 1:4,lty=1, title="Abundance", xpd=TRUE, inset = abInset,cex=0.75)
    }
    #if(!is.null(stressSpecies))
    }
  # par(tmp)
  }
```

#Demonstration

We generate some data as before with the NB distribution but differing library sizes, apply our algorithm and plot the results.

```{r DataGen}
#Generate as synthetic dataset with known row and column scores and psi parameters
load("/home/stijn/PhD/American Gut/AGpars.RData")
Nsamples= 150
Ntaxa = 800
thetas=thetas[1:Ntaxa]
thetas = thetas[1/thetas<100]
rhos=rhos[names(thetas)]
Ntaxa=length(rhos) - length(rhos)%%2
rhos=rhos[1:Ntaxa]
thetas = thetas[1:Ntaxa]
rhos = rhos/sum(rhos)

libSizes4 =c(rep(1e4, floor(Nsamples/2)), rep(1e5, floor(Nsamples/2)))
psi1 = 7
psi2 = 5
#Samples 1-10 and Nsamples - 1 - -10 have increased row scores 1
#Samples 6-15 and Nsamples -6 - -15 have increased row scores 2
rowScores1 = rnorm(Nsamples-Nsamples%%2, sd=8)  + c(rep(25,10), rep(0, Nsamples-20), rep(25,10))
rowScores2 = rnorm(Nsamples-Nsamples%%2, sd=5) - c(rep(0,5),rep(8,15), rep(0, Nsamples-40), rep(8,15), rep(0,5))

#Taxa 1-10 and Nsamples - 1 - -10 have increased col scores 1
#Taxa 6-15 and Nsamples -6 - -15 have increased col scores 2
colScores1 = rnorm(Ntaxa-Ntaxa%%2, sd = 8) + c(rep(0,10),rep(25,10), rep(0, Ntaxa-30), rep(25,10))
colScores2 = rnorm(Ntaxa-Ntaxa%%2, sd = 5) - c(rep(0,5),rep(2,15), rep(0, Ntaxa-40), rep(2,15), rep(0,5))

normalize = function(mat, weights, dim){
  mat = apply(mat,dim, function(vec){
    vec - sum(vec*weights)/sum(weights)#
  })
  if(dim==1) mat= t(mat)
  mat = apply(mat, dim, function(y){
    y/sqrt(sum(weights*y^2))
  })
  if(dim==1) mat =t(mat)
  return(mat)
}

rowScoresMat = normalize(cbind(rowScores1, rowScores2),dim=2,weights=rep(1, length(rowScores1)))
colScoresMat = normalize(rbind(colScores1, colScores2),dim=1,weights=rhos)

meanMat = outer(libSizes4, rhos)* exp(psi1*outer(rowScoresMat[,1],colScoresMat[1,]) + psi2*outer(rowScoresMat[,2],colScoresMat[2,]))

thetaMat = matrix(thetas, nrow=Nsamples, ncol=Ntaxa, byrow=TRUE)

dataMat4 = apply(array(data= c(meanMat, thetaMat), dim=c(Nsamples, Ntaxa, 2)), c(1,2), function(x){rnbinom(1,mu=x[1], size=x[2])})
colnames(dataMat4) = names(rhos)
#Introduce DA
# dataMat4[1:(Nsamples/4),1:(Ntaxa/4)] = sapply(1:(Nsamples/4),function(i){rpois(n = Ntaxa/4, lambda =10)})
#Remove all zero columns and rows
dataMat4 = dataMat4[rowSums(dataMat4)>0, colSums(dataMat4) > 0]
colScoresMat=colScoresMat[, colSums(dataMat4) > 0]
rowScoresMat =rowScoresMat[rowSums(dataMat4)>0,]
rownames(dataMat4) = paste0("Sample", 1:Nsamples)
rhos=rhos[colnames(dataMat4)]
thetas =thetas[colnames(dataMat4)]
plotRCM(psis = c(psi1, psi2), rMat = rowScoresMat, cMat = colScoresMat, X = dataMat4, biplot = TRUE, libLegend=TRUE, abunds=rhos, thetas = thetas, libInset = c(0,-0.2), dispInset = c(0,-0.25), abInset = c(0,-0.25), arrowFrac=0.01)
points(col="green", pch=2, c(rowScoresMat[1:9,1],rowScoresMat[(Nsamples-9):Nsamples,1])*psi1, c(rowScoresMat[1:9,2],rowScoresMat[(Nsamples-9):Nsamples,2])*psi2)
points(col="black", pch=3, c(rowScoresMat[(Nsamples-9):Nsamples,1])*psi1, c(rowScoresMat[(Nsamples-9):Nsamples,2])*psi2)
quantile(exp(outer(c(psi1,psi2),(rowScoresMat %*% colScoresMat))))
```

```{r Own Method: loglinear NB}
maxItOut = 5e2
tmp3Job = mcparallel(outerLoop(dataMat4, k=2, maxItOut = maxItOut, rMat = NULL, cMat =  NULL, psiInit =NULL, nleqslv.control=list(xtol=1e-10, ftol=1e-12, btol=1e-3, sigma=0.5, trace=FALSE, allowSingular=FALSE, maxit=250), tol = 5e-3,Psitol = 1e-4, global = c("dbldog"),method=c("Broyden"), lambdaCol=NULL, lambdaRow = NULL))#, "pwldog", "cline")), "gline"
tmp3j = mccollect(tmp3Job, FALSE)[[1]]

tmp3Jobc = mcparallel(outerLoop(dataMat4, k=2, maxItOut = maxItOut, rMat = tmp3j$rMat, cMat =  tmp3j$cMat, psiInit = tmp3j$psis, nleqslv.control=list(xtol=1e-10, ftol=1e-10, btol=1e-3, sigma=0.5, trace=FALSE, allowSingular=FALSE, maxit=150), tol = 5e-3,Psitol = 1e-4, global = c("dbldog"),method=c("Broyden"), lambdaCol=tmp3j$lambdaCol, lambdaRow = tmp3j$lambdaRow, dispFrec = 5))#, "pwldog", "cline")), "gline"
tmp3jc = mccollect(tmp3Jobc, FALSE)[[1]]

tmp3 =outerLoop(dataMat4, k=2, maxItOut = maxItOut, nleqslv.control=list(xtol=1e-10, ftol=1e-12, btol=1e-3, sigma=0.5, trace=TRUE, allowSingular=FALSE, maxit=150), tol = 5e-3,Psitol = 1e-4, global = c("dbldog"),method=c("Broyden"))

tmp3c = outerLoop(dataMat4, k=2, maxItOut = maxItOut,  rMat = tmp3$rMat, cMat =  tmp3$cMat, psiInit = tmp3$psis, nleqslv.control=list(xtol=1e-10, ftol=1e-12, btol=1e-3, sigma=0.5, trace=FALSE, allowSingular=FALSE, maxit=150), tol = 5e-3,Psitol = 1e-4, global = c("dbldog"), lambdaCol=tmp3$lambdaCol, lambdaRow = tmp3$lambdaRow)
save(tmp3c, dataMat4, psi1, psi2, rowScoresMat, colScoresMat,thetas, rhos,  file="tmp3c.RData")

#Check constraints
apply(tmp3$rMat, 2, function(y){
  sum(y)
})
apply(tmp3$cMat, 1, function(y){
  sum(y*colSums(dataMat4)/sum(dataMat4))
})
apply(tmp3$rMat, 2, function(y){
  sum(y^2)
})
apply(tmp3$cMat, 1, function(y){
  sum(y^2*colSums(dataMat4)/sum(dataMat4))
})
sapply(1:(ncol(tmp3$rMat)-1), function(i){
  sapply(2:ncol(tmp3$rMat), function(j){
    sum(tmp3$rMat[,i]*tmp3$rMat[,j])
  })
})
sapply(1:(nrow(tmp3$cMat)-1), function(i){
  sapply(2:nrow(tmp3$cMat), function(j){
    sum(colSums(dataMat4)/sum(dataMat4)*tmp3$cMat[i,]*tmp3$cMat[j,])
  })
})

plotRCM(tmp3$psis, tmp3$rMat, tmp3$cMat, X = dataMat4, thetas = thetas,abunds = rhos, arrowFrac = 0.05, biplot =FALSE)
points(y=tmp3$psis[2]*tmp3$rMat[c(1:10),2], tmp3$psis[1]*tmp3$rMat[c(1:10),1], col="green", pch=2)
points(y=tmp3$psis[2]*tmp3$rMat[,2][c((Nsamples-9):Nsamples)], tmp3$psis[1]*tmp3$rMat[,1][c((Nsamples-9):Nsamples)], col="black", pch=3)

plot(y=tmp3$rMat[,1]*tmp3$psis[1], tmp3$rMat[,2]*tmp3$psis[2], col=1:200 %in% c(1:10,191:200) + 1:200 %in% c(1:10)+1)
```

If we put nothing in there then our function does not find anything either. It does not model random noise.

Compare with CA solution

```{r CA solution}
SVD4 = initSVD(dataMat4)
plotRCM(SVD4$d[1:2], t(SVD4$u[1:2,]), t(SVD4$v[,1:2]), X = dataMat4, main="Correspondence analysis", arrowFrac = 0.05, thetas = NULL, abunds = colSums(dataMat4)/sum(dataMat4), biplot = TRUE)
points(SVD4$u[1,c(1:10)]*SVD4$d[1],SVD4$u[2,c(1:10)]*SVD4$d[2], col="green", pch=2)
points(SVD4$u[1,c((Nsamples-9):Nsamples)]*SVD4$d[1],SVD4$u[2,c((Nsamples-9):Nsamples)]*SVD4$d[2], col="black", pch=3)
```

Another dataset

```{r Other dataset}
load("/home/stijn/PhD/American Gut/AGpars.RData")
Nsamples= 100
Ntaxa = 600
thetas=thetas[1:Ntaxa]
thetas = thetas[1/thetas<100]
rhos=rhos[names(thetas)]
Ntaxa=length(rhos) - length(rhos)%%2
rhos=rhos[1:Ntaxa]
thetas = thetas[1:Ntaxa]
rhos = rhos/sum(rhos)

libSizes5 =c(rep(1e4, floor(Nsamples/2)), rep(1e5, floor(Nsamples/2)))
psi1 = 10
psi2 = 0.1
#Samples 1-10 and Nsamples - 1 - -10 have increased row scores 1
#Samples 6-15 and Nsamples -6 - -15 have increased row scores 2
rowScores1b = rnorm(Nsamples-Nsamples%%2, sd=5)  + c(rep(25,10), rep(0, Nsamples-20), rep(25,10))
rowScores2b = rnorm(Nsamples-Nsamples%%2, sd=3) - c(rep(0,5),rep(8,15), rep(0, Nsamples-40), rep(8,15), rep(0,5))

#Taxa 1-10 and Nsamples - 1 - -10 have increased col scores 1
#Taxa 6-15 and Nsamples -6 - -15 have increased col scores 2
colScores1b = rnorm(Ntaxa-Ntaxa%%2, sd = 5) + c(rep(0,10),rep(25,10), rep(0, Ntaxa-30), rep(2,10))
colScores2b = rnorm(Ntaxa-Ntaxa%%2, sd = 3) - c(rep(0,5),rep(10,15), rep(0, Ntaxa-40), rep(10,15), rep(0,5))

normalize = function(mat, weights, dim){
  mat = apply(mat,dim, function(vec){
    vec - sum(vec*weights)/sum(weights)#
  })
  if(dim==1) mat= t(mat)
  mat = apply(mat, dim, function(y){
    y/sqrt(sum(weights*y^2))
  })
  if(dim==1) mat =t(mat)
  return(mat)
}

rowScoresMatb = normalize(cbind(rowScores1b, rowScores2b),dim=2,weights=rep(1, length(rowScores1)))
colScoresMatb= normalize(rbind(colScores1b, colScores2b),dim=1,weights=rhos)

meanMat = outer(libSizes4, rhos)* exp(psi1*outer(rowScoresMat[,1],colScoresMat[1,]) + psi2*outer(rowScoresMat[,2],colScoresMat[2,]))

thetaMat = matrix(thetas, nrow=Nsamples, ncol=Ntaxa, byrow=TRUE)

dataMat5 = apply(array(data= c(meanMat, thetaMat), dim=c(Nsamples, Ntaxa, 2)), c(1,2), function(x){rnbinom(1,mu=x[1], size=x[2])})
colnames(dataMat5) = names(rhos)
#Introduce DA
# dataMat4[1:(Nsamples/4),1:(Ntaxa/4)] = sapply(1:(Nsamples/4),function(i){rpois(n = Ntaxa/4, lambda =10)})
#Remove all zero columns and rows
dataMat5 = dataMat5[rowSums(dataMat5)>0, colSums(dataMat5) > 0]
rownames(dataMat5) = paste0("Sample", 1:Nsamples)
rhos=rhos[colnames(dataMat5)]
thetas =thetas[colnames(dataMat5)]
plotRCM(psis = c(psi1, psi2), rMat = rowScoresMatb, cMat = colScoresMatb, X = dataMat5, biplot = TRUE, libLegend=TRUE, abunds=rhos, thetas = thetas, libInset = c(0,-0.2), dispInset = c(0,-0.25), abInset = c(0,-0.25))
points(col="green", pch=2, c(rowScoresMatb[1:9,1],rowScoresMatb[(Nsamples-9):Nsamples,1])*psi1, c(rowScoresMatb[1:9,2],rowScoresMatb[(Nsamples-9):Nsamples,2])*psi2)
points(col="black", pch=3, c(rowScoresMatb[(Nsamples-9):Nsamples,1])*psi1, c(rowScoresMatb[(Nsamples-9):Nsamples,2])*psi2)
#How did the means change?
quantile(exp(outer(c(10,0.1),(rowScoresMatb %*% colScoresMatb ))))
```

```{r Own Method: loglinear NB: dataMat5}
maxItOut = 2e2
tmp5Job = mcparallel(outerLoop(dataMat5, k=2, maxItOut = maxItOut, rMat = NULL, cMat =  NULL, psiInit =NULL, nleqslv.control=list(xtol=1e-10, ftol=1e-12, btol=1e-3, sigma=0.5, trace=FALSE, allowSingular=FALSE, maxit=100), tol = 5e-3,Psitol = 1e-4, global = c("dbldog"),method=c("Broyden"), lambdaCol=NULL, lambdaRow = NULL))#, "pwldog", "cline")), "gline"
tmp5 = mccollect(tmp5Job, FALSE)[[1]]

tmp5Jobc = mcparallel(outerLoop(dataMat5, k=2, maxItOut = maxItOut, rMat = tmp5$rMat, cMat =  tmp5$cMat, psiInit = tmp5$psis, nleqslv.control=list(xtol=1e-10, ftol=1e-10, btol=1e-3, sigma=0.5, trace=FALSE, allowSingular=FALSE, maxit=150), tol = 5e-3,Psitol = 1e-4, global = c("dbldog"),method=c("Broyden"), lambdaCol=tmp5$lambdaCol, lambdaRow = tmp5$lambdaRow))#, "pwldog", "cline")), "gline"
tmp5jc = mccollect(tmp5Jobc, FALSE)[[1]]

#Check constraints
apply(tmp5$rMat, 2, function(y){
  sum(y)
})
apply(tmp5$cMat, 1, function(y){
  sum(y*colSums(dataMat5)/sum(dataMat5))
})
apply(tmp5$rMat, 2, function(y){
  sum(y^2)
})
apply(tmp5$cMat, 1, function(y){
  sum(y^2*colSums(dataMat5)/sum(dataMat5))
})
sapply(1:(ncol(tmp5$rMat)-1), function(i){
  sapply(2:ncol(tmp5$rMat), function(j){
    sum(tmp5$rMat[,i]*tmp5$rMat[,j])
  })
})
sapply(1:(nrow(tmp5$cMat)-1), function(i){
  sapply(2:nrow(tmp5$cMat), function(j){
    sum(colSums(dataMat5)/sum(dataMat5)*tmp5$cMat[i,]*tmp5$cMat[j,])
  })
})

plotRCM(tmp5$psis, tmp5$rMat, tmp5$cMat, X = dataMat4, thetas = thetas,abunds = rhos, arrowFrac = 0.05, biplot =TRUE)
points(y=tmp5$psis[2]*tmp5$rMat[c(1:10),2], tmp5$psis[1]*tmp5$rMat[c(1:10),1], col="green", pch=2)
points(y=tmp5$psis[2]*tmp5$rMat[,2][c((Nsamples-9):Nsamples)], tmp5$psis[1]*tmp5$rMat[,1][c((Nsamples-9):Nsamples)], col="black", pch=3)

plot(y=tmp5$rMat[,1]*tmp5$psis[1], tmp5$rMat[,2]*tmp5$psis[2], col=1:200 %in% c(1:10,191:200) + 1:200 %in% c(1:10)+1)
```

#Real data

We apply our algorithm to the Kostic data and see if it converges

```{r Kostic data}
load("/home/stijn/PhD/Biplots/Kostic_data/phyloDD.RData")
#First try with a reduced version
DDtrim=prune_samples(x=phyloDD, sample_sums(phyloDD)>quantile(sample_sums(phyloDD), 0.75))
DDtrim2=prune_taxa(x=DDtrim, taxa_sums(DDtrim)>quantile(taxa_sums(DDtrim), 0.67))
DDtrim2Job= mcparallel(phyloWrapper(DDtrim2, k=2, nleqslv.control=list(trace=TRUE), method="Broyden", tol=1e-2, maxItOut=2e2))
DDtrimLL = mccollect(DDtrim2Job, FALSE)[[1]]
save(DDtrimLL, file="DDtrimLL.RData")
plotRCM(DDtrimLL$psis, DDtrimLL$rMat, DDtrimLL$cMat)

kostLLjob = mcparallel(phyloWrapper(phyloDD, k=2, nleqslv.control=list(trace=TRUE), method="Newton", tol=1e-2))
kostLL = mccollect(kostLLjob, FALSE)[[1]]
load("/home/stijn/PhD/American Gut/AGphylo.RData")
AGjob = mcparallel(phyloWrapper(AGphylo, k=2, nleqslv.control=list(trace=TRUE), method="Newton", tol=1e-2))
AGLL = mccollect(AGjob, FALSE)[[1]]
AGLL = phyloWrapper(AGphylo, k=2, nleqslv.control=list(trace=TRUE, maxit=200), method="Newton", dispFrec=5, tol=1e-2)
```

```{r Other dataset dataMat6}
load("/home/stijn/PhD/American Gut/AGpars.RData")
Nsamples= 50
Ntaxa = 300
thetas=thetas[1:Ntaxa]
thetas = thetas[1/thetas<100]
rhos=rhos[names(thetas)]
Ntaxa=length(rhos) - length(rhos)%%2
rhos=rhos[1:Ntaxa]
thetasc = thetas[1:Ntaxa]
rhosc = rhos/sum(rhos)

libSizes6 =c(rep(1e4, floor(Nsamples/2)), rep(1e5, floor(Nsamples/2)))
psi1c = 4
psi2c = 3
#Samples 1-10 and Nsamples - 1 - -10 have increased row scores 1
#Samples 6-15 and Nsamples -6 - -15 have increased row scores 2
rowScores1c = rnorm(Nsamples-Nsamples%%2, sd=5)  + c(rep(25,10), rep(0, Nsamples-20), rep(25,10))
rowScores2c = rnorm(Nsamples-Nsamples%%2, sd=3) - c(rep(0,5),rep(8,15), rep(0, Nsamples-40), rep(8,15), rep(0,5))

#Taxa 1-10 and Nsamples - 1 - -10 have increased col scores 1
#Taxa 6-15 and Nsamples -6 - -15 have increased col scores 2
colScores1c = rnorm(Ntaxa-Ntaxa%%2, sd = 5) + c(rep(0,10),rep(25,10), rep(0, Ntaxa-30), rep(2,10))
colScores2c= rnorm(Ntaxa-Ntaxa%%2, sd = 3) - c(rep(0,5),rep(10,15), rep(0, Ntaxa-40), rep(10,15), rep(0,5))

normalize = function(mat, weights, dim){
  mat = apply(mat,dim, function(vec){
    vec - sum(vec*weights)/sum(weights)#
  })
  if(dim==1) mat= t(mat)
  mat = apply(mat, dim, function(y){
    y/sqrt(sum(weights*y^2))
  })
  if(dim==1) mat =t(mat)
  return(mat)
}

rowScoresMatc = normalize(cbind(rowScores1c, rowScores2c),dim=2,weights=rep(1, length(rowScores1c)))
colScoresMatc= normalize(rbind(colScores1c, colScores2c),dim=1,weights=rhos)

meanMat = outer(libSizes6, rhosc)* exp(psi1c*outer(rowScoresMatc[,1],colScoresMatc[1,]) + psi2c*outer(rowScoresMatc[,2],colScoresMatc[2,]))

thetaMat = matrix(thetasc, nrow=Nsamples, ncol=Ntaxa, byrow=TRUE)

dataMat6 = apply(array(data= c(meanMat, thetaMat), dim=c(Nsamples, Ntaxa, 2)), c(1,2), function(x){rnbinom(1,mu=x[1], size=x[2])})
colnames(dataMat6) = names(rhos)
#Introduce DA
# dataMat4[1:(Nsamples/4),1:(Ntaxa/4)] = sapply(1:(Nsamples/4),function(i){rpois(n = Ntaxa/4, lambda =10)})
#Remove all zero columns and rows
dataMat6 = dataMat6[rowSums(dataMat6)>0, colSums(dataMat6) > 0]
rownames(dataMat6) = paste0("Sample", 1:Nsamples)
rhos=rhos[colnames(dataMat6)]
thetas =thetas[colnames(dataMat6)]
plotRCM(psis = c(psi1c, psi2c), rMat = rowScoresMatc, cMat = colScoresMatc, X = dataMat6, biplot = TRUE, libLegend=TRUE, abunds=rhos, thetas = thetas, libInset = c(0,-0.2), dispInset = c(0,-0.25), abInset = c(0,-0.25))
points(col="green", pch=2, c(rowScoresMatc[1:9,1],rowScoresMatc[(Nsamples-9):Nsamples,1])*psi1c, c(rowScoresMatc[1:9,2],rowScoresMatc[(Nsamples-9):Nsamples,2])*psi2c)
points(col="black", pch=3, c(rowScoresMatc[(Nsamples-9):Nsamples,1])*psi1c, c(rowScoresMatc[(Nsamples-9):Nsamples,2])*psi2c)
save(tmp6, dataMat6, thetasc, rhosc,psi1c, psi2c, rowScoresMatc, colScoresMatc,libSizes6,file="tmp6Est.RData")
```

```{r Own Method: loglinear NB: dataMat5}
maxItOut = 1e2
tmp6Job = mcparallel(outerLoop(dataMat6, k=2, maxItOut = maxItOut, rMat = NULL, cMat =  NULL, psiInit =NULL, nleqslv.control=list(xtol=1e-10, ftol=1e-12, btol=1e-3, sigma=0.5, trace=FALSE, allowSingular=FALSE, maxit=150), tol = 1e-2,Psitol = 1e-3, global = c("dbldog"),method=c("Newton"), lambdaCol=NULL, lambdaRow = NULL))#, "pwldog", "cline")), "gline"
tmp6 = mccollect(tmp6Job, FALSE)[[1]]

tmp6Jobc = mcparallel(outerLoop(dataMat5, k=2, maxItOut = maxItOut, rMat = tmp6$rMat, cMat =  tmp6$cMat, psiInit = tmp6$psis, nleqslv.control=list(xtol=1e-10, ftol=1e-10, btol=1e-3, sigma=0.5, trace=FALSE, allowSingular=FALSE, maxit=250), tol = 5e-3,Psitol = 1e-4, global = c("dbldog"),method=c("Broyden"), lambdaCol=tmp6$lambdaCol, lambdaRow = tmp6$lambdaRow))#, "pwldog", "cline")), "gline"
tmp6jc = mccollect(tmp6Jobc, FALSE)[[1]]

tmp6 =outerLoop(dataMat6, k=2, maxItOut = maxItOut, rMat = NULL, cMat =  NULL, psiInit =NULL, nleqslv.control=list( trace=TRUE, allowSingular=FALSE, maxit=150), tol = 1e-2,Psitol = 1e-3, global = c("dbldog"),method=c("Broyden"), lambdaCol=NULL, lambdaRow = NULL, dispFrec = 5)

tmp62 = outerLoop(dataMat6, k=2, maxItOut = maxItOut, rMat = tmp6$rMat, cMat =  tmp6$cMat, psiInit = tmp6$psis, nleqslv.control=list(trace=TRUE, allowSingular=FALSE, maxit=500), tol = 5e-3,Psitol = 1e-4, global = c("dbldog"),method=c("Broyden", "Newton"), lambdaCol=tmp6$lambdaCol, lambdaRow = tmp6$lambdaRow, dispFrec=5)#, "pwldog", "cline")), "gline"


#Check constraints
apply(tmp6$rMat, 2, function(y){
  sum(y)
})
apply(tmp6$cMat, 1, function(y){
  sum(y*colSums(dataMat6)/sum(dataMat6))
})
apply(tmp6$rMat, 2, function(y){
  sum(y^2)
})
apply(tmp6$cMat, 1, function(y){
  sum(y^2*colSums(dataMat6)/sum(dataMat6))
})
sapply(1:(ncol(tmp6$rMat)-1), function(i){
  sapply(2:ncol(tmp6$rMat), function(j){
    sum(tmp6$rMat[,i]*tmp6$rMat[,j])
  })
})
sapply(1:(nrow(tmp6$cMat)-1), function(i){
  sapply(2:nrow(tmp6$cMat), function(j){
    sum(colSums(dataMat6)/sum(dataMat6)*tmp6$cMat[i,]*tmp6$cMat[j,])
  })
})

plotRCM(tmp6$psis, tmp6$rMat, tmp6$cMat, X = dataMat6, thetas = thetasc,abunds = rhosc, arrowFrac = 0.025, biplot =FALSE, main="Estimated", libInset = c(-0.4,-0.3))
points(y=tmp6$psis[2]*tmp6$rMat[c(1:10),2], tmp6$psis[1]*tmp6$rMat[c(1:10),1], col="green", pch=2)
points(y=tmp6$psis[2]*tmp6$rMat[,2][c((Nsamples-9):Nsamples)], tmp6$psis[1]*tmp6$rMat[,1][c((Nsamples-9):Nsamples)], col="black", pch=3)

plot(y=tmp6$rMat[,1]*tmp6$psis[1], tmp6$rMat[,2]*tmp6$psis[2], col=1:200 %in% c(1:10,191:200) + 1:200 %in% c(1:10)+1)
```

Suggestions for improvement:

 - ZIP
 - Beh papers
 - Enforce constraints after estimation
