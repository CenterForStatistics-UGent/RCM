---
title: "RC(M)"
author: "Stijn"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    includes:
      in_header: packagesRCM.sty
---

```{r check-install-load-packages, warning=FALSE, message=FALSE, echo=FALSE, purl=TRUE}
WD = "/home/stijn/PhD/Biplots"
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE,  warning=FALSE, message=FALSE, echo=FALSE, eval=FALSE, tidy = TRUE, fig.width=9, fig.height=6, root.dir=WD, purl=FALSE, fig.show = "hold")
# The required package list:
reqpkg <- c("phyloseq","MASS", "parallel","nleqslv", "edgeR", "VGAM","HMP", "ggplot2", "pscl", "vegan", "alabama")
# Load all required packages and show version
for(i in reqpkg)
{
#   print(i) 
#   print(packageVersion(i))
  library(i, quietly=TRUE, verbose=FALSE, warn.conflicts=FALSE, character.only=TRUE)
} 
if(file.exists("/home/stijn/PhD/American Gut/AGpars.RData")) {load("/home/stijn/PhD/American Gut/AGpars.RData")}
par(pty="s") #Make sure the biplots are square!
palStore = palette()
```

```{r setwd, eval=FALSE, echo=FALSE, purl = FALSE}
setwd(WD)
```

\setcounter{tocdepth}{2}
\tableofcontents

# Introduction

The aim of this notebook is to wrap the codes to fit the RC(M) model with the negative binomial, zero-inflated poisson and zero-inflated negative binomial models. Also an overview of the theory is given here.

The underlying motivation is to develop a visualization technique for the microbiome that does not suffer from the defects of currently applied techniques such as PCA, PCoA with Bray-Curtis distance etc. These methods are in particular sensitive to differences in library size.

# Theory

## Unconstrained analysis

### Correspondence analysis

Suppose we have a $nxp$ count data matrix $\mathbf{X}$ with $n$ samples and $p$ taxa, with $i$ and $j$ row respectively column indices.

#### Independence model

Under independence between rows and columns we model the counts in a contingency table as 

$$E(X_{ij}) = a_i b_j$$

whereby usually $a_i=x_{i.}=\sum_{j=1}^px_{ij}$ (the library sizes) and $b_j=\frac{x_{.j}}{x_{..}}=\frac{\sum_{i=1}^nx_{ij}}{\sum_{j=1}^p\sum_{i=1}^nx_{ij}}$ (the average relative abundances).

#### Reconsitution formula of Correspondence Analysis (CA)

A more extended model is

$$E(X_{ij}) = a_i  b_j + c_i d_j \sum_{k=1}^K \omega_k v_{ki} w_{jk}$$.

For $a_i = c_i = x_{i.}$ and $b_j = d_j = \frac{x_{.j}}{x_{..}}$ this is regular correspondence analysis. The second term in the equation represents deviation from independence. This is called the *reconstitution formula* since it decomposes the observed average count into its expectation under independence and a residual. The residual is then further decomposed into $k$ pieces. The expected count can then be written as

$$E(X_{ij}) = \frac{x_{i.}x_{.j}}{x_{..}} \big(1 + \sum_{k=1}^K \omega_k v_{ki} w_{jk}\big)$$.

Regular corrsepondence analysis is done through singular value decomposition(SVD) of the matrix of weighted (Pearson) residuals

$$R^{-1/2}(X-E)C^{-1/2}=U\Sigma V$$

with $\Sigma$ a diagonal matrix and $R$ and $C$ diagonal matrices with row and column sums.

In matrix notation the reconstitution formula becomes

$$X = E_{independence} + R^{1/2}U\Sigma VC^{1/2}$$

with $\mathbf{1'}R^{1/2}U=\mathbf{0}$ and $\mathbf{1'}C^{1/2}V=\mathbf{0}$ (weighted means equal zero) and $U'R^{1/2}U=\mathbf{1}$ and $V'C^{1/2}V=\mathbf{1}$ (weighted variances equal one) (see VanDerHeijden et al., 1985).

For plotting usually $\omega_1 \mathbf{v_{1}}$ is plotted versus $\omega_2 \mathbf{v_{2}}$ to correctly represent the distances between the samples. $\mathbf{w_{1}}$ and $\mathbf{w_{2}}$ can then be added to construct a biplot. The orthogonal projection of $\omega_1 v_{ki}$ on $w_{jk}$ then represents the departure from the independence model of taxon $j$ in sample $i$ in dimension $k$.

#### Skewness

Because of the skewness of the distribution, the residuals are also very skewed. If we have an expectation of around 1, it is very likely to observe a zero, but much less likely to observe a 2. Still both deviations are weighted equally. I think a residual-based approach is ill suited to asymmetric, skewed data. Also the library size correction will never be appropriate, so that part of the biplot will reflect differences in library sizes. We considered decomposing

$$R^{-1}(X-E)C^{-1}$$

in the past to account for the fact that the data are not Poisson ($Var(X) = E(X)$) but rather negative binomial ($Var(X) = E(X) + \phi*E(X)^2$) and thus the variance for large means proportional to the squared expectation. Even more general we considered

$$R^{-a}(X-E)C^{-b}$$

and estimate $a$ and $b$ by maximum likelihood but we did not really see how to go on from here.

### Hellinger distance

With PCoA, any distance metric can be used to produce a distance matrix, which can be decomposed to make biplots.

The Hellinger distance divides by the square root of the observation as means of normalization:

$$\frac{X}{\sqrt{Var(X)}} \approx \frac{X}{\sqrt{X}} = \sqrt{X}$$

The rationale is that each observation x is the best estimate of its expectation E(X), and under the Poisson model also of its variance. Still also these plots are susceptible to differences in library sizes.

### Log-linear analysis

Another modelling approach is to use log-linear modelling an thereby introduce the negative binomial as error term. We know from previous goodness of fit testing that this is an appropriate error model for microbiome data.

In log-linear analysis the logged expected count $l_{ij}$ is modelled as

$$log(E(x_{ij})) = l_{ij} = u + u_i + u_j + u_{ij}$$.

For $u_{ij} \neq 0$ this is the saturated model, which provides a perfect fit by setting all expected values equal to the observed ones. If $u_{ij} = 0$ this is the independence model presented above.

### The RC(M)-association model

For the purpose of making biplots, we might consider the following model (proposed by Leo Goodman, 1985. See also VanDerHeijden 1994)

$$log(E(x_{ij})) = l_{ij} = u + u_i + u_j + \psi_1r_{1i}s_{1j} + \psi_2r_{2i}s_{2j}$$

or in matrix form:

$$\mathbf{lE} = \mathbf{u_{sam,MLE}}\mathbf{u_{tax,MLE}}^t + \mathbf{r}_k \mathbf{\psi}_k \mathbf{s}_k^t$$

with $\mathbf{lE}$ the n-by-p matrix of logged expectations, $\mathbf{u_{sam,MLE}}$ the vector with length $n$ with the row intercepts estimated by ML, $\mathbf{u_{tax,MLE}}$ the vector of length $p$ with the column intercepts estimated by ML, $\mathbf{r}_k$ the n-by-k matrix of row scores, $\mathbf{\psi}_k$ a diagonal matrix with the $k$ $\psi$'s on the diagonal and $\mathbf{s}_k$ the p-by-k matrix of column scores

Constraints are needed to render this model identifiable:

Centering:

$$\sum_{i=1}^nw_ir_{ki} = 0$$

with k=1,2 and

Normalization($k=k'$) and orthogonality ($k \neq k'$)

$$\sum_{i=1}^nw_ir_{ki}r_{k'i} = I(k=k')$$

$$\sum_{j=1}^pz_js_{kj} = 0$$

$$\sum_{j=1}^pz_js_{kj}s_{k'j} = I(k=k')$$
Hereby $w_i$ and $z_j$ are row and column weigths.

#### RC(M)-weights

The weights can be regarded as probability density functions, they represent the likelihood of sampling a certain sample or taxon from the population. On the population level we could say that

$$E(w_iX_{ik}) = 0$$,

i.e. the average row score on the population level is zero. This is a useful restriction to make sure that the biplot is centered around zero. Analogously we want that

$$E(w_iX_{ik}X_{ik'}) = I(k=k')$$

and accordingly for the column scores. But which weights $w_i$ and $z_j$ should we use?

Goodman proposes to use $w_i = x_{i.}$ and $z_j = x_{.j}$ thus uses _weighted_ constraints, to retain the relationship with correspondence analysis (see below). Becker _et al._ (1989) recommend using uniform weights not to let the marginal distribution affect the model fit. 

For the microbiome case, every subject comes from the same population under the null-hypothesis and all subjects are thus eqaully likely to be sampled and have the same importance. The library sizes as a purely technical artefact, unrelated to the biological importance of the subject. Consequently we use uniform row weights $w_i = 1/n$ (or $w_i = 1$, the magnitude is of no importance since the associated $\psi_k$ will grow or shrink accordingly). However, some taxa are more prevalent in the population than others. We want the average column scores on the population level to be centered around zero, have variance one and be orthogonal. That is why we set

$$E(z_j S_{jk}) = 0$$

$$E(z_j S_{jk} S_{jk'}) = I(k=k')$$

Because the more abundant species are in fact more abundant in the population as a whole (as opposed to samples with a large library size) it makes sense to use a marginal weighting scheme for the column scores. The weights $z_j$ will be estimated by maximum likelihood, as explained in the next paragraph.

#### RC(M) margins

The most logical choices for the margins have long seemed to use $u=-log(x_{..})$, $u_i = log(x_{i.})$ and $u_j = log(x_{.j})$.

However, the library sizes do not correspond to the maximum likelihood estimate of $u_i$ under the Negative Binomial model. As a result the first dimensional row scores $r_{1i}$ tried to correct for this effect and became related (linearly correlated) to the library sizes, which we want to avoid absolutley. To avoid this we set $u=0$ and estimate the $u_i$'s, $u_j$'s and overdispersions iteratively using maximum likelihood in a first step (this converges very quickly). The main disadvantage I see of this approach is that the independence model becomes model dependent. Next we'll have to iterate between fitting the overdispersions, the imporance parameters $\psi$, the $r's$ and the $s's$. 

The centering, normalization and orthogonality of the row and column scores is enforced through Lagrange multipliers. This makes the systems of equations much harder to solve but assures independence of the dimensions.

Note that this is a case of __indirect gradient__ analysis: the dimensions are estimated without incorporating gradient information (measured covariates) to maximally represent variability in the data. In a next step these obtained gradients can be compared with measured covariates (often graphically through a colour code) to see how well these covariates explain the gradients. 

#### Relationship between CA and log-linear analysis

According to Escoufier, 1985 if $a =\sum_{k=1}^K \omega_k v_{ki} w_{jk}$ is small (i.e. the deviation from independence is small) then $log(1+a) \approx a$ and

$$log(E(x_{ij})) = log(x_{i.}) + log(x_{.j}) - log(x_{..}) + log\big(1 + \sum_{k=1}^K \omega_k v_{ki} w_{jk}\big) \approx log(x_{i.}) + log(x_{.j}) - log(x_{..}) + \sum_{k=1}^K \omega_k v_{ki} w_{jk}$$

i.e. an equivalence between the RC(M)-model and correspondence analysis

Since the same restricitions apply to the scores $v_{ki}$ and $w_{jk}$ as to U and V, we can state that  $\psi_k \approx \omega_k$. The assumption that the departure from independence seems unlikely in our case, but it does provide us with some starting values.

### Fitting the RC(M) model

The models are usually fitted by ML. See page 55 of Goodman 1985 for details. Goodman assumed a Poisson distribution, but it might be possible to use a negative binomial model with unique dispersions for each taxon $j$ as well.

See Goodman 1985 (appendix), Becker 1988 for information on estimation. The code below is generic but we take k=2 for simplicity.

Parameters of interest to estimate are $\psi$, $r_1$, $r_2$, $c_1$, $c_2$. In the process we'll also need the nuisance parameters $\mathbf{\theta}$ and the offsets of the independence model $u_i$ and $u_j$. Assume $E(X_{ij}) = \mu_{ij}$ and  $Var(X_{ij})=\mu_{ij} + \frac{\mu_{ij}^2}{\theta}$

#### The negative binomial density function

For the sake of completeness we give the density function of the negative binomial distribution (see Lawless, 1987) in the ($\mu$, $\theta$) parametrization whereby $E(X_{ij}) = \mu_{ij}$ and $Var(X_{ij}) = \mu_{ij} + \frac{\mu_{ij}^2}{\theta_j}$ .

$$f_{NB}(X_{ij}) = \frac{\Gamma(X_{ij}+\theta_j)}{\Gamma(\theta_j)} \big(\frac{\mu_{ij}}{\theta_j+\mu_{ij}}\big)^{X_{ij}} \big(\frac{\theta_j}{\theta_j+\mu_{ij}}\big)^{\theta_j}$$

#### Starting values

1. Obtain a singular value decomposition as $R^{-c}(X-E)C^{-d} = U\Sigma V$. This gives us initial values $[r_{11}^{init}, r_{12}^{init},..., r_{1i}^{init}] = U_{(1.)}$, the first row of $U$, and correspondingly $\Sigma$ and $V$ give initial values for $\mathbf{\psi}$ and $\mathbf{s}$. The values for $c$ and $d$ depend ont he weighting scheme used, e.g. if the sample weights are uniform ($w_i=1/n$) and the taxon weights marginal($z_j = x_{.j}$) then $c=1$ and $d=0$. This makes sure the initial centering conditions are fulfilled.

We still need to ensure that the (weighted) variances equal 1, so we set

$$r_{ki}^{init} = \big(\frac{w_ir_{ki}^{SVD}}{\sum_{i=1}^n{r^{SVD}_{ki}}^2w_i}\big)^{1/2}$$

and

$$s_{ik}^{init} = \big(\frac{z_js_{ik}^{SVD}}{\sum_{i=1}^n{s^{SVD}_{ik}}^2z_j}\big)^{1/2}$$

#### Iteration

2. If the independence model is estimated by MLE, estimate the $u_i$'s, $u_j$'s and $\theta_j$'s iteratively until convergence (set $u=0$). The estimating equeations are independent (Jacobian is diagonal) so this converges very quickly. Otherwise set $u_i = log(x_{i.})$ , $u_j = log(x_{.j})$ and $u = log(x_{..})$.

3. Estimate overdispersions

Theoretically we might estimate the dispersions by ML, but the estimates are very variable, which dramatically affects the rest of the fitting procedure.

 - Maximum likelihood estimation (not used) 
 
$$log(E(x_{ij})) = l_{ij} = offset\big(log(\frac{x_{i.}x_{.j}}{x_{..}}  \big) + \psi_1 (s_{1i}^{init}r_{1j}^{init}) + \psi_2 (s_{2i}^{init}r_{2j}^{init})\big)$$

to obtain estimates for the overdispersions $\theta_j$, whereby the means $\mu_{ij}$ are assumed known

 $$\sum_{i=1}^n \sum^{y_i^*}_{t=0} \big( \frac{t}{1+\frac{t}{\theta_j} } \big) + \theta_j^2 log(1+\frac{\mu_{ij}}{\theta_j})- \mu_{ij}\frac{y_{ij}+\theta_j}{1+\frac{\mu_{ij}}{\theta_j} }$$
 
 For this we can use the theta.ml() function in the _MASS_ package
 
 - edgeR robust
 
To get more stable estimates of the dispersions, RNA-seq packages such as _DESeq_ and _edgeR_ use an empirical Bayes approach to shrink dispersion estimates towards a fitted mean-dispersion trend. Here I use the machinery of edgeR robust (version `r packageVersion("edgeR")`) to estimate the dispersions
 
To reduce the computational cost and because the estimates do not change a lot anyway the estimation of the overdispersions is not repeated in every iteration

4. Estimate the psi parameters
 
 Solve the system of score equations (see Lawless 1987) for $\psi_1$ and $\psi_2$, assuming $\theta_j$'s given
 
 $$\sum_{i=1}^n \sum_{j=1}^p r_{im}s_{mj} \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}} = 0$$ for all m=1,...,k
 
 with
 
 $$E(X_{ij}) = \mu_{ij} = exp\big(log(\frac{x_{i.}x{.j}}{x_{..}}) + \psi_1^{MLE} s_{1j}^{init}r_{1i}^{init} + \psi_2^{MLE} s_{2j}^{init}r_{2i}^{init} \big)$$.
 
 These are non-linear equations for which we use the _nleqslv_ package, with its default the Broyden method. This is a variation of the Newton-Raphson method (or Fisher scoring in case of ML) whereby the Jacobian is recycled and not recalculated at every iteration. The Jacobian is a square matrix of dimension k*(k-1)/2. The diagonal entry at position $kk'$ is:
 
 $$\frac{\delta^2 L_{NB}(\mu, \theta)}{\delta \psi_k \delta \psi_{k'}} = -\sum_{i=1}^n \sum_{j=1}^p r_{ik}r_{ik'}s_{kj}s_{k'j}\frac{\mu_{ij}(1+\frac{x_{ij}}{\theta_j})}{(1+\frac{\mu_{ij}}{\theta_j})^2}$$
 
5. Estimate the row scores. 

To estimate the $r_{i}$'s we would like to use weighted least squares to minimize

$$\sum_{j=1}^p\Big(log(x_{ij}) - \big(u + u_i + u_j +\sum_{k=1}^M r_{ik} \big( \hat{\psi_ks_{jk}} \big)\Big)^2$$

for every $i$. Unfortunately we cannot calculate $log(x_{ij})$ because of the zeroes.

We'll have to use another way to estimate them, maximum likelihood in our case. I don't like using weighted least squares on the non-transformed counts,

$$\sum_{j=1}^p\Big(x_{ij} - exp\big(u + u_i + u_j +\sum_{k=1}^M r_{ik} \big( \hat{\psi_ks_{jk}} \big)\Big)^2,$$

because of the skewness of the residuals as explained above.

To enforce the constraints on the row scores one option would be to estimate all scores through full maximum likelihood and then modify a few of them to make them satisfy the constraints and repeat until convergence. This runs into numerical problems though, because the modified scores are usually very large initially which leads to overflow when exponenttiated. Instead we use the methods of the Lagrange multipliers to implement the constraints. We thus seek to maximize the Lagrangian function

$$Lag(r_{ik}, \mathbf{\lambda}) = L_{NB}(\mathbf{r}) + \sum_{k=1}^M \big( \lambda_{1k} \sum_{i=1}^n w_i r_{ik} \big) + \sum_{k=1}^M \big( \lambda_{2k} ( \sum_{i=1}^n w_i r_{ik}^2 ) - 1 \big) + \sum_{k \neq k'} \big(\lambda_{3kk'} \sum_{i=1}^n (w_ir_{ik}r_{ik'}) \big)$$

with $L_{NB}(\mathbf{r})$ the log-likelihood function of the negative binomial regression. The derivatives of this function for the following system of equations

$$\frac{\partial Lag(r_{ik}, \mathbf{\lambda})}{\partial r_{ik}} = \sum_{j=1}^p \hat{s}_{jk} \hat{\psi}_k \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}} + \sum_{k=1}^M w_i \lambda_{1k}  + \sum_{k=1}^M \big(2 \lambda_{2k} w_ir_{ik}\big) +  \sum_{k' \neq k} w_ir_{ki} \lambda_{kk'} = 0$$

and 

$$\frac{\partial Lag(r_{ik}, \mathbf{\lambda})}{\partial \lambda_{1k}} = \sum_{i=1}^n w_i r_{ik} = 0$$

and 

$$\frac{\partial Lag(r_{ik}, \mathbf{\lambda})}{\partial \lambda_{2k}} = (\sum_{i=1}^n w_i r_{ik}^2) - 1 = 0$$

and 

$$\frac{\partial Lag(r_{ik}, \mathbf{\lambda}))}{\partial \lambda_{3kk'}} = (\sum_{i=1}^n w_i r_{ik}r_{ik'}) - 1 = 0$$

. The latter three groups of equations represent the original centering, normalization and orthogonality constraints. 

There are additional equations in the system now, but also as many extra $\lambda$ parameter to optimize. These $\lambda$ parameters are also stored and used as initial values in the next iteration run. There size represent the extent to which the constraints pull away the solution from the optimal, unconstrained solution, but I don't see how we can use that in a biologically or statistically meaningful way.

We now truly have a system of equations to solve, which we do with the _nleqslv_ package. It is straightforward but a bit cumbersome to also provide the analytical Jacobian, which greatly speeds up the iterations (otherwise it has to be approximated numerically). The Jacobian is sparse and symmetric. with following non-zero entries:

 $$\frac{\delta^2 Lag(\mu, \theta)}{ \delta r_{ik} \delta r_{ik'}} = -\sum_{i=1}^n \sum_{j=1}^p \psi_{k}\psi_{k'}s_{kj}s_{k'j}\frac{\mu_{ij}(1+\frac{x_{ij}}{\theta_j})}{(1+\frac{\mu_{ij}}{\theta_j})^2}=0$$
 
  $$\frac{\delta^2 Lag(\mu, \theta)}{\delta r_{ik} \delta \lambda_{1k}} = w_i$$
  
  $$\frac{\delta^2 Lag(\mu, \theta)}{\delta r_{ik} \delta \lambda_{2k}} = 2w_ir_{ik}$$
  
  $$\frac{\delta^2 Lag(\mu, \theta)}{\delta r_{ik} \delta \lambda_{3kk'}} = w_ir_{ik'}$$
  
  All other entries are zero.
  
6. Estimate the column scores

Repeat step 4 but now estimate $s_{jk}$ column scores in the restrictions, e.g. the Lagrangian becomes:

$$Lag(s_{jk}, \mathbf{\lambda}) = L_{NB}(s_{jk}) + \sum_{k=1}^M \big( \lambda_{1k} \sum_{j=1}^p z_j s_{jk} \big) + \sum_{k=1}^M \big( \lambda_{2k}  ( \sum_{j=1}^p z_js_{jk}^2 ) - 1 \big) + \sum_{k \neq k'} \big(\lambda_{3kk'} \sum_{j=1}^p (z_js_{jk}s_{jk'}) - 1\big)$$

#### Termination

7. Repeat steps 3-6 until convergence. Convergence is assumed if between two iterations

 - None of the $\psi$ parameters change less than $0.01\%$ (infinity norm)
 
 AND
 
 - None of the square roots of the L2-norms of the relative changes of row and column scores exceeds 0.1%, or
 
 $$\forall \mathbf{r_k}, \mathbf{s_k} \Big(\sqrt{\sum_{i=1}^n (1-\frac{r^{(iter-1)}_{ik}}{r^{(iter)}_{ik}})^2} < 0.001 \Big) \cup \Big( \sqrt{\sum_{j=1}^p (1-\frac{s^{(iter-1)}_{kj}}{s^{(iter)}_{kj}})^2} < 0.001 \Big)$$
 
 and analogously for all the scores. Other norms, such as L1 or infinity norms can also be used. A norm between 1 and 2 will lead to fast convergence.
 
 Overdispersions are not taken into account in determining the convergence since they're merely nuisance parameters.

#### Biplot

8. Plot $\psi_1r_{1i}$ vs $\psi_2r_{2i}$ and add $s_{1j}$ vs $s_{2j}$ as arrows to make a biplot. This makes sure the distnaces between the samples are represented correctly.

In the end we'll have estimated  p (abundances) + n (library sizes) + p (dispersions) + kxp (column scores) + kxn (row scores) + k (importance parameters) = (k+2)p + (k+1)xn + k parameters out of np entries. We have imposed 4k + k(k-1) restrictions, so the final model is still very parsimonious for n and p in the hundreds.

##### Model quality

9. Assess the goodness of fit

Since the model is overparametrized, classical ML theory (such as asymptotic behavious of maximum likelihood statistics) does not apply to our solution. Still we can compare the likelihoods of the independence, the RC(2) and the intermediate models to get an idea of the relative importance of the dimensions. The same concept exists for correspondence analysis, where the size of the diagonal elements of $\Sigma$ is proportional to the importance of the corresponding dimension. The log-likelihood of the saturated model is by definition equal to 0.

We can decompose the difference in log-likelihood between the independence and the full RC(K) model as

$$(LL_2 - LL_0) = (LL_1 - LL_0) + (LL_2 - LL_1)$$

with $LL_0$, $LL_1$ and $LL_2$ -2 the log-likelihoods of the independence, RC(1) and RC(2) models respectively. Scaling by $(LL_2 - LL_0)$ will provide interpretable fractions.

### One-by-one (1B1)

Initially all dimension were estimated jointly, but for reasons of speed we moved to a one-by-one approach estimating first the first dimension, then the second given the first, then the third given the first two and so on. By splitting the estimation procedures into smaller parts like this the estimation speeds up considerably, presumably because the systems are easier to solve.

### Confounders

Sometimes some covariates are known to affect the abundances but one is not interested in their effect. We think first and foremost of technical parameters such as sequencing center or technology. Thanks to our log-linear approach we can filter out the effect of these parameters by including interaction terms between the taxa and the confounding covariate to be filtered out. This willl occcur after fitting the independence model but before fitting the RC(M) component. We thus fit the following model in case of $c$ confounding variables:

 $$log(E(X_{ij})) = \lambda + \lambda_i + \lambda_j + \textcolor{red}{\sum_{l=1}^c \zeta_{jl}e_{il}} + \sum_{k=1}^M \psi_k r_{ik} s_{jk}$$
 
with $e_{il}$ the value of covariate $l$ in sample $i$ and $\zeta_{jl}$ the interaction parameter between taxon $j$ and confounding variable $l$. Note that we assume that the categorical variables have already been converted to multiple dummy variables.

#### Confounders and zero counts

Filtering out the effect of confounders brings the problems of the zeroes to the fore again. If a taxon has only zero counts in one of the subgroups defined by the confounding variables, the model will fail to fit again. For continuous confounders this is of course not an issue. Maybe there exists a more efficient solution, but for now we will have to trim the taxa for which this occurs from further analysis in order to render the model identifiable. This trimming best occurs already before fitting the independence model.

#### Implementation

For the filtering based on the prevalence and abundance, we need a confounder matrix

 1. Without intercept (Overall filtering has happened already)
 2. With __all__ levels of the categorical variables, so set $contrasts=FALSE$ to avoid hidden reference levels of the factors
 
 When actually filtering on the confounders, we can simply use a confounder matrix
 
 1. With intercept
 2. With treatment coding
 
 In this case all that matters is to change the offset formed by the independence model. We will still return the confoundes' parameters though.

### Zero-inflated poisson

We could also augment our model with a zero-inflated Poisson rather than a negative binomial distribution. This opens additional modeling perspectives. We can model the chance on a structural zeroes:

$$logit(P(X_{ij}=0)) = f + f_i + f_j + t_{i1}v_{j1}\chi_1 + t_{i2}v_{j2}\chi_2$$.

The probability of a strucutral zero does not depend on the sample: strucutural zeroes are assumed to be truly absent species and not due to undersampling. All samples come from the same population under $H_0$, and there the structural zeroes are only dependent on the columns. Therefor we set all $f_i=0$. In practice $f$ is not estimated either, instead $f_j$ is estimated without restrictions. The terms $t_{i1}v_{j1}\chi_1 + t_{i2}v_{j2}\chi_2$ indicate in which two independent directions the observed number of zeroes deviates from this base level. I think we need the term $f_i$ because otherwise the first dimension $t_{i1}$ scores will start correcting for it. The independence moel in this case is that the chance on a structural zero only depends on the marginal probabilities. A logit model without an intercept does not make sense, it assumes a baseline probability of structural zero of 0.5.

#### Restrictions

The same restrictions apply to the scores $t_i$ and $v_j$ as to $r_i$ and $s_j$

#### Estimation (see Lambert 1992 for details)

The mean and zero probability are modelled independently, absence of a species is independent of its abundance when it is present. Say $Z=0$ when $X$ is from the Poisson state and $Z=1$ when X is a structural zero. Evidently, when $X=0$, Z is unknown.

EM algorithm: iterate between 

 - E: estimate Z, assuming  $\mathbf{r_i}$, $\mathbf{s_j}$ and $\mathbf{\Psi}$ known, through its expectation
 - M: Maximize the log-likelihood given Z
 
Unlike the RC(2)NB model, $f_j$ has to be estimated as well, there is no obvious candidate here. We estimate it marginally and then keep it fixed during the iterations. This way the score visualize departures in zero inflation from this independence of zero-inflation between rows and columns.

Newton-Raphson would be incredibly complicated here, the derivatives of the log-likelihood functions barely fit on a page!

## Constrained analysis

So far we have been doing unconstrained or unsupervised explorative visualization, or _indirect_ gradient analysis. We just visualized the strongest signals in the data, independent of sample covariates. Only then did we use the covariate information to try to correlate the covariates with the latent variables formed by the row and column scores. We usually do this by adding a colour code for the samples in our biplots.

A logical next step is to develop a constrained method in the same framework, i.e. with the same error distribution. We argued that PCoA with Bray-Curtis distance or regular correspondence analysis did not perform well, and we expect the same for canonical correspondence analysis since it relies on the same distributional assumptions. We will visualize those departures from the independence model that can be explained by the sample covariates. Each dimension will thus consist of a linear combination of sample covariates, the weight of each of these covariates in the dimensions reflecting how much they contribute to explaining the variation. For background information see also Zhu _et al._ , 2005, and Zhang and Thas, 2012.

We call our n-by-d covariate matrix $C$, for every sample $i$, $d$ baseline covariates are measured. These covariates can be either discrete or continuous. $c_{iy}$ is the observed value of covariate $y$ in sample $i$, $\mathbf{c_{i.}}$ the whole covariate vector of length $d$ of sample $i$, and $\mathbf{c_{.y}}$ all recorded values of variable $y$.

### Constrained RC(M) model

I can see multiple possible modelling options

1) The classical approach

This model comes from ecology and is the one used by Zhu _et al._ (2005) and Zhang and Thas (2012 and 2016). This assumes each taxon has an optimal combination of covariates, and its expected abundance decreases as the covariates depart from this optimal combination. The expected abundance follows a Gaussian distribution in function of this departure.

The expected counts are modelled as 

$$log(E(X_{ij})) = \sum_{k=1}^K \big(a_{jk} - \frac{(h_{ik} - q_{jk})^2}{2m_{jk}^2}\big)$$

whereby 

 - $h_{ik} = \boldsymbol{\alpha}_k^t \mathbf{c}_{i.}$ the optimal linear combination of $\mathbf{c}_{i.}$ in dimension $k$
 - $\boldsymbol{\alpha}_k$ is called the _environmental gradient_
 - $\alpha_{ky}$ is element corresponding to covariate $y$ of the environmental gradient in dimension $k$
 - $a_{jk}$ is the maximal logged expected abundance achieved when the linear combination of covariates $\boldsymbol{\alpha}_k^t \mathbf{c}_{i.}$ equals the optimum $q_{jk}$
 - $m_{jk}$ is the tolerance, it determines how fast the expected value decreases as $\mathbf{c}_{i.}$ moves away from its optimal values $\boldsymbol{\alpha}_k^{-t}h_{ik}$
 
 The scales of $h_{ik}$, $q_{jk}$ and $m_{jk}$ are undefined, so we need an additional restriction to render the model identifiable. Zhu _et al._, 2005 propose setting $\sum_{j=1}^p m_{jk}^2/p=1$. Note that $a_{jk}$, $q_{jk}$ and $m_{jk}$ are species specific parameters whereas $\boldsymbol{\alpha}_k$ is specific for the dimension but common to all species.

The estimation iterates between estimating $\boldsymbol{\alpha}_k$ and $\boldsymbol{\beta}_{jk} = \big(a_{jk}, q_{jk}, m_{jk})$.

This model was designed with the ecological context in mind, whereby (continuous) environmental variables such as pH, temperature, sunlight intensity, ion concentrations, distance to landscape elements etc. were measured. However for microbiome data, we usually do not have direct measurements of variables in the ecosystem, but rather characterstics of the patient, which are very often categorical variables. Even for a continuous variable like age the Gaussian assumption seems weird to me. Keeping the other covariates fixed, this would mean there is an optimal age for bacteria to grow and their expected abundance would decrease for older or younger people. This may or may not be a realistic model.

Let's take a look at the nature of the covariate data.

```{r Nature of the covariates, eval=TRUE, purl=FALSE}
load("/home/stijn/PhD/American Gut/AGphylo.RData")
load("/home/stijn/PhD/Simulations/data/physeqList4Trim.RData")
load("/home/stijn/PhD/Simulations/data/zellerData.RData")
load("/home/stijn/PhD/Biplots/Kostic_data/phyloD.RData")
tableSamVar = function(x){
  table(sapply(sample_data(x), class))
}
phyList = list("American gut"=AGphylo, "Kostic"=phyloD, "HMP" = physeqList4Trim[["Throat"]], "zeller16S" = zellerSphy, "zellerMetagenomics" = zellerMphy)
sapply(phyList, tableSamVar)
```

Most variables are factors or booleans. Below are the names of the variables shown:

```{r covariate names, purl=FALSE}
sapply(phyList, sample_variables)
```

I wonder how realistic the Gaussian response model still is with so many categorical variables and so many species. I guess we would best seek some expert advice of microbiome scientists.

2) Classical approach: RC(M) variation

If we want to stay in the RC(M) framework we can also model the mean as:

$$log(E(X_{ij})) =  u + u_i + u_j  + \sum_{k=1}^K \big(a_{jk} - \frac{(h_{ik} - q_{jk})^2}{2m_{jk}^2}\big)$$

This approach differs from the one proposed in Zhu _et al._, 2005 in the intercepts $u_i$ and $u_j$. These intercepts will still be estimated by maximum likelihood in the first step, just as in the unconstrained case. We do not continue on this track.

3) Constrained RC(M)

A third option would be to drop the whole Gaussian assumption and build a model as follows:

$$log(E(X_{ij})) = u + u_i + u_j  + \sum_{k=1}^K \psi_k s_{jk} f_j(\boldsymbol{\alpha}_k^t \mathbf{c}_{i.}) $$

with $f_j$ the species-specific response function, which can be linear, quadratic (Gaussian on count scale), non-parametric, ... . The usual restrictions apply to the column scores $\mathbf{s}$:

$$\mathbf{s^tu_{tax}} = \mathbf{0}$$
$$\mathbf{s_k^tUs_{k'}} = I(k=k')$$

with $u_{tax}$ the vector of $exp(u_j)$'s and  $\mathbf{U}$ a diagonal p-by-p matrix with $u_{tax}$ on the diagonal.

For $\alpha$ (a d-by-k matrix) we enforce the following restrictions:

- Centering is desirable for the triplot (if $k \geq 2$)

$$\boldsymbol{\alpha}^t\mathbf{1} = \mathbf{0}$$
- Normalization restricts the size, only $\psi_k$ can grow in magnitude (if $k \geq 3$)

$$\boldsymbol{\alpha}^t\boldsymbol{\alpha} = \mathbb{1}_k$$

with $\mathbf{1}$ a vector of ones of length $d$ and $\mathbb{1}_k$ the identity matrix of dimension $k$.

### Response function

For the response function previously a Gaussian has been proposed but this may not be realistic. Also the setting is different: we are looking for response function that maximize the __departure from independence__, no longer the abundances themselves! In a first stage we will try fit the linear model with $f(x) = \beta_0 + \beta_1 x$ but later we might estimate $f$ non-parametrically as in Zhu _et al._ (2005). For any response function with $q$ parameters $\boldsymbol{\beta}_j$ for every taxon $j$ the mean equals:

$$log(E(X_{ij})) = log(\mu_{ij}) = u_i + u_j + \sum_{k=1}^K \Big(\psi_k s_{jk} f_j \big( \boldsymbol{\alpha}_k^t \mathbf{C}_{i.}|\boldsymbol{\beta}_j \big) \Big).$$

Note that the concept of a "row score" is lost here: since every taxon reacts differently to changes in the environment $C_{i.}$. Still this will not yield a perfect fit since $\boldsymbol{\alpha}_k^t$ is only unique to the dimension and the number of parameters estimated per response function $f_j$ is much smaller than the number of samples $n$. In practice in R we will work with a row score matrix

#### Score equations

The score function for $\beta_{jv}$ under the negative binomial model with $v$ a parameter index running along $\boldsymbol{\beta}_j$ (for k=1 and omitting dimension indices)

$$ \frac{\partial l_{NB}(X_{.j}|\boldsymbol{\beta}_j, \theta_j)}{\partial \beta_{jv}} = \sum_{i=1}^n \psi s_j \frac{\partial f_j \big( \boldsymbol{\alpha}^t \mathbf{C}_{i.}|\boldsymbol{\beta}_j \big)}{\partial \beta_{jv}} \frac{x_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}}$$

In the linear case (when $f \big( \boldsymbol{\alpha}^t \mathbf{C}_{i.}|\boldsymbol{\beta}_j \big) = \beta_0 + \beta_1 \boldsymbol{\alpha}^t \mathbf{C}_{i.}$) this becomes:

$$ \frac{\partial l_{NB}(X_{.j}|\boldsymbol{\beta}_j, \theta_j)}{\partial \beta_{j1}} = \sum_{i=1}^n \psi s_j \boldsymbol{\alpha}^t \mathbf{C}_{i.} \frac{x_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}}$$

The second order derivative in its general form is just horrible:

$$ \frac{\partial^2 l_{NB}(X_{.j}|\boldsymbol{\beta}_j, \theta_j)}{\partial \beta_{jv} \partial \beta_{jv'}} = \sum_{i=1}^n \frac{\psi s_j}{1+\frac{\mu_{ij}}{\theta_j}} \Big( \frac{\partial^2 f_j \big( \boldsymbol{\alpha}^t \mathbf{C}_{i.}|\boldsymbol{\beta}_j \big)}{\partial \beta_{vj} \partial \beta_{v'j}} (x_{ij} - \mu_{ij}) + \frac{\partial f_j \big( \boldsymbol{\alpha}^t \mathbf{C}_{i.}|\boldsymbol{\beta}_j \big)}{\partial \beta_{vj}} \frac{\partial f_j \big( \boldsymbol{\alpha}^t \mathbf{C}_{i.}|\boldsymbol{\beta}_j \big)}{\partial \beta_{v'j}} \psi s_j \big(\frac{\mu_{ij}(1-\theta_j(1+\mu_{ij}))-x_{ij}}{\theta_j+\mu_{ij}} \big)  \Big)$$

In the linear case this simplifies to:

$$ \frac{\partial^2 l_{NB}(X_{.j}|\boldsymbol{\beta}_j, \theta_j)}{\partial \beta_{j1}^2 } = \sum_{i=1}^n - \frac{(\psi s_j \boldsymbol{\alpha}^t \mathbf{C}_{i.})^2 \mu_{ij} (1+ \frac{x_{ij}}{\theta_j})}{(1+\frac{\mu_{ij}}{\theta_j})^2} $$

$$ \frac{\partial^2 l_{NB}(X_{.j}|\boldsymbol{\beta}_j, \theta_j)}{\partial \beta_{j1} \partial \beta_{j0} } = \sum_{i=1}^n - \frac{(\psi s_j \boldsymbol{\alpha}^t)^2 \mathbf{C}_{i.} \mu_{ij} (1+ \frac{x_{ij}}{\theta_j})}{(1+\frac{\mu_{ij}}{\theta_j})^2} $$

$$ \frac{\partial^2 l_{NB}(X_{.j}|\boldsymbol{\beta}_j, \theta_j)}{\partial \beta_{j0}^2 } = \sum_{i=1}^n - \frac{(\psi s_j )^2 \mu_{ij} (1+ \frac{x_{ij}}{\theta_j})}{(1+\frac{\mu_{ij}}{\theta_j})^2}$$

Except for the linear case, we will just calculate these numerically in practice, which is necessary anyway when $f$ is non-parametric.

### Constrained RC(M): Estimation

We would again first estimate the independence model ($u_i$ and $u_j$) as before, and then estimate one dimension at the time, conditional on the previous ones. If desirable, also the offset can be modified to eliminate some confounding variables. $\mathbf{\psi}$ and $\mathbf{s}$ will be estimated as before. $\boldsymbol{\alpha}$ could be estimated by ML as before, but in order to ensure maximum separation of the response functions of all the species we will consider a version of the log-likelihood ratio approach from Zhu _et al._ (2005):

$$LR(\boldsymbol{\alpha}) = log \frac{\prod_{i=1}^n \prod_{j=1}^p \big(p_j^{(\alpha)}(\boldsymbol{\alpha}^T \mathbf{c}_i) \big)^{x_{ij}}}{\prod_{i=1}^n \prod_{j=1}^p \big(p^{(\alpha)}(\boldsymbol{\alpha}^T \mathbf{c}_i) \big)^{x_{ij}}}$$

with $p^{(\alpha)}$ and $p^{(\alpha)}_j$ estimated probability density functions without and with taxon labels. $p^{(\alpha)}$ and $p^{(\alpha)}_j$ are calculated as the expected counts under a certain error distribution (e.g. negative binomial) whereby $f_j()$ and $f_0()$ are estimated given $\boldsymbol{\alpha}$ : 
$$p_j^{(\alpha)}(\boldsymbol{\alpha}^T \mathbf{c}_i) = e_{ij} = \exp\Big[u_i + u_j + \sum_{k=1}^K \Big(\psi_k s_{jk} f_j \big( \boldsymbol{\alpha}_k^t \mathbf{C}_{i.}|\boldsymbol{\beta}_j \big) \Big) \Big] $$ 
and 

$$p^{(\alpha)}(\boldsymbol{\alpha}^T \mathbf{c}_i) = e_{ij0} = \exp\Big[u_i + u_j + \sum_{k=1}^K \Big(\psi_k s_{jk} f \big( \boldsymbol{\alpha}_k^t \mathbf{C}_{i.}|\boldsymbol{\beta} \big) \Big) \Big] $$ 

(ignoring taxon labels when estimating $f()$).

The likelihood ratio then equals

$$LR(\boldsymbol{\alpha}) = \sum_{i=1}^n \sum_{j=1}^p log(e_{ij})x_{ij}-\sum_{i=1}^n \sum_{j=1}^p log(e_{ij0})x_{ij}$$

$$LR(\boldsymbol{\alpha}) = \sum_{i=1}^n \sum_{j=1}^p log(e_{ij})x_{ij}-\sum_{i=1}^n \sum_{j=1}^p log(e_{ij0})x_{ij}$$

Estimating $p^{(\alpha)}_k$ non-parametrically in this iterative algorithm may be computationally unfeasible.

### Estimating the environmental gradient

#### Naive approach

The naive approach to optimize $LR(\alpha)$ would be to use constrained optimization (constraining the $\alpha$'s to be zero-centered etc.) and estimate $p_j^{(\alpha)}$ and $p^{(\alpha)}$ in every evaluation of this function. This implies a analytical jacobian cannot be estimated and requires a huge amount of re-estimation of the response functions. This naive approach is computationally unfeasible

#### Iterative approach

A smarter approach is to iterate between estimating $\alpha$ by optimizing $LR(\alpha)$ given $p_j^{(\alpha)}$ and $p^{(\alpha)}$, and estimating $p_j^{(\alpha)}$ and $p^{(\alpha)}$ given $\alpha$. This results in much fewer estimations of $p_j^{(\alpha)}$ and $p^{(\alpha)}$ and allows to optimize $LR(\alpha)$ by finding the roots of $\frac{\partial LR(\alpha)}{\partial \alpha} = 0$ (and analytically specifying the second order derivative).

Convergence is assumed when every single change in $\alpha$ is below a tolerance level (infinity norm).

In case of linear response functions, the likelihood ratio then equals (dropping the k-subscripts)

$$ LR(\boldsymbol{\alpha}| \boldsymbol{\beta}_{j}, \boldsymbol{\beta}) = \sum_{i=1}^n \sum_{j=1}^p \psi s_{j}x_{ij} \Big(\mathbf{C}_i \boldsymbol{\alpha} (\beta_{1j} - \beta_{1}) + (\beta_{0j} - \beta_{0}) \Big)$$

Taking into account the restrictions mentioned above, the associated Lagrangian becomes

$$ Lag_{LR}(\boldsymbol{\alpha}_k| \boldsymbol{\beta}_{jk}, \boldsymbol{\beta}_{k}) =  \psi_k \Big(\sum_{i=1}^n  \sum_{j=1}^p s_{j}x_{ij} \big(\mathbf{C}_i \boldsymbol{\alpha} (\beta_{1j} - \beta_{1}) + (\beta_{0j} - \beta_{0}) \big) \Big) + \lambda_{1k} \boldsymbol{\alpha}_k^t\mathbf{1} + \lambda_{2k} (\boldsymbol{\alpha}_k^t \boldsymbol{\alpha}_k - 1) + \boldsymbol{\lambda}_{orth,k} \boldsymbol{\alpha}_k^t \boldsymbol{\alpha}_{-k}$$

with $\boldsymbol{\alpha}_{-k}$ the $\boldsymbol{\alpha}$-matrix from the first to the (k-1)th column.

The first order derivatives become

$$\frac{\partial Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k} = \psi_k \Big( \sum_{i=1}^n  \sum_{j=1}^p s_{jk}x_{ij} \mathbf{C}_i (\beta_{1j} - \beta_{1}) \Big) + \lambda_{1k}\mathbf{1} + 2\lambda_{2k} \boldsymbol{\alpha}_k + \boldsymbol{\lambda}_{orth,k} \boldsymbol{\alpha}_{-k} = \mathbf{0}$$

$$\frac{\partial Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \lambda_{1k}} = \boldsymbol{\alpha}_k^t\mathbf{1} = 0$$
$$\frac{\partial Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \lambda_{2k}} = \boldsymbol{\alpha}_k^t \boldsymbol{\alpha}_k - 1 = 0$$
$$\frac{\partial Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \lambda_{3k}} = \boldsymbol{\alpha}_k^t \boldsymbol{\alpha}_{-k} = 0$$

The second order derivatives are

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k^2} =  2\lambda_{2k} $$

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k \partial \boldsymbol{\alpha}_{k'}} =  \boldsymbol{\lambda}_{orth,k}$$

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k \partial \lambda_{1k}} =  1$$

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k \partial \lambda_{2k}} =  2\boldsymbol{\alpha}_k$$

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_{k} \partial \boldsymbol{\lambda}_{orth,k}} =  \boldsymbol{\alpha}_{-k}$$
All other second order derivatives equal zero. 

#### Algorithm

The estimation would then proceed as follows:

 1. Estimate the independence model ($u_i$'s, and $u_j$'s)
 2. (Optional): Filter out confounders by modifying offset
 3. Find initial values for $\psi_k$ and $s_{jk}$ as before.
 4. Find starting values for $\alpha$ based on CCA
 5. Iterate between
  - Estimate $f_j$'s and $f_0$ and corresponding $e_{ij}$ and $e_{i.}$ given $\boldsymbol{\alpha}$
  - Find $\boldsymbol{\alpha}$ by optimizing $LR(\boldsymbol{\alpha})$
  6. Estimate $\psi_k$ and $s_{jk}$ using ML and overdispersions using empirical Bayes
7. Repeat steps 4-6 until convergence

### Restricitions

The $\boldsymbol{\alpha}$'s of different dimensions will be constrained as follows:

$$\boldsymbol{\alpha}_k^t\mathbf{1} = \mathbf{0}$$

$$\boldsymbol{\alpha}_k^t\boldsymbol{\alpha}_k = \mathbf{1}$$

$$\boldsymbol{\alpha_k}^t\boldsymbol{\alpha_{k'}} = 0$$
#### Restricitons: Categorical variables

For continuous variables the previous discussion suffises, but for categorical variables special attention is required. For fitting the model we will use _treatment coding_ for simplicity. However, the above centering, normalization and orthogonality constraints should apply to all of the parameters, so using we will need a coding scheme that includes all parameters.

Say we have $d_{cont}$ continuous covariates and $d_{cat}$ categorical variables with $d = d_{cont} + d_{cat}$, and $m_l$ levels for every categorical variable $l$. Then $\boldsymbol{\alpha}_k$ compromises

$$ q = d_{cont} + \sum_{l=1}^{d_{cat}} (m_l-1) $$
rows, or $q$ parameters to be estimated. Note that there is no intercept needed in this case: This will be included in the $f()$ function.  We can write $\boldsymbol{\alpha}_k$ then as:

$$\boldsymbol{\alpha}_k = (\beta_{cont_1}, ... , \beta_{cont_{d_{cont}}}, \beta_{cat_{11}}, \beta_{cat_{12}}, ..., \beta_{cat_{1(m_1-1)}}, ...,  \beta_{cat_{d_{cat}(m_{d_{cat}}-1)}})_k$$
For imposing the restrictions and for plotting we want a representation whereby all covariate levels are represented so that the parameters sum to 0 within each covariate.

$$\boldsymbol{\alpha}_k' = (\beta_{cont_1}', ... , \beta_{cont_{d_{cont}}}', \beta_{cat_{11}}', \beta_{cat_{12}}', ..., \beta_{cat_{1m_1}}', ...,  \beta_{cat_{d_{cat}m_{d_{cat}}}}')_k$$
Whereby $q' = d_{cont} + \sum_{l=1}^{d_{cat}}(m_l)$, but under the restriction that for all categorical variables $l$

$$\sum_{m=1}^{m_l} \beta_{cat_{lm}k}' = 0$$ 

We again have $g$ degrees of freedom. It is this $\boldsymbol{\alpha}_k'$ that will need to be centered, normalized and orthogonalized _in order to treat each variable level equally_ and _avoid dependence of the outcome on the choice of reference level_. Also this representation will come in handy for plotting, centering the levels of the same covariate around zero. We will call this the __zero-sum__ representation. If we use a treatment coding approach, the reference levels will correspond with the centroid, which is harder to interpret I think, and I'm not sure if the normalization is still ok in that case. <!---For a single covariate we go from the treatment (with first level the reference level) to the zero sum representation by solving the following system:

$$\begin{pmatrix}
 1 & 0 & 0 & 0 & \ldots & 0 \\
 -1 & 1 & 0 & 0 & \ldots & 0\\
 0 & -1 & 1 & 0 &  \ldots & 0\\
 \vdots & \vdots &  \ddots & \ddots &  \vdots & \vdots\\
 0 & 0 &  \ldots & 0 &  -1 & 1\\
 1 & 1 & \ldots & 1 & 1 & 1\\
 \end{pmatrix} 
\begin{pmatrix}
\beta_1'\\
\beta_2'\\
\beta_3'\\
\beta_4'\\
\vdots \\
\beta_{m_l}'\\
\end{pmatrix} = 
 \begin{pmatrix}
 \beta_2\\
 \beta_3\\
 \beta_4\\
 \vdots \\
 \beta_{m_l}\\
 0
 \end{pmatrix}
 $$
For more parameters it suffises to append the system to the existing one, $\beta_0'$ is the only common term in all of them.
--->

Filtering of rare taxa is not needed in this case, as all covariates are summarized in one "continuous" row score.

### Triplot

The interpretation of the triplot is different from the one from Zhu _et al._ (2005), and also different from that of the unconstrained RC(M).

 - The components of $\alpha_k$ represent the extent to which the covariates contribute to the departure from independence in dimension $k$. Categorical variables are centered, all their levels are shown. For a scale-free solution, the continuous variables should be centered and scaled. Should the dummy variables not also be normalized then? QUESTION!
 - The samples represent the values of the product of the environmental gradient with their covariates. Samples close together in one dimension come from similar environments, considering that environmental gradient.
 - The taxa still represent the departure from independence of the taxa. They will be multiplied by the importance parameters $\psi$ in this case. A large score in a dimension means that the taxon departs from uniformity because of changes in the environmental conditions in this dimension. Taxa close together respond in a similar way to changes in environmental conditions. The sample-taxon distances and projections are meaningless in this case.

\newpage

# Implementation

## Correspondence analysis

```{r Auxfuns Correspondence analysis, purl=TRUE}
## A function to perform correspondence analysis

caSVD = function(X){
# @param X: the nxp count matrix
  
# @return: the singular value decomposition of the matrix of pearson residuals
  
  C = colSums(X)
  R = rowSums(X)
  onesn =rep.int(1, nrow(X))
  onesp = rep.int(1, ncol(X))
  E = diag(R) %*% outer(onesn ,onesp) %*% diag(C)/sum(C)
  Goal = diag(1/sqrt(R)) %*% (X-E) %*% diag(1/sqrt(C))
  dimnames(Goal) = dimnames(X)
  svd(Goal)
}
#-------------------------------------------------#
```

## Negative binomial

```{r NBauxFuns, purl=TRUE}
#-------------------------------------#
#A score function for the row components of the independence model (library sizes (u_i))
dNBlibSizes = function(beta, X, reg, thetas){
  mu = exp(outer(beta,reg, "+"))
  rowSums((X-mu)/(1+(mu/thetas)))
}
#------------------------------------#
# The corresponding Jacobian
NBjacobianLibSizes = function(beta, X, reg, thetas){
       mu = exp(outer(beta,reg, "+"))
  diag(-rowSums((1+(X/thetas))*mu/(1+(mu/thetas))^2))
}
#-------------------------------------#
#A score function for the column components of the independence model (mean relative abundances (u_j))
dNBabunds = function(beta, X, reg, thetas){
    mu = exp(outer(reg,beta, "+"))
  score = colSums((X-mu)/(1+(mu/thetas)))
}
#------------------------------------#
# The corresponding Jacobian
NBjacobianAbunds = function(beta, X, reg, thetas){
    mu = exp(outer(reg,beta, "+"))
  -diag(colSums((1+(X/thetas))*mu/(1+(mu/thetas))^2))
}

#------------------------------#
#A small aux function for the length of the lambdas
seq_k = function(y){
  (y-1)*(2+(y-2)/2) + seq_len(y+1)
}

#--------------------------------------#
# A score function for the psis
dNBpsis = function(beta, X, reg, theta,  muMarg){
  # @param beta: the current guess for the psi parameter, a scalar
  # @param X: the nxp data matrix
  # @param muMarg: the nxp offset
  # @param reg: the regressor matrix, the outer product of current row and column scores
  # @param theta: a vector of length p with the dispersion parameters

  # @return The evaluation of the score function at beta, a scalar

  mu = muMarg * exp(reg* beta)
  
  sum(reg*(X-mu)/(1+(mu/theta)))

}
#--------------------------------------#
#A jacobian for the psi parameters
NBjacobianPsi = function(beta, X, reg, muMarg, theta){
  # @param beta: the current guess for the psi parameter, a scalar
  # @param X: the nxp data matrix
  # @param muMarg: the nxp offset
  # @param reg: the regressor matrix, the outer product of current row and column scores
  # @param theta: nxp matrix with the dispersion parameters (converted to matrix for numeric reasons)
  
  # @return The evaluation of the Jacobian function at beta, a 1x1 matrix
  
  mu = muMarg * exp(reg* beta)
  matrix(-sum(reg^2*(1+X/theta)*mu/(1+mu/theta)^2),1,1)

}
#-------------------------------------# 
#The influence function for the psis
NBpsiInfl = function(psi, X, cMat, rMat, muMarg, theta){
  reg = rMat %*% cMat
  mu = muMarg * exp(reg* psi)
  -((X-mu)*(theta+mu))/(reg*(theta+X)*mu)
}

#--------------------------------------#
#A score function of the NB for the column scores
dNBllcol = function(beta, X, reg, thetas, muMarg, k, p, n, colWeights, nLambda, cMatK, constrained = FALSE) {
  # @param beta: vector of length p+1+1+(k-1): p row scores, 1 centering, one normalization  and (k-1) orhtogonality lagrangian multipliers
  # @param X: the nxp data matrix
  # @param reg: a nx1 regressor matrix: outer product of rowScores and psis
  # @param theta: nxp matrix with the dispersion parameters (converted to matrix for numeric reasons)
  # @param mumarg: the nxp offset
  # @param k: a scalar, the dimension of the RC solution
  # @param p: a scalar, the number of taxa
  # @param n: a scalar, the number of samples
  # @param nLambda: the number of restrictions
  # @param colWeights: the weights used for the restrictions
  # @param cMatK: the lower dimensions of the colScores
  # @param constrained: a boolean, indicates if ordination is constrained.
  
  # @return A vector of length p+1+1+(k-1) with evaluations of the derivative of lagrangian
  
  cMat = matrix(beta[1:p], byrow=TRUE, ncol=p, nrow=1)
  
  mu = if (constrained){exp(t(t(reg) * c(cMat))) * muMarg} else {exp(reg %*% cMat) * muMarg}
  
  lambda1 = beta[p+1] #Lagrangian multiplier for centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[p+2] #Lagrangian multiplier for normalization restrictions sum(abunds*r^2_{ik}) = 1
  
  lambda3 = if(k==1){0} else {beta[(p + 3):length(beta)]} #Lagrangian multiplier for orhtogonalization restriction
  
  score = if(constrained) {
    colSums(reg*((X-mu)/(1+mu/thetas))) + (colWeights*lambda1 + colWeights*lambda2*2*cMat + (lambda3 %*% cMatK)*colWeights)
  } else {
    crossprod(reg,((X-mu)/(1+mu/thetas))) + (colWeights*lambda1 + colWeights*lambda2*2*cMat + (lambda3 %*% cMatK)*colWeights)
    }

  center = sum(colWeights*cMat)
  unitSum = sum(colWeights*cMat^2)-1
  if(k==1) {
      return(c(score, center, unitSum))
    }
  orthogons = tcrossprod(cMatK, cMat*colWeights)
      return(c(score, center, unitSum, orthogons))
}
#--------------------------------------#
# A function to provide an analytical jacobian in the hope to speed up the calculations
NBjacobianCol = function(beta, X, reg, thetas, muMarg, k, n ,p, colWeights, nLambda, cMatK, constrained = FALSE){
  # @param beta: vector of length p+1+1+(k-1): p row scores, 1 centering, one normalization  and (k-1) orhtogonality lagrangian multipliers
  # @param X: the nxp data matrix
  # @param reg: a nx1 regressor matrix: outer product of rowScores and psis
  # @param theta: nxp matrix with the dispersion parameters
  # @param mumarg: the nxp offset
  # @param k: a scalar, the dimension of the RC solution
  # @param p: a scalar, the number of taxa
  # @param n: a scalar, the number of samples
  # @param nLambda: the number of restrictions
  # @param colWeights: the weights used for the restrictions
  # @param cMatK: the lower dimensions of the colScores
  # @param constrained: a boolean, indicates if ordination is constrained.

  #@return a symmetric jacobian matrix of size p + k +1
  cMat = matrix(beta[1:p], byrow=TRUE, nrow=1, ncol=p)

  #Calculate the mean
  mu = if (constrained){exp(t(t(reg) * c(cMat))) * muMarg} else {exp(reg %*% cMat) * muMarg}
  
  Jac = matrix(0, nrow= p + nLambda, ncol=p + nLambda)
  #The symmetric jacobian matrix. The upper part is filled first, then mirror image is taken for lower triangle
#   
  #dLag/dr_{ik}dlambda_{1k}
  Jac[1:p,(p+1)] = colWeights
  #Jac[1:(p*k),(p*k+1):((p+1)*k)] = sapply(1:k, function(K){c(rep(0,(K-1)*p),colWeights,rep(0,(k-K)*p))})
  Jac[1:p,p+2] = colWeights*2 *cMat

  tmp = (1+X/thetas)*mu/(1+mu/thetas)^2

    #dLag/ds_{ik}dlambda_{3kk'}
  if(k>1){
    Jac[1:p,(p+3):(p+nLambda)] = t(cMatK)*colWeights
  }

  #Symmetrize
   Jac = Jac + t(Jac)
  #dLag/dr_{ik}
    diag(Jac[1:p,1:p]) = if(constrained) {
          -colSums(tmp * reg^2) + 2*beta[p+2]*colWeights
    } else {
    -crossprod(tmp, reg^2) + 2*beta[p+2]*colWeights
    }
  Jac
}
#--------------------------------------#
# The influence function for the column scores
NBcolInfl = function(X, psis, cMat, rMat, thetas, colWeights, k, lambdaCol){
  reg = psis[k] *rMat[,k]
  muMarg = outer(rowSums(X), colSums(X)/sum(X)) * exp(rMat[,1:k, drop=FALSE] %*% (cMat[1:k,, drop=FALSE]*psis[1:k]))
  p=ncol(X)
  n = nrow(X)
  lambdaCol = lambdaCol[seq_k(k)]
  cMatK = cMat[1:(k-1),,drop=FALSE]
  nLambda = length(lambdaCol)
  tmp = if(k>1) lambdaCol[-(1:2)] %*% cMatK else 0
  
  score= t(t(reg*((X-muMarg)/(1+t(t(muMarg)/thetas)))) + c(colWeights*(lambdaCol[1] + lambdaCol[2]*2*cMat[k,] + tmp)))

  JacobianInv = solve(NBjacobianCol(beta = c(cMat[k,], lambdaCol), X = X, reg= reg, thetas = thetas, muMarg = muMarg, k = k, p = p, n=n ,colWeights = colWeights , nLambda = nLambda, cMatK = cMatK)) #Inverse Jacobian
  
  #After a long thought: The X's do not affect the estimation of the lambda parameters!
  #Matrix becomes too large: return score and inverse jacobian
  return(list(score=score, InvJac = JacobianInv[1:p,1:p]))
}
#-------------------------------------------#

#A function to extract the influence for a given parameter index
getInflCol = function(score, InvJac, parIndex){
  t(t(score)* InvJac[, parIndex])
}

#-------------------------------------------#
#A score function of the NB for the row scores
dNBllrow = function(beta, X, reg, thetas, muMarg, k, n , p, rowWeights, nLambda, rMatK) {
  # @param beta: a vector of k regression parameters to optimize: the k column scores
  # @param X: the data matrix of dimensions nxp
  # @param reg: a 1xp regressor matrix: outer product of rowScores and psis
  # @param thetas: nxp matrix with the dispersion parameters (converted to matrix for numeric reasons)
  # @param muMarg: an nxp offset matrix
  # @param k: a scalar, the dimension of the RC solution
  # @param p: a scalar, the number of taxa
  # @param n: a scalar, the number of samples
  # @param rowWeights: a vector of length n, the weights used for the restrictions
  # @param nLambda: an integer, the number of lagrangian multipliers
  # @param rMatK: the lower dimension row scores

  # @return A vector of length n + k +1 with evaluations of the derivative of the lagrangian
  rMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)
  mu = exp(rMat %*% reg)* muMarg
  
  lambda1 = beta[n+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[n+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(n+3):length(beta)]}

  score = c(tcrossprod(reg, (X-mu)/(1+(mu/thetas)))) + (rowWeights*lambda1 + rowWeights*lambda2* 2*rMat + (rMatK %*% lambda3)*rowWeights)
  #
  center = sum(rMat*rowWeights)
  unitSum = sum(rMat^2*rowWeights)-1
  if(k==1){ return(c(score,center, unitSum))}
  orthogons = crossprod(rMatK, rMat*rowWeights)
  return(c(score,center, unitSum, orthogons))
}
#-------------------------------------------#
# A function to provide an analytical jacobian to speed up the calculations
NBjacobianRow = function(beta, X, reg, thetas, muMarg, k, n ,p, nlambda, rowWeights, nLambda, rMatK){
  # @param beta: a vector of k regression parameters to optimize: the k column scores
  # @param X: the data matrix of dimensions nxp
  # @param reg: a 1xp regressor matrix: outer product of rowScores and psis
  # @param thetas: nxp matrix with the dispersion parameters (converted to matrix for numeric reasons)
  # @param muMarg: an nxp offset matrix
  # @param k: a scalar, the dimension of the RC solution
  # @param p: a scalar, the number of taxa
  # @param n: a scalar, the number of samples
  # @param rowWeights: a vector of length n, the weights used for the restrictions
  # @param nLambda: an integer, the number of lagrangian multipliers
  # @param rMatK: the lower dimension row scores
  
  #@return a symmetric jacobian matrix of size (n+2) *k + k(k-1)/2
  rMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)

  mu = exp(rMat %*% reg)* muMarg

  Jac = matrix(0, nrow= n + nLambda, ncol= n + nLambda)
  #The symmetric jacobian matrix. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag/dr_{ik}dlambda_{1k}
  Jac[1:n, n+1] = rowWeights
  #dLag/dr_{ik}dlambda_{2k}
  Jac[1:n, n+2] = 2 *rMat*rowWeights
  tmp = (1+X/thetas)*mu/(1+mu/thetas)^2

  #dLag/dr_{ik}dlambda_{3kk'}
  if(k>1){
    Jac[1:n,(n+3):(n+nLambda)] = rMatK*rowWeights
  }
  #Symmetrize
  Jac = Jac + t(Jac)
#dLag/dr_{ik}

    diag(Jac[1:n,1:n]) = -tcrossprod(reg^2 ,tmp) + 2*rowWeights*beta[n+2]
    
  Jac
}
#--------------------------------------#
# The influence function for the row scores
NBrowInfl = function(X, psis, cMat, rMat, thetas, rowWeights, k, lambdaRow){
  reg = psis[k] *cMat[k,]
  muMarg = outer(rowSums(X), colSums(X)/sum(X)) * exp(rMat[,1:k, drop=FALSE] %*% (cMat[1:k,, drop=FALSE]*psis[1:k]))
  p=ncol(X)
  n = nrow(X)
  lambdaRow = lambdaRow[seq_k(k)]
  rMatK = rMat[,1:(k-1),drop=FALSE]
  nLambda = length(lambdaRow)
  tmp = if(k>1) rMatK %*% lambdaRow[-(1:2)] else 0
  
  score= reg*(X-muMarg)/(1+t(t(muMarg)/thetas)) + c(rowWeights*(lambdaRow[1] + lambdaRow[2]*2*rMat[,k] + tmp))

  JacobianInv = solve(NBjacobianRow(beta = c(rMat[,k], lambdaRow), X = X, reg= reg, thetas = thetas, muMarg = muMarg, k = k, p = p, n=n, rowWeights = rowWeights , nLambda = nLambda, rMatK = rMatK)) #Inverse Jacobian
  
  #After a long thought: The X's do not affect the estimation of the lambda parameters!
  #Matrix becomes too large: return score and inverse jacobian
  return(list(score=score, InvJac = JacobianInv[1:n,1:n]))
}
#-------------------------------------------#

#A function to extract the influence for a given parameter index
getInflRow = function(score, InvJac, parIndex){
  score* InvJac[, parIndex]
}

#-------------------------------------------#
# A function to estimate the overdispersion of the NB with empirical Bayes using the edgeR pacakage
estDisp = function (X, cMat, rMat = NULL, muMarg, psis, trended.dispersion, prior.df = 10, dispWeights = NULL, rowMat = NULL)
{
  # @param X: the data matrix of dimensions nxp
  # @param cMat: a 1xp colum scores matrix
  # @param rMat(optional): a nx1 rowscores matrix, if unconstrained
  # @param muMarg: an nxp offset matrix
  # @param psis: a scalar, the current psi estimate
  # @param trended.dispersion: a vector of length p with pre-calculated trended.dispersion estimates. They do not vary in function of the offset anyway
  # @param prior.df(optional): an integer, number of degrees of freedom of the prior for the Bayesian shrinkage
  # @param dispWeights (optional): Weights for estimating the dispersion in a zero-inflated model
  # @param rowMat: matrix of row scores in case of constrained ordination
  
  # @return A vector of length p with dispersion estimates
  
  require(edgeR)
  logMeansMat = 
    if(!is.null(rMat)){ #Unconstrained
      t(rMat %*% (cMat * psis) + log(muMarg))
    } else { #Constrained
      t(log(muMarg) + psis* t(t(rowMat)*c(cMat)))
    }
  thetaEsts <- 1/estimateGLMTagwiseDisp(y = t(X), design = NULL,
                                        prior.df = prior.df, offset = logMeansMat, dispersion = trended.dispersion, weights = dispWeights)
  if (anyNA(thetaEsts)) {
    idNA = is.na(thetaEsts)
    thetaEsts[idNA] = mean(thetaEsts[!idNA])
    warning(paste(sum(idNA), "dispersion estimations did not converge!"))
  }
  return(thetas = thetaEsts)
}
#-------------------------------------------#
# A function to trim based on confounders to ensure there are no taxa with only zero counts in one of the subgroups defined by the confounders. Returns a trimmed data matrix, should be called prior to fitting the independence model
trimOnConfounders = function(confounders, X, prevCutOff, minFraction, n){
  # @param confounders: a nxt confounder matrix
  # @param X: the nxp data matrix
  # @param prevCutOff: a scalar, the prevalence cut off
  # @param minFraction: a scalar, each taxon's total abundance should equal at leat the number of samples n times minFraction, otherwise it is trimmed
  # @param n: the number of samples
  
  # @return A trimmed data matrix nxp'
  
  trimmingID = apply(X, 2, function(x){ #Over taxa
    any(apply(confounders, 2, function(conf){ #Over confounding variables
      tapply(X = x, INDEX = conf, FUN = function(y){mean(y!=0) <= prevCutOff | sum(y)<(n*minFraction)}) #Any all-zero subgroup?
    }))
  })
  
  if(all(trimmingID)){
    stop("All taxa would be trimmed, please provide a covariate with less levels! \n")
  }
  
  X[, !trimmingID] #Return trimmed X
}

#-------------------------------------------#
#A function to filter out the effect of known confounders. This is done by fitting interactions of every taxon with the levels of the confounders. 

filterConfounders = function(muMarg, confMat, X, thetas, p, n, nleqslv.control, trended.dispersion, tol = 1e-3, maxIt = 20){
  # @param muMarg: a nxp matrix, the current offset
  # @param confounders: a nxt confounder matrix
  # @param X: the nxp data matrix
  # @param thetas: a vector of length p with the current dispersion estimates
  # @param p: a scalar, the number of columns of X
  # @param tol: a scalar, the convergence tolerance
  # @param maxIt (optional): maximum number of iterations, defaults to 20

  # @return a list with components:
  # -thetas: new theta estimates
  # -muMarg: a new offset matrix
  # -NB_params: the estimated parameters of the interaction terms

  NB_params = matrix(0, ncol(confMat), p)

  iter = 1
  while((iter ==1) || ((iter <= maxIt) && (!convergence))){
    
    NB_params_old = NB_params

     NB_params = sapply(seq_len(p), function(i){
     nleq = try(nleqslv(NB_params[,i] , reg = confMat,  fn = dNBllcol_constr, theta = thetas[i], muMarg = muMarg[,i], cReg = 1, X = X[,i], control = nleqslv.control, jac = JacCol_constr)$x)  # , ... Fit the taxon-by taxon NB with given overdispersion parameters and return predictions
     if(class(nleq)=="try-error"| anyNA(nleq)| any(is.infinite(nleq))){
     nleq = nleqslv(NB_params[,i] , reg = confMat,  fn = dNBllcol_constr, theta = thetas[i], muMarg = muMarg[,i], cReg =1, X = X[,i], control = nleqslv.control)$x # If fails try with numeric jacobian
     }
     return(nleq)
   }) #Estimate response functions

     thetas = estDisp(X = X, cMat = matrix(0, ncol=p), rMat = matrix(0, nrow=n), psis = 0, muMarg = muMarg * exp(confMat %*% NB_params), trended.dispersion = trended.dispersion) #Estimate overdispersion
     
     iter = iter +1
     
  convergence = sqrt(mean((NB_params_old-NB_params)^2)) < tol #Check for convergence, L2-norm
  
  }
  
  list(muMarg = muMarg * exp(confMat %*% NB_params), thetas = thetas, NB_params = NB_params)
  }
#An auxiliary R function to "array" multiply an array with a vector, kindly provided by Joris
#-------------------------------------------#
arrayprod <- function(x,y){
  # @param x: a axbxc array
  # @param y: a vector of length c matrix
  
  # @return a axb matrix
  xdim <- dim(x)
  outdim <- xdim[1:2]
  outn <- prod(outdim)
 
  yexpand <- rep(y, each = outn)

  tmp <- x * yexpand
 
  dim(tmp) <- c(outn, xdim[3])
  out <- rowSums(tmp)
 
  dim(out) <- outdim
 
  out
}
```

```{r Negative binomial fit function, purl = TRUE}
#-------------------------------------------#
## A function to fit the RC(M) model with the negative binomial distribution. Includes fitting of the independence model, filtering out the effect of confounders and fitting the RC(M) components in a constrained or an unconstrained way for any dimension k. Not intended to be called directly but only through the RCM() function

RCM_NB = function(X, k, rowWeights = "uniform", colWeights = "marginal", tol = 1e-3, maxItOut = 500, Psitol = 1e-3, verbose = TRUE, NBRCM = NULL, global = "dbldog", nleqslv.control=list(maxit = 500), jacMethod = "Broyden", dispFrec = 20, convNorm = 2, prior.df=10, marginEst = "MLE", confounders = NULL, prevCutOff = 2.5e-2, minFraction = 0.1, covariates = NULL, centMat = NULL, responseFun = "linear"){
  
  # @param X: a nxp data matrix
  # @param k: a scalar, number of dimensions in the RC(M) model
  # @param rowWeights(optional): a character string, either "uniform" or "marginal" row weights. Defaults to "uniform" 
  # @param colWeights(optional): a character string, either "uniform" or "marginal" column weights. Defaults to "uniform "marginal"
  # @param tol(optional): a scalar, the relative convergende tolerance for the row scores and column scores parameters, defaults to 1e-3  
   # @param maxItOut(optional): an integer, the maximum number of iteration in the outer loop, defaults to 500
   # @param Psitol(optional): a scalar, the relative convergence tolerance for the psi parameters, defaults to 1e-4
  # @param verbose(optional): a boolean, should information on iterations be printed? Defaults to TRUE
  # @param NBRCM(optional): a previously fitted NBRCM object, from which the lower dimensions can be extracted. Only useful if NBRCM$xk < k  
  # @param global(optional): global strategy for solving non-linear systems, see ?nleqslv
   # @param nleqslv.control(optional): a list with control options, see nleqslv
  # @param method(optional): Method for solving non-linear equations, ?see nleqslv. Defaults to Broyden. The difference with the newton method is that the Jacobian is not recalculated at every iteration, thereby speeding up the algorithm
  # @param dispFreq(optional): an integer, how many iterations the algorithm should wait before reestimationg the dispersions. Defaults to 20
  # @param convNorm(optional): a scalar, the norm to use to determine convergence
  # @param prior.df(optional): an integer, see estDisp()
  # @param marginEst(optional): a character string, either "MLE" or "marginSums", indicating how the independence model should be estimated
  # @param confounders(optional): a list with
  # -confounders: an nxg matrix with confounders
  # -confoundersFilt: an nxh matrix with confounders for filtering, with all levels and without intercept
  # @param covariates(optional): an nxd matrix with covariates. If set to null an unconstrained analysis is carried out, otherwise a constrained one. Factors must have been converted to dummy variables already
  # @param centMat(optional): a fxd matrix containing the contrasts to center the categorical variables. f equals the number of continuous variables + the total number of levels of the categorical variables.
  # @param responseFun(optional): a character string, either "linear", "gaussian" (or alias "quadratic") or "non-parametric"
  # @param prevCutOff(optional): a scalar: the minimum prevalence needed to retain a taxon before the the confounder filtering
  # @param minFraction(optional): a scalar, total taxon abundance should equal minFraction*n if it wants to be retained before the confounder filtering

  # @return A list with elements:
  # - converged: a vector of booleans of length k indicating if the algorithm converged for every dimension
  # - rMat (if not constrained: a nxk matrix with estimated row scores
  # - cMat: a kxp matrix with estimated column scores
  # - psis: a vector of length k with estimates for the importance parameters psi
  # - thetas: a vector of length p with estimates for the overdispersion
  # - rowRec(if not constrained): a n x k x maxItOut array with a record of all rMat estimates through the iterations 
  # - colRec: a k x p x maxItOut array with a record of all cMat estimates through the iterations
  # - psiRec.: a k x maxItOut array with a record of all psi estimates through the iterations
  # - thetaRec: a matrix of dimension pxmaxItOut with estimates for the overdispersion along the way
  # - iter: number of iterations
  # - Xorig (if confounders provided): the original fitting matrix
  # - X: the trimmed matrix if confounders provided, otherwise the original one
  # - fit: type of fit, either "RCM_NB" or "RCM_NB_constr"
  # - lambdaRow(if not constrained): vector of Lagrange multipliers for the rows
  # - lambdaCol: vector of Lagrange multipliers for the columns
  # - rowWeights(if not constrained): the row weights used
  # - colWeights: the column weights used

  # - alpha(if constrained): the kxd matrix of environmental gradients
  # - alphaRec(if constrained): the kxdxmaxItOut array of alpha estimates along the iterations
  # - covariates(if constrained): the matrix of covariates
  
  # - libSizes: a vector of length n with estimated library sizes
  # - abunds: a vector of length p with estimated mean relative abundances
  
  # - confounders(if provided): the confounder matrix
  # - confParams: the parameters used to filter out the confounders
  
  Xorig = NULL #An original matrix, not returned if no trimming occurs
  
    #If previous fit provided with higher or equal dimension, stop here
  if((!is.null(NBRCM)) ){
    if(NBRCM$fit != "RCM_NB"){
      stop("Fit provided is not of same type as the one requested! \n Make sure you used exactly the same trimming parameters, otherwise start a new fit. \n")
    } else if((k <= NBRCM$k)) {
      stop("Fit provided is already of the required dimension or higher! \n")
    } else{
      #Read in all starting values
      rMat = NBRCM$rMat
      cMat = NBRCM$cMat
      psis = NBRCM$psis
      Kprev = NBRCM$k
      if(!all(X == NBRCM$X)){ stop("Different count matrix provided from original fit! \n")
      } else {X = NBRCM$X}
      n=NROW(X)
      p=NCOL(X)
      thetas = NBRCM$thetas
      thetasMat = matrix(thetas, byrow = TRUE, n, p)
      rowWeights = NBRCM$rowWeights
      colWeights = NBRCM$colWeights
      libSizesMLE = NBRCM$libSizes
      logLibSizesMLE  =log(libSizesMLE)
      abundsMLE = NBRCM$abunds
      logAbundsMLE = log(abundsMLE)
      #Extend the record matrices, force same number of outer iterations as previous fit
      if(dim(NBRCM$rowRec)[3]!=maxItOut){
        maxItOut = dim(NBRCM$rowRec)[3]
        warning("Using same number of outer iterations as old fit! \n")
      }
      rowRec = array(0, dim = c(n,k,maxItOut))
      rowRec[,1:Kprev,] = NBRCM$rowRec
      colRec = array(0, dim = c(k,p,maxItOut))
      colRec[1:Kprev,,] = NBRCM$colRec
      thetaRec = array(0, dim = c(k,p,maxItOut))
      thetaRec[1:Kprev,,] = NBRCM$thetaRec
      psiRec = array(0, dim = c(k,maxItOut))
      psiRec[1:Kprev,] = NBRCM$psiRec
      lambdaCol = lambdaRow = rep(0, k*(2+(k-1)/2))
      lambdaCol[1:(Kprev*(2+(Kprev-1)/2))] = NBRCM$lambdaCol
      lambdaRow[1:(Kprev*(2+(Kprev-1)/2))] = NBRCM$lambdaRow
      convergence = c(NBRCM$converged, rep(FALSE, k-Kprev))
      iterOut = c(NBRCM$iter, rep(1,k-Kprev))
      muMarg = outer(libSizesMLE, abundsMLE)*exp( rMat %*% (cMat*psis))
      # Fill out the other columns of the SVD (in case of unconstrained analysis)
      if(is.null(covariates)){
      svdX = svd(diag(1/rowSums(X)) %*% (X-muMarg) %*% diag(1/colSums(X)))
      newK = (Kprev+1):k
      rMat = cbind(rMat, svdX$u[,newK, drop=FALSE])
      cMat = rbind(cMat, t(svdX$v[,newK, drop=FALSE]))
      psis = c(psis, svdX$d[newK])
      }
      trended.dispersion <- estimateGLMTrendedDisp(y = t(X), design = NULL, method = "bin.loess", offset = t(log(outer(rowSums(X), colSums(X)/sum(X)))), weights = NULL)
      confParams = NBRCM$confParams
    }
  #Otherwise start the fit from scratch
  } else {
  if(!is.null(confounders[[1]])){ #First and foremost: filter on confounders
    Xorig = X
    X = trimOnConfounders(X, confounders = confounders$confoundersTrim, prevCutOff = prevCutOff, n=nrow(Xorig), minFraction = minFraction)
  }
  
  n=NROW(X)
  p=NCOL(X)
  
  #Initialize some parameters
  abunds = colSums(X)/sum(X)
  libSizes = rowSums(X)
  
  #Get the trended-dispersion estimates. Very insensitive to the offset so only need to be calculated once
  trended.dispersion <- estimateGLMTrendedDisp(y = t(X), design = NULL, method = "bin.loess", offset = t(log(outer(libSizes, abunds))), weights = NULL)

  if(marginEst == "MLE"){

  logLibSizesMLE = log(libSizes)
  logAbundsMLE = log(abunds)
  
  initIter = 1
  
  cat("Estimating the independence model \n")
  
   while((initIter ==1) || ((initIter <= maxItOut) && (!convergenceInit))){
     logLibsOld = logLibSizesMLE
     logAbsOld = logAbundsMLE
     
         thetas = estDisp(X = X, rMat = as.matrix(rep(0,n)), cMat = t(as.matrix(rep(0,p))),  muMarg=exp(outer(logLibSizesMLE, logAbundsMLE, "+")), psis = 0, prior.df = prior.df, trended.dispersion = trended.dispersion)
         thetasMat = matrix(thetas, n, p, byrow=TRUE)

 logLibSizesMLE = nleqslv(fn = dNBlibSizes, x = logLibSizesMLE, theta = thetasMat, X = X, reg=logAbundsMLE, global=global, control = nleqslv.control, jac=NBjacobianLibSizes, method=jacMethod)$x
logAbundsMLE = nleqslv(fn = dNBabunds, x = logAbundsMLE, theta = thetasMat, X = X, reg=logLibSizesMLE, global=global, control = nleqslv.control, jac=NBjacobianAbunds, method=jacMethod)$x
  initIter = initIter + 1
  
  convergenceInit = ((initIter <= maxItOut) && 
                    ((sum(abs(1-logLibSizesMLE/logLibsOld)^convNorm)/n)^(1/convNorm) < tol) && 
                   ((sum(abs(1-logAbundsMLE/logAbsOld)^convNorm)/p)^(1/convNorm) < tol) )
  
   }
  #Converges very fast, even when dispersions are re-estimated. For the library sizes there is a big difference, for the abundances less so
 
    muMarg = exp(outer(logLibSizesMLE, logAbundsMLE, "+")) #The marginals to be used as expectation. These are augmented with the previously estimated dimensions every time
  } else if(marginEst=="marginSums"){
    muMarg = outer(libSizes, abunds)
  } else{
    stop("No valid margin estimation paradigm provided! \n")
  }
  
  
  rowWeights = switch(paste(marginEst, rowWeights, sep = "_"),
                      "marginSums_marginal" = libSizes/sum(libSizes),
                      "MLE_marginal" = exp(logLibSizesMLE)/sum(exp(logLibSizesMLE)),
                      rep.int(1/n,n) #For uniform weights
                      )
  colWeights = switch(paste(marginEst, colWeights, sep = "_"),
                      "marginSums_marginal" = abunds,
                      "MLE_marginal" = exp(logAbundsMLE),
                      rep.int(1/p,p) #For uniform weights
                      )

  nLambda = 2*k+k*(k-1)/2
  # Pre-allocate arrays to track iterations
  rowRec = array(0,dim=c(n,k, maxItOut))
  colRec = thetaRec = array(0,dim=c(k,p, maxItOut))
  psiRec = matrix(0, nrow=k,ncol=maxItOut)
  convergence = rep(FALSE, k)
  iterOut = rep(1,k)
  if(!is.null(confounders[[1]])){
  ## Filter out the confounders by adding them to the intercept, also adapt overdispersions
  filtObj = filterConfounders(muMarg = muMarg, confMat = confounders$confounders, p=p, X=X, thetas = thetas, nleqslv.control = nleqslv.control, n=n, trended.dispersion = trended.dispersion)
  muMarg = filtObj$muMarg
  thetas = filtObj$thetas
  confParams = filtObj$NB_params
  } else {
    confParams=NULL
    }
 ## 1) Initialization
  svdX = svd(diag(1/libSizes) %*% (X-muMarg) %*% diag(1/colSums(X)))
  rMat = svdX$u[,1:k, drop=FALSE]
  cMat = t(svdX$v[,1:k, drop=FALSE])
  psis = svdX$d[1:k]

#Center
  cMat = t(apply(cMat, 1, function(colS){
      colS-sum(colS*colWeights)/sum(colWeights)
  }))
  rMat = apply(rMat, 2, function(rowS){
      rowS-sum(rowS*rowWeights)/sum(rowWeights)
  })
   
#Redistribute some weight to fit the constraints 
psis = c(psis *t(apply(cMat, 1, function(colS){
      sqrt(sum(colWeights * colS^2))
  })) * apply(rMat, 2, function(rowS){
      sqrt(sum(rowWeights * rowS^2))
  }))

#Normalize  
cMat = t(apply(cMat, 1, function(colS){
      colS/sqrt(sum(colWeights * colS^2))
  }))

rMat = apply(rMat, 2, function(rowS){
      rowS/sqrt(sum(rowWeights * rowS^2))
  })
  lambdaRow =  rep.int(0,nLambda)
  lambdaCol =  rep.int(0,nLambda )
  } # END if-else: no previous fot provided
  
  if(is.null(covariates)){ #If no covariates provided, perform an unconstrained analysis
  
  minK = ifelse(is.null(NBRCM),1,Kprev+1)
  for (KK in minK:k){

    cat("Dimension" ,KK, "is being esimated \n")
    
  #Modify offset if needed
  if(KK>1){muMarg = muMarg * exp(rMat[,(KK-1), drop=FALSE] %*% (cMat[(KK-1),, drop=FALSE]*psis[(KK-1)]))}

    idK = seq_k(KK)
  ## 2) Propagation
  
  while((iterOut[KK] ==1) || ((iterOut[KK] <= maxItOut) && (!convergence[KK])))
    {
    
  if(verbose && iterOut[KK]%%1 == 0){
  cat("\n","Outer Iteration", iterOut[KK], "\n","\n")
    if(iterOut[KK]!=1){
  cat("Old psi-estimate: ", psisOld, "\n")
  cat("New psi-estimate: ", psis[KK], "\n")
    }
  }
  ## 2)a. Store old parameters
  psisOld = psis[KK]
  rMatOld = rMat[,KK]
  cMatOld = cMat[KK,]
 
#Overdispersions (not at every iterations to speed things up, the estimates do not change a lot anyway)
    if((iterOut[KK] %% dispFrec) ==0 || (iterOut[KK]==1)){
  if (verbose) cat("\n Estimating overdispersions \n")
 thetas = estDisp(X = X, rMat = rMat[,KK,drop=FALSE], cMat = cMat[KK,,drop=FALSE],  muMarg=muMarg, psis = psis[KK], prior.df = prior.df, trended.dispersion = trended.dispersion)
 thetasMat = matrix(thetas, n, p, byrow=TRUE) #Make a matrix for numerical reasons, it avoids excessive use of the t() function
  }
#Psis
    if (verbose) cat("\n Estimating psis \n")
regPsis = outer(rMat[,KK] ,cMat[KK,])

psis[KK]  = abs(nleqslv(fn = dNBpsis, x = psis[KK], theta = thetasMat, X = X, reg=regPsis, muMarg=muMarg, global=global, control = nleqslv.control, jac=NBjacobianPsi, method=jacMethod)$x)
#Column scores
  if (verbose) cat("\n Estimating column scores \n")
regCol = rMat[,KK, drop=FALSE]*psis[KK]
tmpCol = nleqslv(fn = dNBllcol, x = c(cMat[KK,], lambdaCol[idK]), thetas = thetasMat, X = X, reg = regCol, muMarg = muMarg, k = KK,  global = global, control = nleqslv.control, n=n, p=p, jac = NBjacobianCol, method = jacMethod, colWeights = colWeights, nLambda = (KK+1), cMatK = cMat[1:(KK-1),,drop=FALSE])

  cat(ifelse(tmpCol$termcd==1, "Column scores converged \n", "Column scores DID NOT converge \n"))
  cMat[KK,] = tmpCol$x[1:p]
  lambdaCol[idK] = tmpCol$x[p + seq_along(idK)]

#Normalize (speeds up algorithm if previous step had not converged)
cMat[KK,] = cMat[KK,] - sum(cMat[KK,] * colWeights)/sum(colWeights) 
cMat[KK,] = cMat[KK,]/sqrt(sum(colWeights * cMat[KK,]^2))
#Row scores
  if (verbose) cat("\n Estimating row scores \n")
regRow = cMat[KK,,drop=FALSE]*psis[KK]
tmpRow = nleqslv(fn = dNBllrow, x = c(rMat[,KK], lambdaRow[idK]), thetas=thetasMat, X = X, reg = regRow, muMarg=muMarg, k=KK,  global = global, control = nleqslv.control, n=n, p=p, jac = NBjacobianRow, method=jacMethod, rowWeights=rowWeights, nLambda=(KK+1), rMatK = rMat[,1:(KK-1), drop=FALSE])

  if(verbose) cat(ifelse(tmpRow$termcd==1, "Row scores converged \n", "Row scores DID NOT converge \n"))
  rMat[,KK] = tmpRow$x[1:n]
  lambdaRow[idK] = tmpRow$x[n + seq_along(idK)]

#Normalize (speeds up algorithm if previous step had not converged)
rMat[,KK] = rMat[,KK] - sum(rMat[,KK] * rowWeights)/sum(rowWeights) 
rMat[,KK] = rMat[,KK]/sqrt(sum(rowWeights * rMat[,KK]^2))

#Store intermediate estimates
  rowRec[,KK, iterOut[KK]] = rMat[,KK]
  colRec[KK,, iterOut[KK]] = cMat[KK,]
  thetaRec [KK,, iterOut[KK]] = thetas
  psiRec[KK, iterOut[KK]] = psis[KK]

  ## Change iterator
  iterOut[KK] = iterOut[KK] + 1
    
  ##Check convergence  (any numbered norm for row and column scores)
  convergence[KK] = ((iterOut[KK] <= maxItOut) && 
                   (all(abs(1-psis[KK]/psisOld) < Psitol)) && #Infinity norm for the psis
                   ((sum(abs(1-rMat[,KK]/rMatOld)^convNorm)/n)^(1/convNorm) < tol) && 
                   ((sum(abs(1-cMat[KK,]/cMatOld)^convNorm)/p)^(1/convNorm) < tol) )
} # END while-loop until convergence
  
  }# END for-loop over dimensions
  
  ## 3) Termination
  
  rownames(rMat) = rownames(X)
  colnames(cMat) = colnames(X)
  rownames(cMat) = colnames(rMat) = paste0("Dim",1:k)

    returnList =   list(converged = convergence, rMat = rMat, cMat=cMat, psis = psis, thetas = thetas,  rowRec = rowRec, colRec = colRec, psiRec = psiRec, thetaRec = thetaRec, iter = iterOut-1, X=X, Xorig = Xorig, fit = "RCM_NB", lambdaRow = lambdaRow, lambdaCol = lambdaCol, rowWeights = rowWeights, colWeights = colWeights,
       libSizes = switch(marginEst, "MLE" = exp(logLibSizesMLE), "marginSums" = libSizes), abunds = switch(marginEst, "MLE" = exp(logAbundsMLE), "marginSums" = abunds), 
       confounders = confounders, confParams = confParams)
  
  } else { #If covariates provided, do a constrained analysis
  
d = ncol(covariates)
CCA = cca(X = X, Y = covariates)$CCA #Constrained correspondence analysis for starting values
alpha = matrix(0,d,k)
alpha[!colnames(covariates) %in% CCA$alias,] = CCA$biplot[,1:k] #Leave the sum constraints for the factors alone for now, may or may not speed up the algorithm
alpha = t(t(alpha)-colMeans(alpha))
alpha = t(t(alpha)/sqrt(colSums(alpha^2)))
if(!is.null(NBRCM)){
 newK = (Kprev+1):k
 cMat = rbind(cMat, t(CCA$eig[,newK, drop=FALSE]))
 psis = c(psis, CCA$eig[newK])
} else {
  cMat = t(CCA$v)[1:k,,drop=FALSE]
  psis = CCA$eig[1:k]
}

alphaRec = array(0, dim=c(d,k, maxItOut))
v = switch(responseFun, linear = 2, quadratic = 3, 1) #Number of parameters per taxon
NB_params = array(0,dim=c(v,p,k)) #Initiate parameters of the response function, taxon-wise
NB_params_noLab = matrix(0,v,k) #Initiate parameters of the response function, ignoring taxon-labels
  
 if(!is.null(NBRCM)){ #If fit provided, replace lower dimension starting values
   alpha[,1:Kprev] = NBRCM$alpha
   alphaRec[,1:Kprev,] = NBRCM$alphaRec
   NB_params[,,1:Kprev] = NBRCM$NB_params
   NB_params_noLab[,,1:Kprev] = NBRCM$NB_params_noLab
 }

#Number of lambda parameters for centering
nLambda1s = NROW(centMat)

  minK = ifelse(is.null(NBRCM),1,Kprev+1) #Next dimension to fit
  for (KK in minK:k){
    
  cat("Dimension" ,KK, "is being esimated \n")
  
  #Modify offset if needed
  if(KK>1){muMarg = muMarg * exp(rMat[,(KK-1), drop=FALSE] %*% (cMat[(KK-1),, drop=FALSE]*psis[(KK-1)]))}

  idK = seq_k(KK)
  ## 2) Propagation
  
  while((iterOut[KK] ==1) || ((iterOut[KK] <= maxItOut) && (!convergence[KK])))
    {
    
  if(verbose && iterOut[KK]%%1 == 0){
  cat("\n","Outer Iteration", iterOut[KK], "\n","\n")
    if(iterOut[KK]!=1){
  cat("Old psi-estimate: ", psisOld, "\n")
  cat("New psi-estimate: ", psis[KK], "\n")
    }
  }
  ## 2)a. Store old parameters to check for convergence
  psisOld = psis[KK]
  alphaOld = alpha[,KK]
  cMatOld = cMat[KK,]
  
  #The matrix of row scores
  rowMat = getRowMatConstr(v = v, NB_params = NB_params[,,KK], covariates = covariates, alpha = alpha)

#Overdispersions (not at every iterations to speed things up, doesn't change a lot anyway)
  if((iterOut[KK] %% dispFrec) == 0 || iterOut[KK] == 1){
    if (verbose) cat("\n Estimating overdispersions \n")
 thetas = estDisp(X = X, cMat = cMat[KK,,drop=FALSE],  muMarg = muMarg, psis = psis[KK], prior.df = prior.df, trended.dispersion = trended.dispersion, rowMat = rowMat)
 thetasMat = matrix(thetas, n, p, byrow=TRUE)
  }

#Psis
    if (verbose) cat("\n Estimating psis (k = ", KK, ") \n", sep="")
regPsis = t(t(rowMat) *cMat[KK,])

psis[KK]  = abs(nleqslv(fn = dNBpsis, x = psis[KK], theta =  thetasMat , X = X, reg=regPsis, muMarg=muMarg, global=global, control = nleqslv.control, jac=NBjacobianPsi, method = jacMethod)$x)
#Column scores
if (verbose) cat("\n Estimating column scores \n")
regCol = rowMat*psis[KK]
tmpCol = nleqslv(fn = dNBllcol, x = c(t(cMat[KK,]), lambdaCol[idK]), thetas =  thetasMat , X = X, reg = regCol, muMarg = muMarg, k = KK,  global = global, control = nleqslv.control, n=n, p=p, jac = NBjacobianCol, method = jacMethod, colWeights = colWeights, nLambda = (KK+1), cMatK = cMat[1:(KK-1),,drop=FALSE], constrained = TRUE)

cat(ifelse(tmpCol$termcd==1, "Column scores converged \n", "Column scores DID NOT converge \n"))
cMat[KK,] = tmpCol$x[1:p]
lambdaCol[idK] = tmpCol$x[p + seq_along(idK)]
#Normalize (speeds up algorithm if previous step had not converged)

cMat[KK,] = cMat[KK,] - sum(cMat[KK,] * colWeights)/sum(colWeights) 
cMat[KK,] = cMat[KK,]/sqrt(sum(colWeights * cMat[KK,]^2))

#Row scores
  if (verbose) cat("\n Estimating environmental gradient \n")
regAlpha = cMat[KK,,drop=TRUE]*psis[KK]
tmpAlpha = estAlpha(Alpha = alpha[, KK,drop=FALSE], X = X, CC = covariates, d = d, ncols = p, v = v, responseFun = responseFun, muMarg = muMarg, thetas =  thetasMat , cReg = regAlpha, alphaK = alpha[,1:(KK-1),drop=FALSE], k=KK , centMat = centMat, n=n, NB_params = NB_params[,,KK], NB_params_noLab = NB_params_noLab[,KK], psi = psis[KK], cMat = cMat[KK,,drop=TRUE], nLambda1s = nLambda1s, nLambda = KK + nLambda1s) #Provide previous parameters of the response function since they might provide good initial estimates

if(verbose)  cat(ifelse(tmpAlpha$converged, "Row scores converged \n", "Row scores DID NOT converge \n"))
  alpha[,KK] = tmpAlpha$Alpha
  NB_params[,,KK] = tmpAlpha$NB_params
  NB_params_noLab[,KK] = tmpAlpha$NB_params_noLab
  alphaLambdas[, KK] = tmpAlpha$alphaLambdas

#Normalize (speeds up algorithm if previous step had not converged)
alpha[,KK] = alpha[,KK] - mean(alpha[,KK]) 
alpha[,KK] = alpha[,KK]/sqrt(sum(alpha[,KK]^2))

#Store intermediate estimates
  alphaRec[,KK, iterOut[KK]] = alpha[,KK]
  colRec[KK,, iterOut[KK]] = cMat[KK,]
  thetaRec [KK,, iterOut[KK]] = thetas
  psiRec[KK, iterOut[KK]] = psis[KK]

    ## Change iterator
    iterOut[KK] = iterOut[KK] + 1
    
  ##Check convergence  (any numbered norm for row and column scores)
  convergence[KK] = ((iterOut[KK] <= maxItOut) && 
                   (all(abs(1-psis[KK]/psisOld) < Psitol)) && #Infinity norm for the psis
                   ((sum(abs(1-alpha[,KK]/alphaOld)^convNorm)/n)^(1/convNorm) < tol) && 
                   ((sum(abs(1-cMat[KK,]/cMatOld)^convNorm)/p)^(1/convNorm) < tol) )
} # END while-loop until convergence
  
  }# END for-loop over dimensions

  ## 3) Termination
  
  rownames(alpha) = colnames(covariates)
  colnames(cMat) = colnames(X)
  rownames(cMat) = colnames(alpha) = paste0("Dim",1:k)
  
   returnList = list(converged = convergence, cMat=cMat, psis = psis, thetas = thetas, colRec = colRec, psiRec = psiRec, thetaRec = thetaRec, iter = iterOut-1, X=X, Xorig = Xorig, fit = "RCM_NB_constr", lambdaCol = lambdaCol, rowWeights = rowWeights, colWeights = colWeights,  
       alpha = alpha, alphaRec = alphaRec, covariates = covariates, NB_params = NB_params, NB_params_noLab = NB_params_noLab,
       libSizes = switch(marginEst, "MLE" = exp(logLibSizesMLE), "marginSums" = libSizes), abunds = switch(marginEst, "MLE" = exp(logAbundsMLE), "marginSums" = abunds), 
       confounders = confounders, confParams = confParams)
  }
    if(!all(convergence)){
    warning("Algorithm did not converge for all dimensions! Check for errors or consider changing tolerances or number of iterations")
    }
  return(returnList)
}
#-------------------------------------------#
```

### Zero-inflated Poisson

```{r Zero-inflated poisson 1B1, purl=TRUE, echo = FALSE}
#----------------------------#
#expit
expit=function(x){
  tmp = exp(x)/(1+exp(x))
  tmp[is.na(tmp)]=1 #Adjust for overflow
  tmp}
#-----------------------------------#

## A function to perform the M step: maximize the likelihoods. This will again be an iterative process, estimating the parameters step by step. estimation of poisson and zero-inflated part can occur independently, which opens up opportunities for parallelization.

MstepZIP = function(Z, X, rMat, cMat, tMat, vMat,  k, muMarg,  zeroMarg, psis, chis, lambdaCol, lambdaRow, lambdaColZero, lambdaRowZero, nLambda, rowWeights, colWeights, rMatK, cMatK, vMatK, tMatK, twoCores=TRUE, tol=1e-3, psiTol = 1e-4, chiTol = psiTol, convNorm = 2, maxItMean=5, maxItZeroes= 20, n=n, p=p, global=global, nleqslv.control= nleqslv.control){
  
#Optimization of the mean and zero-inflated components are independent (see Lambert 1992), so fork here
resList = mclapply(mc.cores=twoCores + 1, c(meanEstZIP, ZIestZIP), function(fun){
  fun(X=X, rMat=rMat, cMat=cMat, tMat=tMat, chis=chis, vMat=vMat, zeroMarg = zeroMarg, lambdaCol=lambdaCol, lambdaRow=lambdaRow, lambdaRowZero=lambdaRowZero, lambdaColZero=lambdaColZero, psiTol=psiTol, chiTol=chiTol, tol=tol, convNorm = convNorm, nleqslv.control = nleqslv.control, global=global, nLambda=nLambda, k=k, Z=Z, muMarg=muMarg,n=n, p=p, psis=psis, maxItMean = maxItMean, maxItZeroes = maxItZeroes, rowWeights=rowWeights, colWeights=colWeights, rMatK = rMatK, cMatK = cMatK, tMatK = tMatK, vMatK = vMatK)
})

return(unlist(resList, recursive=FALSE))
}
#--------------------------------------#

# A function to estimate the mean component of the ZIP model by 1B1
meanEstZIP = function(X, rMat, cMat, Z, muMarg, k, global, nleqslv.control, tol, psiTol,lambdaCol, lambdaRow, convNorm,  nLambda, n, p, psis, rowWeights, colWeights, maxItMean = 10, maxItZeroes = 10, rMatK, cMatK,...){
 #Mean component
  
    iter = 1
  while((iter==1 || !converged) && iter<maxItMean){
    
  cat("Inner iteration(mean)", iter, "\n")
    
  psiOld = psis
  rMatOld = rMat
  cMatOld = cMat

    ## Row scores
  cat("Estimating row scores mean \n")
  regRow = cMat*psis
  rMatSol = try(nleqslv(fn = dZipMeanRmat, x = c(rMat, lambdaRow),X=X, reg =regRow, muMarg=muMarg, n=n, k=k, global=global, control = nleqslv.control, jac=ZipJacobianRmat, Z=Z, nLambda=nLambda, rowWeights=rowWeights, rMatK = rMatK)$x, silent=TRUE)
  
  if(class(rMatSol)!="try-error"){
  rMat = matrix(rMatSol[1:n], byrow=FALSE, ncol=1, nrow=n)
  lambdaRow = rMatSol[(n+1):length(rMatSol)]
  }
  
  #Normalize (speeds up algorithm if previous step had not converged)
rMat =  rMat - sum(rMat * rowWeights)/sum(rowWeights) 

rMat = rMat/sqrt(sum(rowWeights * rMat^2))
  
  ## Column scores
  cat("Estimating column scores mean \n")
  regCol = t(t(rMat)*psis)
  cMatSol = try(nleqslv(fn = dZipMeanCmat, x = c(t(cMat), lambdaCol), X=X, reg=regCol, muMarg=muMarg, p=p, k=k, global=global, control = nleqslv.control, jac=ZipJacobianCmat, Z=Z, nLambda=nLambda, colWeights=colWeights, cMatK = cMatK)$x, silent=TRUE)
    if(class(cMatSol)!="try-error"){
  cMat = matrix(cMatSol[1:p], byrow=TRUE, nrow=1, ncol=p)
  lambdaCol = cMatSol[(p+1):length(cMatSol)]
    }
  
cMat = cMat - sum(cMat * colWeights)/sum(colWeights) 
cMat = cMat/sqrt(sum(colWeights * cMat^2))

  ## Psis
 cat("\n Estimating psis (k =",k,") \n")
    
  regPsi =  rMat %*% cMat

  psisSol = try(sort(abs(nleqslv(fn = dZipMeanPsi, x = psis, X=X, reg=regPsi, Z=Z, muMarg=muMarg, global=global, control = nleqslv.control, jac=ZipJacobianPsi)$x), decreasing=TRUE), silent=TRUE)
    if(class(psisSol)!="try-error") psis=psisSol

    converged = all(abs(psiOld-psis) < psiTol) &&  (sum(abs(1-rMat/rMatOld)^convNorm))^(1/convNorm) < tol &&  (sum(abs(1-cMat/cMatOld)^convNorm))^(1/convNorm) < tol 
  iter = iter +1
  }
  
  return(list(cMat=cMat, rMat=rMat, iterMean = iter, psis=psis, convergedMean=converged, lambdaCol = lambdaCol, lambdaRow=lambdaRow))
}
#--------------------------------------#

# A function to estimate the zero inflated component of the ZIP model

ZIestZIP = function(X, Z, muMarg, k, global, nleqslv.control, tol, chiTol, tMat, vMat, chis, zeroMarg, lambdaColZero, lambdaRowZero, convNorm, n, p, nLambda, psis, rowWeightsZeroNum, colWeightsZeroNum, rMatK, cMatK, tMatK, vMatK, maxItMean = 10, maxItZeroes = 10, ...){
  
      iter = 1
  while((iter==1 || !converged) && iter < maxItZeroes){
   chiOld = chis
  tMatOld = tMat
  vMatOld = vMat
  
    cat("Inner iteration(zeroes)", iter, "\n")

    ## Row scoers
    cat("Estimating row scores zeroes \n")
    regRowZero = vMat*chis
    tMatSol = try(nleqslv(fn = dZipMeanTmat, x = c(tMat, lambdaRowZero),  n=n,k=k, reg=regRowZero, global=global, control = nleqslv.control, zeroMarg = zeroMarg, jac=ZipJacobianTmat, Z=Z, nLambda=nLambda, rowWeights = rowWeightsZeroNum, tMatK = tMatK)$x, silent=TRUE)
  if(!inherits(tMatSol,"try-error")){
    tMat = matrix(tMatSol[1:n], byrow=FALSE, ncol=1, nrow=n)
    lambdaRowZero = tMatSol[(n+1):(n+nLambda)]
      }
    
    ## Column scores
    cat("Estimating column scores zeroes \n")
    regColZero = tMat*chis
    vMatSol = try(nleqslv(fn = dZipMeanVmat, x = c(t(vMat), lambdaColZero), reg=regColZero, p=p,k=k, global=global, control = nleqslv.control, zeroMarg = zeroMarg, jac=ZipJacobianVmat, Z=Z, nLambda=nLambda, colWeights=colWeightsZeroNum, vMatK = vMatK)$x, silent=TRUE)
  if(!inherits(vMatSol,"try-error")){
    vMat = matrix(vMatSol[1:p], byrow=TRUE, nrow=1, ncol=p)
    lambdaColZero = vMatSol[(p+1):(p+nLambda)]
  }
    
  # Chis
  cat("Estimating chis (zeroes) \n")
  regChis =  tMat %*% vMat

  chisSol = try(sort(abs(nleqslv(fn = dZipMeanChi, x = chis, reg=regChis, Z=Z, global=global, control = nleqslv.control, zeroMarg = zeroMarg, jac=ZipJacobianChi)$x), decreasing=TRUE), silent=TRUE)
  if(!inherits(chisSol,"try-error")){
    chis=chisSol
  }

  converged = all ((chiOld-chis) < chiTol) &&  (sum(abs(1-tMat/tMatOld)^convNorm))^(1/convNorm) < tol &&  (sum(abs(1-vMat/vMatOld)^convNorm))^(1/convNorm) < tol 
  iter = iter +1
}
  return(list(vMat=vMat, tMat=tMat, iterZI = iter, chis=chis, convergedZI=converged, lambdaColZero=lambdaColZero, lambdaRowZero=lambdaRowZero))
}

###Estimate the offsets
#--------------------------------------#
dZipMeanLibsizes = function(beta, X, Z, reg){
  # @param beta: a vector of logged library size estimates
  # @param y: the nxp data matrix
  # @param reg: the current logged abundance estimates
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  mu = exp(outer(beta, reg, "+"))
  rowSums((1 - Z)*(X - mu))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianLibsizes = function(beta, X, reg, Z){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  mu = exp(outer(beta, reg, "+"))
  diag(rowSums(mu*(Z-1)))
}
#--------------------------------------#
dZipMeanAbunds = function(beta, X, Z, reg){
  # @param beta: a vector of logged library size estimates
  # @param y: the nxp data matrix
  # @param reg: the current logged abundance estimates
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  mu = exp(outer(reg,beta, "+"))
  colSums((1 - Z)*(X - mu))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianAbunds = function(beta, X, reg, Z){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
   mu = exp(outer(reg,beta, "+"))
  diag(colSums(mu*(Z-1)))
}

#All matrices X are considered to be nxp, i.e. samples are rows and taxa are columns

#--------------------------------------#
dZipMeanPsi = function(beta, X, muMarg, Z, reg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  mu = exp(reg* beta) * muMarg
  sum((1 - Z)*(X - mu)*reg)
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianPsi = function(beta, X, reg, muMarg,Z){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  mu = exp(reg* beta) * muMarg
  sum(mu*reg^2*(Z-1))
}

#--------------------------------------#
dZipMeanRmat = function(beta, X, reg, muMarg, n, k, Z, nLambda, rowWeights, rMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  rMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)
  mu = exp(rMat %*% reg) * muMarg
  
  lambda1 = beta[n+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[n+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(n+3):length(beta)]}

  score = tcrossprod(reg ,(1-Z)*(X-mu)) + c(rowWeights*(lambda1 + lambda2*2*rMat + rMatK %*% lambda3))

  center = sum(rMat*rowWeights)
  unitSum = sum(rMat^2*rowWeights)-1
  if(k==1){return(c(score,center, unitSum))}
  orthogons = apply(rMatK, 2, function(x){
    sum(rMat*x*rowWeights)
  })
      return(c(score,center, unitSum, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianRmat = function(beta, X, reg, muMarg, n,k, nLambda, Z, rowWeights, rMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  rMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)
  mu = exp(rMat %*% reg) * muMarg

  Jac = matrix(0, nrow= n + nLambda, ncol=n + nLambda)
  #The symmetric jacobian matrix. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag/dr_{ik}dlambda_{1k}
  Jac[1:n, n+1] = rowWeights
  #dLag/dr_{ik}dlambda_{2k}
  Jac[1:n, n+2] = 2 *rMat*rowWeights
  
  if(k>1){
    Jac[1:n,(n+3):(n+nLambda)] = apply(rMatK, 2, function(x){rowWeights*x})
  }
  #Symmetrize
  Jac = Jac + t(Jac)
    diag(Jac[1:n,1:n]) = c(tcrossprod(-mu*(1-Z), reg^2) + 2*rowWeights*beta[n+2])
  Jac

}
#--------------------------------------#
dZipMeanCmat = function(beta, X, muMarg, p,k, Z, nLambda, colWeights, reg, cMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  cMat = matrix(beta[1:p], byrow=TRUE, ncol=p, nrow=1)
  mu = exp(reg %*% cMat) * muMarg
  
  lambda1 = beta[p+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[p+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(p+3):length(beta)]}
  
  score = crossprod(reg,(1-Z)*(X-mu)) + colWeights*(lambda1 + lambda2*2*cMat + lambda3 %*% cMatK)
  
  center = sum(colWeights*cMat)
  unitSum = sum(colWeights*cMat^2)-1
  if(k==1){return(c(score,center, unitSum))}
  orthogons = apply(cMatK,1,function(x){
      sum(cMat*x*colWeights)
  })
    return(c(score,center, unitSum, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianCmat = function(beta, X, psis, rMat, colWeights, k, p, muMarg, Z, nLambda, reg, cMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  cMat = matrix(beta[1:p], byrow=TRUE, ncol=p, nrow=1)
  mu = exp(reg %*% cMat) * muMarg

Jac = matrix(0, nrow= p + nLambda, ncol = p + nLambda)
  #The suXmmetric jacobian matrix. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag/dr_{ik}dlambda_{1k}
  Jac[1:p,(p+1)] = colWeights
  #Jac[1:(p*k),(p*k+1):((p+1)*k)] = sapply(1:k, function(K){c(rep(0,(K-1)*p),colWeights,rep(0,(k-K)*p))})
  Jac[1:p,p+2] = colWeights*2 *cMat
  
    #dLag/ds_{ik}dlambda_{3kk'}
  if(k>1){
    Jac[1:p,(p+3):(p+nLambda)] = apply(cMatK, 1,function(x){
      colWeights*x
    })
  }
  #Symmetrize
  Jac = Jac + t(Jac)

  diag(Jac[1:p,1:p]) = c(crossprod(-mu*(1-Z), reg^2)) + 2*beta[p+2]*colWeights
  Jac
}
#--------------------------------------#
#Estimate the offsets for the zeroes
dZipZeroCol = function(beta, Z, reg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  expitZero = expit(outer(reg, beta, "+"))
  colSums((Z-expitZero))
}

#--------------------------------------#
ZipJacobianZeroCol = function(beta, Z, reg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  expZero = exp(outer(reg, beta, "+"))
  tmp=expZero/(1+expZero)^2
  -diag(colSums(tmp))
}
#--------------------------------------#
#Estimate the offsets for the zeroes
dZipZeroRow = function(beta, Z, reg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  expitZero = expit(outer(beta,reg, "+"))
  rowSums((Z-expitZero))
}

#--------------------------------------#
ZipJacobianZeroRow = function(beta, Z, reg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  expZero = exp(outer(beta,reg, "+"))
  tmp=expZero/(1+expZero)^2
  -diag(rowSums(tmp))
}
#--------------------------------------#
dZipMeanChi = function(beta, Z, reg, zeroMarg, nLambda){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  GZero = expit(reg*beta + logit(zeroMarg))
  sum((Z-GZero)*reg)
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianChi = function(beta, Z, reg, zeroMarg, nLambda){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  expZero = exp(reg* beta + logit(zeroMarg))
  tmp=expZero/(1+expZero)^2
  -sum(reg^2*tmp)
}

#--------------------------------------#
dZipMeanTmat = function(beta, reg, k,n, Z, zeroMarg, nLambda, rowWeights, tMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  tMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)
  muZero = expit(tMat %*% reg+logit(zeroMarg))
  
  lambda1 = beta[n+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[n+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(n+3):length(beta)]}
  score = 
    tcrossprod((Z-muZero), reg) + c(rowWeights*(lambda1 + lambda2*2*tMat + tMatK %*% lambda3))

  
  center = sum(tMat*rowWeights)
  unitSum = sum(tMat^2*rowWeights)-1
  if(k==1){return(c(score,center, unitSum))}
  orthogons = apply(tMatK, 2, function(x){
    sum(tMat*x*rowWeights)
  })
    return(c(score,center, unitSum, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianTmat = function(beta, reg, k, n, Z, zeroMarg, nLambda, rowWeights, tMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param k: a scalar, dimension of the RC solution

  tMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)
  muZero = exp(tMat %*% reg + logit(zeroMarg))

  Jac = matrix(0, nrow= n + nLambda, ncol= n + nLambda)
  #The suymmetric jacobian matrix. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag/dr_{ik}dlambda_{1k}
  Jac[1:n, n+1] = rowWeights
  #dLag/dr_{ik}dlambda_{2k}
  Jac[1:n, n+2] = 2*tMat*rowWeights
  tmp=muZero/(1+muZero)^2
  #dLag/dr_{ik}dlambda_{3kk'}
  if(k>1){
    Jac[1:n,(n+3):(n+nLambda)] = apply(tMatK, 2, function(x){rowWeights*x})
  }
  #Symmetrize
  Jac = Jac + t(Jac)
    diag(Jac[1:n,1:n]) = c(-tcrossprod(tmp, reg^2) + 2*beta[n+2]*rowWeights)
    
  Jac
}
#--------------------------------------#
dZipMeanVmat = function(beta, reg, k, p, Z, zeroMarg, nLambda, colWeights, vMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  vMat = matrix(beta[1:p], byrow=TRUE, ncol=p, nrow=1)
  muZero = exp(reg %*% vMat + logit(zeroMarg))

  lambda1 = beta[p+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[p+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(p+3):length(beta)]}
  
  score = 
    crossprod(reg,(Z-muZero/(1+muZero))) + colWeights*(lambda1 + lambda2*2*vMat + (lambda3 %*% vMatK))
  
  center = sum(colWeights*vMat)
  unitSum = sum(colWeights*vMat^2)-1
  if(k==1){return(c(score,center, unitSum))}
  orthogons = apply(vMatK, 1,function(x){
    sum(x*vMat*colWeights)})
  
    return(c(score, center, unitSum, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianVmat = function(beta, reg, k, p, Z, zeroMarg, nLambda, colWeights, vMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  vMat = matrix(beta[1:p], byrow=TRUE, ncol=p, nrow=1)
  muZero = exp(reg %*% vMat +logit(zeroMarg))

    Jac = matrix(0, nrow= p + nLambda, ncol=p + nLambda)
  #The suXmmetric jacobian matrix. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag/dr_{ik}dlambda_{1k}
  Jac[1:p,(p+1)] = colWeights
  #dLag/dr_{ik}dlambda_{2k}
  Jac[1:p,p+2] = colWeights*2 *vMat
tmp=muZero/(1+muZero)^2
  if(k>1){
    Jac[1:p,(p+3):(p+nLambda)] = apply(vMatK, 1,function(x){
      colWeights*x
    })
  }
  #Symmetrize
  Jac = Jac + t(Jac)
    diag(Jac[1:p,1:p]) = c(t( -crossprod(reg^2,tmp)) + colWeights *2*beta[p+2])
    
  Jac
}
#Weighing by abs or relabunds really doesn't matter, only chis and psis get more inflated and deflated
logit=function(x){log(x/(1-x))}

RCM_ZIP = function(X, k, rowWeights, colWeights, weightsChar, tol = 1e-3, maxItOut = 500, psiTol = 1e-4, chiTol=psiTol, verbose = TRUE, global ="dbldog", nleqslv.control = list(), method="Broyden", twoCores = FALSE, convNorm = 2,  maxItMean = 20, maxItZeroes=30, ZIPRCM=NULL){

    # @param X: a nxp data matrix
  # @param k: a scalar, number of dimensions in the RC(M) model
  # @param tol(optional): a scalar, the relative convergende tolerance for the row scores and column scores parameters, defaults to 1e-3  
  # @param Psitol(optional): a scalar, the relative convergence tolerance for the psi parameters, defaults to 1e-4
  # @param maxItOut(optional): an integer, the maximum number of iteration in the outer loop, defaults to 50
  # @param libSizes(optional) : a vector of length n with (known) library sizes. If not provided, rowSums of x are used
  # @param verbose(optional): a boolean, should information on iterations be printed? Defaults to TRUE
  # @param method(optional): Method for jacobian estimation , see nleqslv. Defaults to Broyden. The difference with the newton method is that the Jacobian is not recalculated at every iteration
  # @param global(optional): global strategy for solving non-linear systems , see nleqslv
  # @param nleqslv.control: a list with control options, see nleqslv
  # @param lambdaRow: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers for the row scores
  # @param lambdaCol: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers for the column scores
  # @param rMatInit(optional): a nxk matrix with initial row scores. If not provided values from the singular value decomposition will be used as starting values
  # @param cMatInit(optional): a pxk matrix with initial column scores. If not provided values from the singular value decomposition will be used as starting values
  # @param psisInit(optional): a vector of length k with inital values for the importance parameters psi. If not provided values from the singular value decomposition will be used as starting values
  # @param dispFreq: a scalar, how many iterations the algorithm should wait before reestimationg the dispersions
  # @param convNorm: a scalar, the norm to use to determine convergence

  # @return A list with elements:
  # @return psis: a vector of length k with estimates for the importance parameters psi
  # @return thetas: a vector of length p with estimates for the overdispersion
  # @return rMat: a nxk matrix with estimated row scores
  # @return cMat: a pxk matrix with estimated column scores
  # @return converged: a boolean indicating if the algorithm converged
  # @return rowRec: a n x k x maxItOut array with a record of all rMat estimates through the iterations 
  # @return colRec: a k x p x maxItOut array with a record of all cMat estimates through the iterations
  # @return psiRec.: a k x maxItOut array with a record of all psi estimates through the iterations
  
  abunds = colSums(X)/sum(X)
  libSizes = rowSums(X)
  n=NROW(X)
  p=NCOL(X)
  
  logLibSizesMLE = log(libSizes)
  logAbundsMLE = log(abunds)
  # logitZeroRows = logit(rep(0.25,n)) #Start with a 25 % chance on a structural zero
  logitZeroCols = logit(rep(0.25,p))

  initIter = 1
  
   while((initIter ==1) || ((initIter <= maxItOut) && (!convergenceInit))){
  cat("Starting iteration ",initIter, " \n")
     
     libsOld = logLibSizesMLE
     absOld = logAbundsMLE
     # zeroRowsOld = logitZeroRows
     zeroColsOld = logitZeroCols
     Z = matrix(0,n,p)
     
    #E-step
  expMu = exp(outer(logLibSizesMLE, logAbundsMLE, "+"))
  regZero = matrix(logitZeroCols, n, p, byrow=TRUE)
  Z[X==0] = (1+exp(-regZero-expMu))[X==0]^(-1)
     
     #M-step
  
  initIterMean = 1
  while((initIterMean ==1) || ((initIterMean <= maxItMean) && (!convergenceInitMean))){
    cat("Mean iteration ",initIterMean, "\n")
     libsOldIn = logLibSizesMLE
     absOldIn = logAbundsMLE
 libsTmp = try(nleqslv(fn = dZipMeanLibsizes, x = logLibSizesMLE, X = X, reg=logAbundsMLE, global=global, control = nleqslv.control, jac=ZipJacobianLibsizes, method=method, Z=Z)$x, silent=TRUE)
 if(class(libsTmp)!="try-error"){ logLibSizesMLE = libsTmp}
  absTmp = try(nleqslv(fn = dZipMeanAbunds, x = logAbundsMLE, X = X, reg=logLibSizesMLE, global=global, control = nleqslv.control, jac=ZipJacobianAbunds, method=method, Z=Z)$x, silent=TRUE)
 if(class(absTmp)!="try-error"){ logAbundsMLE = absTmp}
  
  initIterMean = initIterMean + 1
  
   convergenceInitMean = ((initIterMean <= maxItMean) && 
                    ((sum(abs(1-logLibSizesMLE/libsOldIn)^convNorm)/n)^(1/convNorm) < tol) && 
                   ((sum(abs(1-logAbundsMLE/absOldIn)^convNorm)/p)^(1/convNorm) < tol))
  }
  
#     initIterZero = 1
#   while((initIterZero ==1) || ((initIterZero <= maxItZeroes) && (!convergenceInitZeroes))){
#         cat("Zero iteration ",initIterZero, "\n")
    # zeroRowsOldIn = logitZeroRows
    # zeroColsOldIn = logitZeroCols
  
#   zeroRowsTmp = try(nleqslv(fn = dZipZeroRow, x = logitZeroRows,   reg=logitZeroCols, global=global, control = nleqslv.control, jac=ZipJacobianZeroRow, method=method, Z=Z)$x, silent=TRUE)
#  if(class(zeroRowsTmp)!="try-error"){ logitZeroRows = zeroRowsTmp} # We don't let zero-inflation depend on the samples, there is no baseline absence rate for a sample

  zeroColsTmp = try(nleqslv(fn = dZipZeroCol, x = logitZeroCols, reg=rep.int(0L,n), global=global, control = nleqslv.control, jac=ZipJacobianZeroCol, method=method, Z=Z)$x, silent=TRUE)
 if(class(zeroColsTmp)!="try-error"){ logitZeroCols = zeroColsTmp}
  
    # initIterZero = initIterZero + 1
  
#    convergenceInitZeroes = ((initIterZero <= maxItZeroes) && 
#                    # ((sum(abs(1-logitZeroRows/zeroRowsOldIn)^convNorm)/p)^(1/convNorm) < tol) && 
#                    ((sum(abs(1-logitZeroCols/zeroColsOldIn)^convNorm)/p)^(1/convNorm) < tol))
#   }
 
  initIter = initIter + 1
  
  convergenceInit = ((initIter <= maxItOut) && 
                    ((sum(abs(1-logLibSizesMLE/libsOld)^convNorm)/n)^(1/convNorm) < tol) && 
                   ((sum(abs(1-logAbundsMLE/absOld)^convNorm)/p)^(1/convNorm) < tol) && 
                   # ((sum(abs(1-logitZeroRows/zeroRowsOld)^convNorm)/p)^(1/convNorm) < tol) && 
                   ((sum(abs(1-logitZeroCols/zeroColsOld)^convNorm)/p)^(1/convNorm) < tol))
     }
    muMarg = exp(outer(logLibSizesMLE, logAbundsMLE, "+")) 
    zeroMarg = expit(matrix(logitZeroCols, n, p, byrow=TRUE))#The marginals to be used as expectation under independence. These are augmented with the previously estimated dimensions every time

  rowRec = rowRecZeroes = array(0,dim=c(n,k, maxItOut))
  colRec = colRecZeroes = thetaRec = array(0,dim=c(k,p, maxItOut))
  psiRec = matrix(0, nrow=k,ncol=maxItOut)
  convergence = rep(FALSE, k)
  iterOut = rep(1,k)
  
  #If previous fit provided with higher or equal dimension, stop here
  if((!is.null(ZIPRCM)) ){
    if(ZIPRCM$fit != "RCM_ZIP"){
      stop("Fit provided is not of same type as the one requested! \n")
    } else if((k <= ZIPRCM$k)) {
      # stop("Fit provided is already of the required dimension or higher! \n")
    } else{
          for(i in c("rMat","cMat","psis","lambdaCol","lambdaRow", "lambdaRowZero","lambdaColZero","tMat","vMat","chis","Z","zeroMarg")){
      assign(i, ZIPRCM[[i]])
    }
    }
  #Otherwise try to use intelligent starting values
  } else{
      #Depending on the weighting schemes, use other starting values
   svdX = svd(diag(1/sqrt(libSizes)) %*% (X-muMarg)*(1-zeroMarg) %*% diag(1/sqrt(colSums(X))))#switch(weightsChar,
#                 "marginalmarginal" = svd(diag(1/libSizes) %*% (X-muMarg) %*% diag(1/colSums(X))),
#                 "marginaluniform" = svd(diag(1/libSizes) %*% (X-muMarg)),
#                 "uniformmarginal" = svd((X-muMarg) %*% diag(1/colSums(X))),
#                 "uniformuniform" = svd(X-muMarg))
  rMat = svdX$u[,1:k,drop=FALSE]
  cMat = t(svdX$v[,1:k,drop=FALSE])
  psis = log(svdX$d[1:k])
 
#   #Redistribute some weight to fit the constraints 
#   psis = c(psis *t(apply(cMat, 1, function(colS){
#       sqrt(sum(colWeights * colS^2))
#   })) * apply(rMat, 2, function(rowS){
#       sqrt(sum(rowWeights * rowS^2))
#   }))
# 
# #Normalize  
# cMat = t(apply(cMat, 1, function(colS){
#       colS/sqrt(sum(colWeights * colS^2))
#   }))
# rMat = apply(rMat, 2, function(rowS){
#       rowS/sqrt(sum(rowWeights * rowS^2))
#   })

  #Initial estimates for zeroes is also based on an svd

  Xzeroes = X==0
  
  svdZero = svd(diag(sqrt(1/sqrt(libSizes))) %*%(Xzeroes-zeroMarg)%*% diag(1/sqrt(colSums(X))))

  tMat = svdZero$u[,1:k, drop=FALSE]
  vMat = t(svdZero$v[,1:k, drop=FALSE])
  chis = log(svdZero$d[1:k])

#Redistribute some weight to fit the constraints 
# chis = c(chis *t(apply(vMat, 1, function(colS){
#       sqrt(sum(colWeights * colS^2))
#   })) * apply(tMat, 2, function(rowS){
#       sqrt(sum(rowWeights * rowS^2))
#   }))
#   
# #Normalize  
# vMat = t(apply(vMat, 1, function(colS){
#       colS/sqrt(sum(colWeights * colS^2))
#   }))
# tMat = apply(tMat, 2, function(rowS){
#       rowS/sqrt(sum(rowWeights * rowS^2))
#   })
  
  lambdaRow = lambdaCol = lambdaColZero=lambdaRowZero = rep.int(0,2*k+k*(k-1)/2)
    }
  
  rowRec = rowRecZero = array(0,dim=c(NROW(X),k, maxItOut))
  colRec = colRecZero = array(0,dim=c(k,NCOL(X), maxItOut))
  psiRec = chiRec = matrix(0,ncol=maxItOut, nrow=k)
  
   if(!is.null(ZIPRCM)){ #If fit provided, replace lower dimension starting values
   Kprev = ZIPRCM$k
   rMat[,1:Kprev] = ZIPRCM$rMat
   rowRec[,1:Kprev,] = ZIPRCM$rowRec
   cMat[1:Kprev,] = ZIPRCM$cMat
   colRec[1:Kprev,,] = ZIPRCM$colRec
   psis[1:Kprev] = ZIPRCM$psis
   psiRec[1:Kprev,] = ZIPRCM$psiRec
   lambdaCol[1:(Kprev*(2+(Kprev-1)/2))] = ZIPRCM$lambdaCol
   lambdaRow[1:(Kprev*(2+(Kprev-1)/2))] = ZIPRCM$lambdaRow
   tMat[,1:Kprev] = ZIPRCM$tMat
   rowRecZero[,1:Kprev,] = ZIPRCM$rowRecZero
   vMat[1:Kprev,] = ZIPRCM$vMat
   colRecZero[1:Kprev,,] = ZIPRCM$colRecZero
   chis[1:Kprev] = ZIPRCM$chis
   chiRec[1:Kprev,] = ZIPRCM$chiRec
   lambdaColZero[1:(Kprev*(2+(Kprev-1)/2))] = ZIPRCM$lambdaColZero
   lambdaRowZero[1:(Kprev*(2+(Kprev-1)/2))] = ZIPRCM$lambdaRowZero
   convergence[1:Kprev] = ZIPRCM$converged
   iterOut[1:Kprev] = ZIPRCM$iter
   zeroMarg = ZIPRCM$zeroMarg
 }

  minK = ifelse(is.null(ZIPRCM),1,Kprev+1)
  for (KK in minK:k){
  
  cat("Dimension" ,KK, "is being esimated \n")
    
  #Modify offsets if needed
  if(KK>1){
    muMarg = muMarg * exp(rMat[,(KK-1), drop=FALSE] %*% (cMat[(KK-1),, drop=FALSE]*psis[(KK-1)]))
    zeroMarg = expit(logit(zeroMarg) + tMat[,(KK-1), drop=FALSE] %*% (vMat[(KK-1),, drop=FALSE]*chis[(KK-1)]))
  }
    #A lambda parameter
    nLambda = KK + 1
    
    #The location of the lambda parameters
    idK = seq_k(KK)
    Z = matrix(0, n,p)

    ## 2) Propagation
  
    while((iterOut[KK] ==1) || ((iterOut[KK] <= maxItOut) && (!convergence[KK])))
    {
    
  if(verbose && iterOut[KK]%%1 == 0){
  cat("\n","Outer Iteration", iterOut[KK], "\n","\n")
    if(iterOut[KK]!=1){
  cat("Old psi-estimate: ", psiOld, "\n")
  cat("New psi-estimate: ", psis[KK], "\n")
  cat("Old chi-estimates: ", chiOld, "\n")
  cat("New chi-estimates: ", chis[KK], "\n")
    }
  }
  ## 2)a. Store old parameters
  psiOld = psis[KK]
  rMatOld = rMat[,KK]
  cMatOld = cMat[KK,]
  
  chiOld = chis[KK]
  tMatOld = tMat[,KK]
  vMatOld = vMat[KK,]

  #Expectation
  expMu = muMarg* exp(outer(rMat[,KK], cMat[KK,]*psis[KK]))
  regZero = logit(zeroMarg) + outer(tMat[,KK], vMat[KK,]*chis[KK])
  Z[X==0] = (1+exp(-regZero-expMu))[X==0]^(-1)
  
   #Maximization
  Mlist = MstepZIP(Z = Z, X = X, rMat = rMat[,KK, drop=FALSE], cMat = cMat[KK,, drop=FALSE], tMat = tMat[,KK, drop=FALSE], vMat = vMat[KK,, drop=FALSE], k = KK, n=n, p=p, zeroMarg = zeroMarg, psis = psis[KK],chis = chis[KK], twoCores=twoCores, tol = tol, psiTol = psiTol, chiTol = chiTol, convNorm = convNorm, global=global, nLambda = nLambda, nleqslv.control = nleqslv.control, lambdaCol = lambdaCol[idK], lambdaRow=lambdaRow[idK], lambdaColZero = lambdaColZero[idK], lambdaRowZero = lambdaRowZero[idK], maxItMean = maxItMean, maxItZeroes = maxItZeroes, muMarg=muMarg, colWeights = colWeights, rowWeights = rowWeights, rMatK = rMat[,1:(KK-1), drop=FALSE], cMatK = cMat[1:(KK-1),, drop=FALSE], tMatK = tMat[,1:(KK-1), drop=FALSE], vMatK = vMat[1:(KK-1),, drop=FALSE])
#   
#   cat("Mlist:", str(Mlist), "\n")

  #Assign outcomes to tracking vectors and to this environment
   lambdaCol[idK] = Mlist$lambdaCol
   lambdaRow[idK] = Mlist$lambdaRow
   lambdaColZero[idK] = Mlist$lambdaColZero
   lambdaRowZero[idK] = Mlist$lambdaRowZero

  rowRec[,KK, iterOut[KK]] = rMat[,KK] = Mlist$rMat
  colRec[KK,, iterOut[KK]] = cMat[KK,] = Mlist$cMat
  rowRecZero[,KK, iterOut[KK]] = tMat[,KK] = Mlist$tMat
  colRecZero[KK,, iterOut[KK]] = vMat[KK,] = Mlist$vMat
  psiRec[KK, iterOut[KK]] = psis[KK] = Mlist$psis
  chiRec[KK, iterOut[KK]] = chis[KK] = Mlist$chis
  
  ## 2)f. Change iterator
    iterOut[KK] = iterOut[KK] + 1
    
  ##Check convergence  (any numbered norm for row and column scores)
  convergence[KK] = (iterOut[KK] <= maxItOut) && 
                    (all(abs(1-psis/psiOld) < psiTol)) &&
                    ((sum((1-rMatOld/rMat)^convNorm)/n)^(1/convNorm) < tol) && 
                    ((sum((1-cMatOld/cMat)^convNorm)/p)^(1/convNorm) < tol)  && 
                    (all(abs(1-chis/chiOld) < chiTol)) &&  
                    (sum(abs(1-tMat/tMatOld)^convNorm)/n)^(1/convNorm) < tol &&
                    (sum(abs(1-vMat/vMatOld)^convNorm)/p)^(1/convNorm) < tol 
} # END while-loop until convergence
  } # END for-loop over dimensions
  
  ## 3) Termination
  rownames(rMat) = rownames(X)
  colnames(cMat) = colnames(X)
  rownames(cMat) = colnames(rMat) = paste0("Dim",1:k)
  
  if(!convergence[KK] ){
    warning("Algorithm did not converge! Check for errors or consider changing tolerances or number of iterations")
  }
    return(list(converged = convergence,rMat=rMat, cMat=cMat, psis = psis, X=X,
                rowRec = rowRec, colRec = colRec, psiRec = psiRec, lambdaRow = lambdaRow, lambdaCol = lambdaCol, lambdaRowZero = lambdaRowZero, lambdaColZero = lambdaColZero, chis = chis, tMat = tMat, vMat = vMat, zeroMarg = zeroMarg, chiRec = chiRec, rowRecZero = rowRecZero, colRecZero = colRecZero, iter=iterOut-1, Z=Z, fit="RCM_ZIP", libSizesMLE = exp(logLibSizesMLE), abundsMLE = exp(logAbundsMLE), taxaZeroes = expit(logitZeroCols)))
}
```

## Zero-inflated negative binomial

```{r ZINB, purl=TRUE, echo=FALSE}
###Estimate the offsets
#--------------------------------------#
dZinbMeanLibsizes = function(beta, X, Z, reg, thetas){
  # @param beta: a vector of logged library size estimates
  # @param y: the nxp data matrix
  # @param reg: the current logged abundance estimates
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  mu = exp(outer(beta, reg, "+"))
  rowSums((1-Z)*(X-mu)/(1+t(t(mu)/thetas)))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZinbJacobianLibsizes = function(beta, X, reg, Z, thetas){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  mu = exp(outer(beta, reg, "+"))
  diag(rowSums(((1+t(t(X)/thetas))*mu/(1+t(t(mu)/thetas))^2)*(Z-1)))
}
#--------------------------------------#
dZinbMeanAbunds = function(beta, X, Z, reg, thetas){
  # @param beta: a vector of logged library size estimates
  # @param y: the nxp data matrix
  # @param reg: the current logged abundance estimates
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  mu = exp(outer(reg,beta, "+"))
  colSums((1-Z)*(X-mu)/(1+t(t(mu)/thetas)))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZinbJacobianAbunds = function(beta, X, reg, Z, thetas){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
   mu = exp(outer(reg,beta, "+"))
  diag(colSums(((1+t(t(X)/thetas))*mu/(1+t(t(mu)/thetas))^2)*(Z-1)))
}

#--------------------------------------#
#Estimate the offsets for the zeroes
dZinbZeroCol = function(beta, Z, reg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  expitZero = expit(outer(reg, beta, "+"))
  colSums((Z-expitZero))
}

#--------------------------------------#
ZinbJacobianZeroCol = function(beta, Z, reg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  expZero = exp(outer(reg, beta, "+"))
  tmp=expZero/(1+expZero)^2
  -diag(colSums(tmp))
}

#--------------------------------------#
#Estimate the offsets for the zeroes
dZinbZeroRow = function(beta, Z, reg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  expitZero = expit(outer(beta,reg, "+"))
  rowSums((Z-expitZero))
}

#--------------------------------------#
ZinbJacobianZeroRow = function(beta, Z, reg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  expZero = exp(outer(beta,reg, "+"))
  tmp=expZero/(1+expZero)^2
  -diag(rowSums(tmp))
}

#-----------------------------------#
## A function to perform the E-step

EstepNB  = function(X, rMat, cMat, tMat, vMat, muMarg, zeroMarg, psis, chis, thetas){
  
  # @return: The values of Z
  expMu = exp(rMat %*% (cMat*psis)) * muMarg
  pZero =  expit(tMat %*% (vMat * chis) + logit(zeroMarg))

  Z = X
  Z[X>0] = 0
  thetaMat=matrix(thetas, ncol=ncol(X), nrow=nrow(X), byrow=TRUE)
  d0=dnbinom(0,mu=expMu, size=thetaMat)
  Z[X==0] = (pZero/((1-pZero)*d0 + pZero))[X==0]
  Z
}
#-----------------------------------#

## A function to perform the M step: maximize the likelihoods. This will again be an iterative process, estimating the parameters step by step. estimation of poisson and zero-inflated part can occur independently, which opens up opportunities for parallelization.

MstepNB = function(Z, X, rMat, cMat, tMat, vMat, muMarg, k,  zeroMarg, psis, chis, lambdaCol, lambdaRow, lambdaColZero, lambdaRowZero, twoCores=TRUE, tol=1e-3, psiTol = 1e-4, chiTol = psiTol, convNorm = 2 , maxItMean=20 , maxItZeroes= 20,n, p, global=global, nleqslv.control= nleqslv.control, nLambda, thetas, dispFreq,rowWeights, colWeights, rMatK, cMatK, tMatK, vMatK){
  
#Optimization of the mena and zero-inflated components are independent (see Lambert 1992), so fork here
resList = mclapply(mc.cores= 1+twoCores, c(meanEstZINB, ZIestNB), function(fun){
  fun(X=X, rMat=rMat, cMat=cMat, tMat=tMat, chis=chis, vMat=vMat, zeroMarg = zeroMarg, lambdaCol=lambdaCol, lambdaRow=lambdaRow, lambdaRowZero=lambdaRowZero, lambdaColZero=lambdaColZero, psiTol=psiTol, chiTol=chiTol, tol=tol, convNorm = convNorm, nleqslv.control = nleqslv.control, global=global, nLambda=nLambda, k=k, Z=Z, muMarg=muMarg,n=n, p=p, psis=psis, maxItMean = maxItMean, maxItZeroes = maxItZeroes, thetas=thetas, dispFreq=dispFreq, rowWeights = rowWeights, colWeights=colWeights, rMatK = rMatK, cMatK = cMatK, tMatK = tMatK, vMatK = vMatK)
})

return(unlist(resList, recursive=FALSE))
}
#--------------------------------------#

# A function to estimate the mean component of the ZIP model
meanEstZINB = function(X, rMat, cMat, Z, muMarg,  k, global, nleqslv.control, tol, psiTol, thetas, lambdaCol, lambdaRow, convNorm, dispFreq, nLambda, n, p, psis, maxItMean = 10, maxItZeroes = 10, rowWeights, colWeights, rMatK, cMatK,...){
 #Mean component
  
    iter = 1
  while((iter==1 || !converged) && iter<=maxItMean){
    
  cat("Inner iteration(mean)", iter, "\n")
    
  psiOld = psis
  rMatOld = rMat
  cMatOld = cMat

  cat("Estimating overdispersions \n")
  if(iter==1 | iter %% dispFreq ==0){ #Again too slow and unnecessary to reestimate overdispersions every time
  thetasTry = try(estDisp(X=X, cMat=cMat, rMat=rMat, muMarg=muMarg, psis=psis, dispWeights=t(1-Z)), silent=TRUE)
  if(class(thetasTry)!="try-error") thetas=thetasTry
  }
  
  cat("Estimating psis ( k =",k,") \n")
  regPsis = rMat %*% cMat
  psisSol = try(abs(nleqslv(fn = dZinbMeanPsi, reg=regPsis, x = psis, X=X, Z=Z, muMarg=muMarg, global=global, control = nleqslv.control, jac=ZinbJacobianPsi, thetas=thetas)$x), silent=TRUE)
    if(class(psisSol)!="try-error") psis=psisSol

  cat("Estimating row scores mean \n")
  regRows = cMat*psis
  rMatSol = try(nleqslv(fn = dZinbMeanRmat, x = c(rMat, lambdaRow), X=X, reg =regRows, muMarg=muMarg, k=k, n=n, global=global, control = nleqslv.control, jac=ZinbJacobianRmat, Z=Z, nLambda=nLambda, thetas=thetas, rowWeights=rowWeights, rMatK = rMatK)$x, silent=TRUE)
  if(class(rMatSol)!="try-error"){
  rMat = matrix(rMatSol[1:n], byrow=FALSE, ncol=1, nrow=n)
  lambdaRow = rMatSol[(n+1):(n+nLambda)]
  }
  
  ## Column scores
  cat("Estimating column scores mean \n")
  regCols = rMat*psis
  cMatSol = try(nleqslv(fn = dZinbMeanCmat, x = c(cMat, lambdaCol), reg = regCols, X = X, muMarg = muMarg, k = k, p = p, global = global, control = nleqslv.control, jac = ZinbJacobianCmat, Z = Z, nLambda = nLambda, thetas = thetas, colWeights=colWeights, cMatK = cMatK)$x, silent=TRUE)
    if(class(cMatSol)!="try-error"){
  cMat = matrix(cMatSol[1:p], byrow=TRUE, nrow=1, ncol=p)
  lambdaCol = cMatSol[(p+1):(p+nLambda)]
    }

    converged = all(abs(psiOld-psis) < psiTol) &&  (sum(abs(1-rMat/rMatOld)^convNorm))^(1/convNorm) < tol &&  (sum(abs(1-cMat/cMatOld)^convNorm))^(1/convNorm) < tol 
  iter = iter +1
  }
    
  return(list(cMat=cMat, rMat=rMat, iterMean = iter, psis=psis, convergedMean=converged, lambdaCol = lambdaCol, lambdaRow=lambdaRow))
}
#--------------------------------------#

# A function to estimate the zero inflated component of the ZIP model
ZIestNB = function(X, Z, k, global, nleqslv.control, tol,  chiTol, tMat, vMat, chis, zeroMarg, lambdaColZero, lambdaRowZero, convNorm, n, p,  nLambda, rowWeights, colWeights, thetas, tMatK, vMatK, maxItMean = 10, maxItZeroes = 10,  ...){
  
      iter = 1
  while((iter==1 || !converged) && iter<=maxItZeroes){
   chiOld = chis
  tMatOld = tMat
  vMatOld = vMat
  
    cat("Inner iteration(zeroes)", iter, "\n")

  # Zero component
    ## Chis
      cat("Estimating chis (zeroes) \n")
      regChis = tMat %*% vMat
  chisSol = try(abs(nleqslv(fn = dZinbZeroChi, reg=regChis, x = chis, Z=Z, global=global, control = nleqslv.control, zeroMarg=zeroMarg, jac=ZinbJacobianChi)$x), silent=TRUE)
  if(!inherits(chisSol,"try-error")){
    chis=chisSol
  }
  
    ## Row scoers
    cat("Estimating row scores zeroes \n")
    regRows = vMat*chis
    tMatSol = try(nleqslv(fn = dZinbZeroTmat, x = c(tMat, lambdaRowZero), k=k, n=n, global=global, control = nleqslv.control, zeroMarg=zeroMarg, reg=regRows, jac=ZinbJacobianTmat, Z=Z, nLambda=nLambda, rowWeights=rowWeights, tMatK= tMatK)$x, silent=TRUE)
  if(!inherits(tMatSol,"try-error")){
    tMat = matrix(tMatSol[1:n], byrow=FALSE, ncol=1, nrow=n)
    lambdaRowZero = tMatSol[(n+1):(nLambda+n)]
      }
    
    ## Column scores
      cat("Estimating column scores zeroes \n")
      regCols = tMat*chis
    vMatSol = try(nleqslv(fn = dZinbZeroVmat, x = c(vMat, lambdaColZero), reg=regCols, k=k, p=p, global=global, control = nleqslv.control, zeroMarg=zeroMarg, jac=ZinbJacobianVmat, Z=Z, nLambda=nLambda, colWeights=colWeights, vMatK = vMatK)$x, silent=TRUE)
  if(!inherits(vMatSol,"try-error")){
    vMat = matrix(vMatSol[1:p], byrow=TRUE, nrow=1, ncol=p)
    lambdaColZero = vMatSol[(p+1):(p+nLambda)]
          }

  converged = all ((chiOld-chis) < chiTol) &&  (sum(abs(1-tMat/tMatOld)^convNorm))^(1/convNorm) < tol &&  (sum(abs(1-vMat/vMatOld)^convNorm))^(1/convNorm) < tol 
  iter = iter +1
}
  return(list(vMat=vMat, tMat=tMat, iterZI = iter, chis=chis, convergedZI=converged, lambdaColZero=lambdaColZero, lambdaRowZero=lambdaRowZero))
}
#--------------------------------------#

dZinbMeanPsi = function(beta, X, muMarg, Z, reg, thetas){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  mu = exp(reg* beta) * muMarg
  sum(reg*(1-Z)*((X-mu)/(1+t(t(mu)/thetas))))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZinbJacobianPsi = function(beta, X, reg, muMarg, Z, thetas){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  mu = exp(reg*beta) * muMarg
  -sum(reg^2*(1-Z)*((1+t(t(X)/thetas))*mu/(1+t(t(mu)/thetas))^2))
}

#--------------------------------------#
dZinbMeanRmat = function(beta, X, reg, muMarg, k, n, Z, nLambda, thetas, rowWeights, rMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  rMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)
  mu = exp(rMat %*% reg) * muMarg

  lambda1 = beta[n+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[n+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(n+3):length(beta)]}

  score = tcrossprod((1-Z)*((X-mu)/(1+t(t(mu)/thetas))),reg) + c(rowWeights*(lambda1 + lambda2* 2*rMat + rMatK %*% lambda3))
  
  center = sum(rMat*rowWeights)
  unitSum = sum(rMat^2*rowWeights)-1
  if(k==1){return(c(score,center, unitSum))}
  orthogons = apply(rMatK, 2, function(x){
    sum(rMat*x*rowWeights)
  })
    return(c(score,centers, unitSums, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZinbJacobianRmat = function(beta, X, reg, muMarg, k, n, Z, nLambda, thetas, rowWeights, rMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  rMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)
  mu = exp(rMat %*% reg) * muMarg

  Jac = matrix(0, nrow = n + nLambda, ncol=n + nLambda)
  #The suymmetric jacobian matrix. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag/dr_{ik}dlambda_{1k}
  Jac[1:n, n+1] = rowWeights
  #dLag/dr_{ik}dlambda_{2k}
  Jac[1:n, n+2] = 2 *rMat*rowWeights
  
  if(k>1){
    Jac[1:n,(n+3):(n+nLambda)] = apply(rMatK, 2, function(x){rowWeights*x})
  }
  
  #Symmetrize
  Jac = Jac + t(Jac)
  tmp= ((1+t(t(X)/thetas))*mu/(1+t(t(mu)/thetas))^2)*(1-Z)
diag(Jac[1:n,1:n]) = -tcrossprod(reg^2 ,tmp) + 2*rowWeights*beta[n+2]
  Jac
}

#--------------------------------------#
dZinbMeanCmat = function(beta, X, reg, muMarg, k, p, Z, nLambda, thetas, colWeights, cMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  cMat = matrix(beta[1:p], byrow=TRUE, ncol=p, nrow=1)
  mu = exp(reg %*% cMat) * muMarg

  lambda1 = beta[p+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[p+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(p+3):length(beta)]}

  score = 
    crossprod(reg,((1-Z)*((X-mu)/(1+t(t(mu)/thetas))))) + 
                        colWeights*(lambda1 + lambda2*2*cMat + lambda3 %*% cMatK)
    
  center = sum(colWeights*cMat)
  unitSum = sum(colWeights*cMat^2)-1
  if(k==1){return(c(score,center, unitSum))}
  orthogons = apply(cMatK,1,function(x){
      sum(cMat*x*colWeights)
  })
    return(c(score,centers, unitSums, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZinbJacobianCmat = function(beta, X, reg, colWeights, k, p, Z, nLambda, thetas, muMarg, cMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  cMat = matrix(beta[1:p], byrow=TRUE, ncol=p, nrow=1)
  mu = exp(reg %*% cMat) * muMarg

  tmp= ((1+t(t(X)/thetas))*mu/(1+t(t(mu)/thetas))^2)*(1-Z)
  Jac = matrix(0, nrow = p + nLambda, ncol = p + nLambda)
  #The symmetric jacobian matrix. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag/dr_{ik}dlambda_{1k}
  Jac[1:p,(p+1)] = colWeights
  #dLag/dr_{ik}dlambda_{2k}
  Jac[1:p,p+2] = colWeights*2 *cMat
  
  #dLag/ds_{ik}dlambda_{3kk'}
  if(k>1){
    Jac[1:p,(p+3):(p+nLambda)] = apply(cMatK, 1,function(x){
      colWeights*x
    })
  }
  #Symmetrize
  Jac = Jac + t(Jac)
  diag(Jac[1:p,1:p]) = -crossprod(tmp, reg^2) + 2*beta[p+2]*colWeights
  Jac
}

#--------------------------------------#
dZinbZeroChi = function(beta, Z, reg, zeroMarg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  GZero = expit(arrayprod(reg, beta) + logit(zeroMarg))
  sum((Z-GZero)*reg)
}

#--------------------------------------#
#A jacobian for the psi parameters
ZinbJacobianChi = function(beta, reg, Z, zeroMarg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  expZero = exp(reg* beta + logit(zeroMarg))
  tmp = (expZero/(1+expZero)^2)
  # tmp[is.infinite(expZero)]=0
  -sum(reg^2*tmp)
}

#--------------------------------------#
dZinbZeroTmat = function(beta, reg,  k, n, Z, zeroMarg, nLambda, rowWeights, tMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  tMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)

  GZero = expit(tMat %*% reg + logit(zeroMarg))
  
  lambda1 = beta[n+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[n+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(n+3):length(beta)]}

  score = tcrossprod((Z-GZero), reg) + 
                         (lambda1+ lambda2*2*tMat + tMatK %*% lambda3)*rowWeights
  
  center = sum(tMat*rowWeights)
  unitSum = sum(tMat^2*rowWeights)-1
  if(k==1){return(c(score,center, unitSum))}
  orthogons = colSums(tMatK*tMat*rowWeights)
  return(c(score,centers, unitSums, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZinbJacobianTmat = function(beta, reg, k, n, Z, zeroMarg, nLambda, rowWeights, tMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  tMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)
  Jac = matrix(0,nrow = n+nLambda, ncol = n+nLambda)

  expZero = exp(tMat %*% reg + logit(zeroMarg))
  
  tmp = (expZero/(1+expZero)^2)
    #dLag/dr_{ik}dlambda_{1k}
  Jac[1:n, n+1] = rowWeights
  #dLag/dr_{ik}dlambda_{2k}
  Jac[1:n, n+2] = 2*tMat*rowWeights
  tmp=expZero/(1+expZero)^2
  #dLag/dr_{ik}dlambda_{3kk'}
  if(k>1){
    Jac[1:n,(n+3):(n+nLambda)] = apply(tMatK, 2, function(x){rowWeights*x})
  }
  
  #Symmetrize
  Jac = Jac + t(Jac)
    diag(Jac[1:n,1:n]) = -tcrossprod(tmp, reg^2) + 2*beta[n+2]*rowWeights
    
  Jac
}

#--------------------------------------#
dZinbZeroVmat = function(beta, reg, colWeights, k, p, Z, zeroMarg, nLambda, vMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  vMat = matrix(beta[1:p], byrow=TRUE, ncol=p, nrow=1)

  GZero = expit(reg %*% vMat + logit(zeroMarg))

  lambda1 = beta[p+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[p+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(p+3):length(beta)]}

  score = 
    crossprod(reg,(Z-GZero)) + 
                        colWeights*(lambda1 + lambda2*2*vMat + (lambda3 %*% vMatK))

  
  center = sum(colWeights*vMat)
  unitSum = sum(colWeights*vMat^2)-1
  if(k==1){return(c(score,center, unitSum))}
  orthogons = colSums(t(vMatK)*vMat*colWeights)
    return(c(score,centers, unitSums, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZinbJacobianVmat = function(beta, reg, colWeights, k, p, Z, zeroMarg, nLambda, vMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  vMat = matrix(beta[1:p], byrow=TRUE, ncol=p, nrow=1)
  expZero = exp(reg %*% vMat +logit(zeroMarg))
  tmp = (expZero/(1+expZero)^2)
  # tmp[is.infinite(expZero)]=0

  Jac = matrix(0, nrow = p + nLambda, ncol = p + nLambda)
  #The symmetric jacobian matrix. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag/dr_{ik}dlambda_{1k}
  Jac[1:p,(p+1)] = colWeights
  #dLag/dr_{ik}dlambda_{2k}
  Jac[1:p,p+2] = colWeights*2 *vMat
  if(k>1){
    Jac[1:p,(p+3):(p+nLambda)] = apply(vMatK, 1,function(x){
      colWeights*x
    })
  }
  
  #Symmetrize
  Jac = Jac + t(Jac)
  diag(Jac[1:p,1:p]) = -crossprod(reg^2,tmp) + colWeights *beta[p+2]*2
    
  Jac
}

RCM_ZINB = function(X, k, rowWeights , colWeights, weightsChar, tol = 1e-3, maxItOut = 500, psiTol = 1e-4, chiTol = psiTol, verbose = TRUE, ZINBRCM = NULL, global = "dbldog", nleqslv.control = list(), method="Broyden", twoCores = FALSE, convNorm = 2, maxItMean = 20, maxItZeroes = 20, dispFreq = 5){
  
  # @param X: a nxp data matrix
  # @param k: a scalar, number of dimensions in the RC(M) model
  # @param tol(optional): a scalar, the relative convergende tolerance for the row scores and column scores parameters, defaults to 1e-3  
  # @param Psitol(optional): a scalar, the relative convergence tolerance for the psi parameters, defaults to 1e-4
  # @param maxItOut(optional): an integer, the maximum number of iteration in the outer loop, defaults to 50
  # @param libSizes(optional) : a vector of length n with (known) library sizes. If not provided, rowSums of x are used
  # @param verbose(optional): a boolean, should information on iterations be printed? Defaults to TRUE
  # @param method(optional): Method for jacobian estimation , see nleqslv. Defaults to Broyden. The difference with the newton method is that the Jacobian is not recalculated at every iteration
  # @param global(optional): global strategy for solving non-linear systems , see nleqslv
  # @param nleqslv.control: a list with control options, see nleqslv
  # @param lambdaRow: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers for the row scores
  # @param lambdaCol: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers for the column scores
  # @param rMatInit(optional): a nxk matrix with initial row scores. If not provided values from the singular value decomposition will be used as starting values
  # @param cMatInit(optional): a pxk matrix with initial column scores. If not provided values from the singular value decomposition will be used as starting values
  # @param psisInit(optional): a vector of length k with inital values for the importance parameters psi. If not provided values from the singular value decomposition will be used as starting values
  # @param dispFreq: a scalar, how many iterations the algorithm should wait before reestimationg the dispersions
  # @param convNorm: a scalar, the norm to use to determine convergence

  # @return A list with elements:
  # @return psis: a vector of length k with estimates for the importance parameters psi
  # @return thetas: a vector of length p with estimates for the overdispersion
  # @return rMat: a nxk matrix with estimated row scores
  # @return cMat: a pxk matrix with estimated column scores
  # @return converged: a boolean indicating if the algorithm converged
  # @return rowRec: a n x k x maxItOut array with a record of all rMat estimates through the iterations 
  # @return colRec: a k x p x maxItOut array with a record of all cMat estimates through the iterations
  # @return psiRec.: a k x maxItOut array with a record of all psi estimates through the iterations

  libSizes = rowSums(X)
  abunds = (colSums(X)/sum(X))
  n=NROW(X)
  p=NCOL(X)
  logLibSizesMLE = log(libSizes)
  logAbundsMLE = log(abunds)
  # logitZeroRows = logit(rep(0.25,n)) #Start with a 25 % chance on a structural zero
  logitZeroCols = logit(rep(0.25,p))
  iterOut = rep(1,k)

  initIter = 1
  
   while((initIter ==1) || ((initIter <= maxItOut) && (!convergenceInit))){
  cat("Starting iteration ",initIter, " \n")
     
     libsOld = logLibSizesMLE
     absOld = logAbundsMLE
     # zeroRowsOld = logitZeroRows
     zeroColsOld = logitZeroCols
     Z = matrix(0,n,p)
     
     #M-step
  initIterMean = 1
  while((initIterMean ==1) || ((initIterMean <= maxItMean) && (!convergenceInitMean))){
    cat("Mean iteration ",initIterMean, "\n")
     libsOldIn = logLibSizesMLE
     absOldIn = logAbundsMLE
 thetas = estDisp(X = X, cMat = matrix(0,1,p), rMat = matrix(0,n,1),  muMarg=exp(outer(logLibSizesMLE, logAbundsMLE, "+")), psis = 0, dispWeights=t(1-Z))
 libsTmp = try(nleqslv(fn = dZinbMeanLibsizes, x = logLibSizesMLE, X = X, reg=logAbundsMLE, global=global, control = nleqslv.control, jac=ZinbJacobianLibsizes, method=method, Z=Z, thetas = thetas)$x, silent=TRUE)
 if(class(libsTmp)!="try-error"){ logLibSizesMLE = libsTmp}
  absTmp = try(nleqslv(fn = dZinbMeanAbunds, x = logAbundsMLE, X = X, reg=logLibSizesMLE, global=global, control = nleqslv.control, jac=ZinbJacobianAbunds, method=method, Z=Z, thetas = thetas)$x, silent=TRUE)
 if(class(absTmp)!="try-error"){logAbundsMLE = absTmp}
  
  initIterMean = initIterMean + 1
  
   convergenceInitMean = ((initIterMean <= maxItMean) && 
                    ((sum(abs(1-logLibSizesMLE/libsOldIn)^convNorm)/n)^(1/convNorm) < tol) && 
                   ((sum(abs(1-logAbundsMLE/absOldIn)^convNorm)/p)^(1/convNorm) < tol))
  }
#     initIterZero = 1
#   while((initIterZero ==1) || ((initIterZero <= maxItZeroes) && (!convergenceInitZeroes))){
#         cat("Zero iteration ",initIterZero, "\n")
#     zeroRowsOldIn = logitZeroRows
#     zeroColsOldIn = logitZeroCols
  
#   zeroRowsTmp = try(nleqslv(fn = dZinbZeroRow, x = logitZeroRows,   reg=logitZeroCols, global=global, control = nleqslv.control, jac=ZinbJacobianZeroRow, method=method, Z=Z)$x, silent=TRUE)
#  if(class(zeroRowsTmp)!="try-error"){ logitZeroRows = zeroRowsTmp}
  zeroColsTmp = try(nleqslv(fn = dZinbZeroCol, x = logitZeroCols, reg = rep.int(0L, n), global=global, control = nleqslv.control, jac=ZinbJacobianZeroCol, method=method, Z=Z)$x, silent=TRUE)
 if(class(zeroColsTmp)!="try-error"){ logitZeroCols = zeroColsTmp}
  
#     initIterZero = initIterZero + 1
#   
#    convergenceInitZeroes = ((initIterZero <= maxItZeroes) && 
#                    # ((sum(abs(1-logitZeroRows/zeroRowsOldIn)^convNorm)/p)^(1/convNorm) < tol) && 
#                    ((sum(abs(1-logitZeroCols/zeroColsOldIn)^convNorm)/p)^(1/convNorm) < tol))
#   }
  
  #E-step
  expMu = exp(outer(logLibSizesMLE, logAbundsMLE, "+"))
  pZero = expit(outer(logitZeroRows, logitZeroCols, "+"))
  thetaMat=matrix(thetas, ncol=ncol(X), nrow=nrow(X), byrow=TRUE)
  d0=dnbinom(0,mu=expMu, size=thetaMat)
  Z[X==0] = (pZero/((1-pZero)*d0 + pZero))[X==0]
 
  initIter = initIter + 1
  convergenceInit = ((initIter <= maxItOut) && 
                    ((sum(abs(1-logLibSizesMLE/libsOld)^convNorm)/n)^(1/convNorm) < tol) && 
                   ((sum(abs(1-logAbundsMLE/absOld)^convNorm)/p)^(1/convNorm) < tol) && 
                   # ((sum(abs(1-logitZeroRows/zeroRowsOld)^convNorm)/p)^(1/convNorm) < tol) && 
                   ((sum(abs(1-logitZeroCols/zeroColsOld)^convNorm)/p)^(1/convNorm) < tol))
     }
    muMarg = exp(outer(logLibSizesMLE, logAbundsMLE, "+")) 
    zeroMarg = expit(outer(logitZeroRows, logitZeroCols, "+"))#The marginals to be used as expectation. These are augmented with the previously estimated dimensions every time

  rowRec = rowRecZeroes = array(0,dim=c(n,k, maxItOut))
  colRec = colRecZeroes = thetaRec = array(0,dim=c(k,p, maxItOut))
  psiRec = matrix(0, nrow=k,ncol=maxItOut)
  convergence = rep(FALSE, k)
  iterOut = rep(1,k)
  
  #If previous fit provided with higher or equal dimension, stop here
  if((!is.null(ZINBRCM)) ){
    if(ZINBRCM$fit != "RCM_ZINB"){
      stop("Fit provided is not of same type as the one requested! \n")
    } else if((k <= ZINBRCM$k)) {
      # stop("Fit provided is already of the required dimension or higher! \n")
    } else{
          for(i in c("rMat","cMat","psis","lambdaCol","lambdaRow", "lambdaRowZero","lambdaColZero","tMat","vMat","chis","Z","zeroMarg","thetas")){
      assign(i, ZINBRCM[[i]])
    }
    }
  #Otherwise try to use intelligent starting values
  } else{
      #Use a more heavily weighted 
   svdX = svd(diag(1/libSizes) %*% (X-muMarg)*(1-zeroMarg) %*% diag(1/colSums(X)))

  rMat = svdX$u[,1:k,drop=FALSE]
  cMat = t(svdX$v[,1:k,drop=FALSE])
  psis = svdX$d[1:k]
#  
#   #Redistribute some weight to fit the constraints 
#   psis = c(psis *t(apply(cMat, 1, function(colS){
#       sqrt(sum(colWeights * colS^2))
#   })) * apply(rMat, 2, function(rowS){
#       sqrt(sum(rowWeights * rowS^2))
#   }))
# 
# #Normalize  
# cMat = t(apply(cMat, 1, function(colS){
#       colS/sqrt(sum(colWeights * colS^2))
#   }))
# rMat = apply(rMat, 2, function(rowS){
#       rowS/sqrt(sum(rowWeights * rowS^2))
#   })
# 
#   #Initial estimates for zeroes is also based on an svd
# 
  Xzeroes = X==0
  
  svdZero = svd(diag(sqrt(1/libSizes)) %*%(Xzeroes-zeroMarg)%*% diag(1/sqrt(colSums(X))))

  tMat = svdZero$u[,1:k, drop=FALSE]
  vMat = t(svdZero$v[,1:k, drop=FALSE])
  chis = svdZero$d[1:k]
# 
# #Redistribute some weight to fit the constraints 
# chis = c(chis *t(apply(vMat, 1, function(colS){
#       sqrt(sum(colWeights * colS^2))
#   })) * apply(tMat, 2, function(rowS){
#       sqrt(sum(rowWeights * rowS^2))
#   }))
#   
# #Normalize  
# vMat = t(apply(vMat, 1, function(colS){
#       colS/sqrt(sum(colWeights * colS^2))
#   }))
# tMat = apply(tMat, 2, function(rowS){
#       rowS/sqrt(sum(rowWeights * rowS^2))
#   })
    
#     tMat = rMat = matrix(1/n, n, k)
#     vMat = cMat = t(matrix(1/p, p, k))
#     psis = chis = rep(1,k)
  
  lambdaRow = lambdaCol = lambdaColZero=lambdaRowZero = rep.int(0,2*k+k*(k-1)/2)
    }
  
  rowRec = rowRecZero = array(0,dim=c(NROW(X),k, maxItOut))
  colRec = colRecZero = array(0,dim=c(k,NCOL(X), maxItOut))
  psiRec = chiRec = matrix(0,ncol=maxItOut, nrow=k)
  
   if(!is.null(ZINBRCM)){ #If fit provided, replace lower dimension starting values
   Kprev = ZINBRCM$k
   rMat[,1:Kprev] = ZINBRCM$rMat
   rowRec[,1:Kprev,] = ZINBRCM$rowRec
   cMat[1:Kprev,] = ZINBRCM$cMat
   colRec[1:Kprev,,] = ZINBRCM$colRec
   psis[1:Kprev] = ZINBRCM$psis
   psiRec[1:Kprev,] = ZINBRCM$psiRec
   lambdaCol[1:(Kprev*(2+(Kprev-1)/2))] = ZINBRCM$lambdaCol
   lambdaRow[1:(Kprev*(2+(Kprev-1)/2))] = ZINBRCM$lambdaRow
   tMat[,1:Kprev] = ZINBRCM$tMat
   rowRecZero[,1:Kprev,] = ZINBRCM$rowRecZero
   vMat[1:Kprev,] = ZINBRCM$vMat
   colRecZero[1:Kprev,,] = ZINBRCM$colRecZero
   chis[1:Kprev] = ZINBRCM$chis
   chiRec[1:Kprev,] = ZINBRCM$chiRec
   lambdaColZero[1:(Kprev*(2+(Kprev-1)/2))] = ZINBRCM$lambdaColZero
   lambdaRowZero[1:(Kprev*(2+(Kprev-1)/2))] = ZINBRCM$lambdaRowZero
   convergence[1:Kprev] = ZINBRCM$converged
   iterOut[1:Kprev] = ZINBRCM$iter
   zeroMarg = ZINBRCM$zeroMarg
 }

minK = ifelse(is.null(ZINBRCM),1,Kprev+1)
  for (KK in minK:k){
  
  cat("Dimension" ,KK, "is being esimated \n")
    
  #Modify offsets if needed
  if(KK>1){
    muMarg = muMarg * exp(rMat[,(KK-1), drop=FALSE] %*% (cMat[(KK-1),, drop=FALSE]*psis[(KK-1)]))
    zeroMarg = expit(logit(zeroMarg) + tMat[,(KK-1), drop=FALSE] %*% (vMat[(KK-1),, drop=FALSE]*chis[(KK-1)]))
  }
    #A lambda parameter
    nLambda = KK + 1
    
    #The location of the lambda parameters
    idK = seq_k(KK)
    Z = matrix(0, n, p)

  ## 2) Propagation
  
  while((iterOut[KK] ==1) || ((iterOut[KK] <= maxItOut) && (!convergence[KK])))
    {
    
  if(verbose && iterOut[KK]%%1 == 0){
  cat("\n","Outer Iteration", iterOut[KK], "\n","\n")
    if(iterOut[KK]!=1){
  cat("Old psi-estimate: ", psiOld, "\n")
  cat("New psi-estimate: ", psis[KK], "\n")
    }
  }
  ## 2)a. Store old parameters
  psiOld = psis[KK]
  rMatOld = rMat[,KK]
  cMatOld = cMat[KK,]
  
  chiOld = chis[KK]
  tMatOld = tMat[,KK]
  vMatOld = vMat[KK,]

  #Expectation
  Z = EstepNB (X = X, rMat = rMat[,KK, drop=FALSE], cMat = cMat[KK,, drop=FALSE], tMat= tMat[,KK, drop=FALSE], vMat = vMat[KK,, drop=FALSE], muMarg = muMarg, zeroMarg = zeroMarg, psis = psis[KK], chis = chis[KK], thetas =thetas)

  #Maximization
  Mlist = MstepNB(Z = Z, X = X, rMat = rMat[,KK, drop=FALSE], cMat = cMat[KK,, drop=FALSE], tMat = tMat[,KK, drop=FALSE], vMat = vMat[KK,, drop=FALSE], k = KK, n=n, p=p,muMarg = muMarg, zeroMarg = zeroMarg, psis = psis [KK],chis = chis[KK], twoCores=twoCores, tol = tol, psiTol = psiTol, chiTol = chiTol, convNorm = convNorm, global=global, nLambda = nLambda, nleqslv.control = nleqslv.control, lambdaCol = lambdaCol[idK], lambdaRow=lambdaRow[idK], lambdaColZero = lambdaColZero[idK], lambdaRowZero = lambdaRowZero[idK], maxItMean = maxItMean, maxItZeroes = maxItZeroes, dispFreq=dispFreq, thetas = thetas, rowWeights = rowWeights, colWeights = colWeights, rMatK = rMat[,1:(KK-1), drop=FALSE], cMatK = cMat[1:(KK-1),, drop=FALSE], tMatK = tMat[,1:(KK-1), drop=FALSE], vMatK = vMat[1:(KK-1),, drop=FALSE] )

  #Assign outcomes to tracking vectors and to this environment
  lambdaCol[idK] = Mlist$lambdaCol
  lambdaRow[idK] = Mlist$lambdaRow
  lambdaColZero[idK] = Mlist$lambdaColZero
  lambdaRowZero[idK] = Mlist$lambdaRowZero

  rowRec[,KK, iterOut[KK]] = rMat[,KK] = Mlist$rMat
  colRec[KK,, iterOut[KK]] = cMat[KK,] = Mlist$cMat
  rowRecZero[,KK, iterOut[KK]] = tMat[,KK] = Mlist$tMat
  colRecZero[KK,, iterOut[KK]] = vMat[KK,] = Mlist$vMat
  psiRec[KK, iterOut[KK]] = psis[KK] = Mlist$psis
  chiRec[KK, iterOut[KK]] = chis[KK] = Mlist$chis
  
  ## 2)f. Change iterator
    iterOut[KK] = iterOut[KK] + 1
    
  ##Check convergence  (any numbered norm for row and column scores)
  convergence[KK] = (iterOut[KK] <= maxItOut) && 
                    (all(abs(1-psis/psiOld) < psiTol)) &&
                    ((sum((1-rMatOld/rMat)^convNorm)/n)^(1/convNorm) < tol) && 
                    ((sum((1-cMatOld/cMat)^convNorm)/p)^(1/convNorm) < tol)  && 
                    (all(abs(1-chis/chiOld) < chiTol)) &&  
                    (sum(abs(1-tMat/tMatOld)^convNorm)/n)^(1/convNorm) < tol &&
                    (sum(abs(1-vMat/vMatOld)^convNorm)/p)^(1/convNorm) < tol 
} # END while-loop
  } # End for-loop
  
  ## 3) Termination
  rownames(rMat) = rownames(tMat) = rownames(X)
  colnames(cMat) = colnames(vMat) = colnames(X)
  rownames(cMat) = rownames(vMat) = colnames(tMat) = colnames(rMat) = paste0("Dim",1:k)
  
  if(!convergence ){
    warning("Algorithm did not converge! Check for errors or consider changing tolerances or number of iterations")
  }
    return(list(converged = convergence, rMat=rMat, cMat=cMat, psis = psis, X=X,
                 rowRec = rowRec, colRec = colRec, psiRec = psiRec, lambdaRow = lambdaRow, lambdaCol = lambdaCol, lambdaRowZero = lambdaRowZero, lambdaColZero = lambdaColZero, chis = chis, tMat = tMat, vMat = vMat, chiRec = chiRec, rowRecZero = rowRecZero, colRecZero = colRecZero, iter=iterOut-1, Z=Z, thetas=thetas, libSizesMLE = exp(logLibSizesMLE), abundsMLE = exp(logAbundsMLE), taxaZeroes = expit(logitZeroCols), rowWeights=rowWeights, colWeights=colWeights, iter=iterOut-1, fit="RCM_ZINB"))
}
```

## A plotting function

```{r plot, purl=TRUE}
plotRCM = function(RCM, X = NULL, thetas = NULL, abunds = NULL, arrowFrac = 0.04, biplot = TRUE,
                   libLoc ="topright", libInset = c(-0.7,-0.1), libCex = 0.85, libInterSp = 0.75, libLeg = TRUE, samColour=NULL,
                   dispInset = c(0,-0.4), abInset = c(0,-0.4), abundLeg=FALSE, stressSpecies=NULL, taxColour = NULL, taxLegPos = "bottomright", taxCol = NULL, taxInset = libInset, taxIntersp = libInterSp,
                   asp=1, xpd=TRUE, mar=c(4,5,5,5), Dim=c(1,2), ...){
  
  # @param psis: vector of length k with psi estimates
  # @param rMat: a nxk matrix with final row scores
  # @param cMat: a pxk with matrix with final column scores
  # @param Dim (optional): a vector of length 2, indicating which dimensions to fit, defaults to the first two
  # @param X (optional): the nxp data matrix
  # @param thetas (optional): a vector of length p with estimates for the overdispersion
  # @param abunds(optional): a vector of length p with abundance estimates (otherwise it is estimated from the data)
  # @param arrowFrac(optional): Fraction of largest species to plot. defaults to 0.1
  # @param biplot(optional): A boolean indicating if species should be added to the plot, defaults to TRUE
  # @param libLoc(optional): a string, location of the library size legend. Defaults to "topleft"
  # @param libLegend(optional): a boolean, should library size legend be displayed? defaults to TRUE
  # @param libInset, dispInset, abInset(optional): numeric vectors of length 2, insets for library size, dispersion and abundance legends
  # param stressSpecies(optional): names of species to be highlighted
  # @param ... additional arguments, passed on to the plot() function
  
  cMat = RCM$cMat
  rMat = RCM$rMat
  psis = RCM$psis
  
  # @return: NONE,  plots the result in the plotting window
  # tmp = par(no.readonly = TRUE)
  par(mar=mar, pty="s")
  if(!(length(psis)== NCOL(rMat) && length(psis) == NROW(cMat))){
    stop("Dimensions don't match")
  }
  #Reorder dimensions according to importance

  a = Dim[1]
  b = Dim[2]
  
  if(!is.null(RCM$physeq) && is.character(samColour)){
    tmp = get_variable(RCM$physeq, samColour)
    samColour = if(is.factor(tmp)) tmp else factor(tmp)
  }
  
  plot(x = rMat[,a] * psis[a],
  y = rMat[,b] *psis[b],
  xlab=paste0("Dim",a),
  ylab=paste0("Dim",b),
  xlim = max(abs(rMat[,a] * psis[a]))*c(-1,1),
  ylim = max(abs(rMat[,b] * psis[b]))*c(-1,1),
  col = samColour,
  asp=asp,
  ...)
  if(libLeg){
legend(pch = 1, col = seq_along(levels(samColour)), legend = levels(samColour), x=libLoc, inset=libInset, xpd=xpd, cex = libCex, y.intersp = libInterSp, x.intersp = 0.7)
}
# {legend(libLoc,legend=c("Small library size","Large library size"),
#       pch=c(1,1), col=c("blue","red"), inset=libInset, xpd=xpd)
#   }
  
    if (biplot){
      #Arrows
        arrowLengths = apply(cMat[Dim,],2,function(x){sqrt(sum(x^2))})
    id = arrowLengths >= quantile(arrowLengths,1-arrowFrac)
        scalingFactor = max(abs(apply(t(t(rMat[,Dim])*psis[Dim]),2, range)))/
        max(abs(cMat[Dim,id]))*0.99
    arrows(x0=0,y0=0,x1=cMat[a,id]*scalingFactor,y1=cMat[b, id]*scalingFactor, col=taxColour[id]
           )
    # legend(x = taxLegPos, legend = levels(taxColour),pch = 1, col = seq_along(levels(taxColour)) )
    if( abundLeg){
      legend(x = taxLegPos, legend = levels(taxColour), xpd=xpd, cex = libCex,lty = 1, col = taxCol, inset = taxInset, y.intersp = taxIntersp)
    }
    #if(!is.null(stressSpecies))
    }
return(invisible())
  # par(tmp)
}
# A ggplot2-version
PCbiplot = function(RCMfit, Dim=c(1,2), ..., colour = NULL, colLegend = colour, shapeLegend = NULL, size=3, arrowFrac = 0.025, scalingFactor=NULL, Palette=NULL, arrowCol="black", plotArrows =TRUE) {
    #Retrieve dots (will be passed on to aes())
    dotList=list(...)
    
    #Construct datafame for samples
    dataSam <- data.frame(RCMfit$rMat[, Dim] %*% diag(RCMfit$psis[Dim]))
    names(dataSam)=paste0("Dim", Dim)
    #Get the colours
    if(!is.null(colour) && !is.factor(colour)){
      colourPlot = get_variable(RCMfit$physeq, colour)
    } else if(is.factor(colour)){
      colourPlot = colour} else {dataSam$colourPlot=1}
    
    #     #Set colour palette
    if(is.null(Palette)){
Palette=rainbow(length(unique(colourPlot)))
    }  

    #Construct dataframe for taxa
    dataTax = data.frame(t(RCMfit$cMat[Dim,]))
    arrowLengths = apply(RCMfit$cMat[Dim,],2,function(x){sqrt(sum(x^2))})
    id = arrowLengths >= quantile(arrowLengths,1-arrowFrac)
    if(is.null(scalingFactor)){
    scalingFactor = min(abs(apply(dataSam[, paste0("Dim", Dim)],2, range)))/max(abs(RCMfit$cMat[Dim,id]))*0.99
    }
    #Filter out small arrows
    dataTax = dataTax[id,] * scalingFactor
    names(dataTax)=paste0("Dim", Dim)

    plot = ggplot(dataSam, aes(x=Dim1, y=Dim2, col = colourPlot, ...)) + 
      geom_point(size=size) +
      if(is.null(colour)) {guides(color=FALSE)}
    
    #add legend names
    if(!is.null(colLegend) & is.factor(colourPlot) ){
      plot = plot + scale_colour_manual(name=colLegend, values =Palette)
    }    else if(!is.null(colLegend) & !is.factor(colourPlot) ){
      plot = plot + scale_colour_continuous(name=colLegend)
    } 
    if(!is.null(shapeLegend)){
      plot = plot + scale_shape_discrete(name=shapeLegend)
    }
  
    #Add arrows
    if(length(arrowCol)>1){
     arrowCol = Palette[c(arrowCol[id])] 
    }
    if(plotArrows){
    plot <- plot + geom_segment(data=dataTax, aes(x=0, y=0, xend=Dim1, yend=Dim2, shape=NULL), arrow=arrow(length=unit(0.2,"cm")), alpha=0.75, color=arrowCol, show.legend=FALSE)}
    plot
}
```

## Constrained RC(M)

```{r LRnb, purl=TRUE} 
# DON'T USE "p" as variable name, partial matching in the grad-function in the numDeriv package!!!

# A function that returns the value of the log-likelihood ratio of alpha, keeping the response functions fixed
LR_nb <- function(Alpha, X, CC, responseFun = c("linear","quadratic","nonparametric"), muMarg,   cReg, nleqslv.control = list(trace=FALSE), NB_params, NB_params_noLab, ...){#
  # @param Alpha: a vector of length d, the environmental gradient
  # @param X: the n-by-p count matrix
  # @param CC: the n-by-d covariate matrix
  # @param responseFun: a character string indicating the type of response function
  # @param muMarg: an n-by-p offset matrix
  # @param thetas: a vector of length p with estimated dispersion parameters (they will not be re-estimated to keep computation time down)
  # @param ncols: a scalar, the number of columns of X
  # @param cReg: a vector of length p, column scores * importance parameter
  # @param nleqslv.control: the control list for the nleqslv() function
  # @param n: number of samples
  # @param NB_params_start: Starting values for the NB_params
  # @param NB_params_start_noLab: Starting values for the NB_params without label
  # @param v: an integer, number of parameters to be estimated

  # @return: a scalar, the evaluation of the log-likelihood ratio at the given alpha
  
	# alpha<-alpha/sqrt(sum(alpha^2))#
    sampleScore = CC %*% Alpha #A linear combination of the environmental variables yields the sampleScore (the rMat vector)
    #cat(sampleScore, "\n \n")
    design = switch(responseFun,
                     linear = model.matrix(~sampleScore),
                     quadratic = model.matrix(~sampleScore + I(sampleScore^2))) #With intercept

   # A matrix based approach (leads to singular jacobians)
   # NB_params_mat = matrix(nleqslv(matrix(1,v,p) , reg = design,  fn = dNBllcol_constr_mat, theta = thetas[1:p], muMarg = muMarg, cReg = cReg, aX = X, control = nleqslv.control, jac = JacCol_constr_mat)$x,v,p)

    mu = muMarg * exp(t(t(design %*% NB_params)* cReg))
    maxX =max(X)
    mu[mu > maxX] = maxX
    mu[mu==0] = 1e-16
   
   muNoLab = muMarg * exp(outer(colSums(t(design) * NB_params_noLab), cReg))
    muNoLab[muNoLab > maxX] = maxX
    muNoLab[muNoLab==0] = 1e-16
 
    lr <- sum(X*log(mu))-sum(X*log(muNoLab)) # The likelihood ratio
    return(-lr) # opposite sign for the minimization procedure
}
#--------------------------------------#
# A function that returns the value of the partial derivative of the log-likelihood ratio to alpha, keeping the response functions fixed
dLR_nb <- function(Alpha, X, CC, responseFun = c("linear", "quadratic", "nonparametric"), cMat, psi, NB_params, NB_params_noLab, d, alphaK, k, centMat, nLambda, nLambda1s,  ...){
  # @param Alpha: a vector of length d + k*(2+(k-1)/2), the environmental gradient plus the lagrangian multipliers
  # @param X: the n-by-p count matrix
  # @param CC: a n-by-d covariate vector
  # @param responseFun: a character string indicating the type of response function
  # @param NB_params: Starting values for the NB_params
  # @param NB_params_noLab: Starting values for the NB_params without label
  # @param d: an integer, the number of covariate parameters
  # @param k: an integer, the current dimension
  # @param centMat: a nLambda1s-by-d centering matrix

  # @return: The value of the lagrangian and the constraining equations
  
  #Extract the parameters
  alpha = Alpha[seq_len(d)]
  lambda1s = Alpha[d+seq_len(nLambda1s)] #Multiple centering requirements now!
  lambda2 = Alpha[d+nLambda1s+1]
  lambda3 = if(k==1) {0} else {Alpha[(d+nLambda1s+2):(d+nLambda)]}
  
  lag = switch(responseFun, #The lagrangian depends on the shape of the response function
               "linear" = c(psi*(crossprod(X %*% (cMat* (NB_params[2,]-NB_params_noLab[2])), CC))) + 
                 colSums(lambda1s * centMat) +
                 lambda2 * 2 *alpha + 
                 alphaK %*% lambda3,
               "quadratic" = stop("Quadratic RF not implemented yet! \n"),
               stop("Unknown response function provided! \n"))
  centerFactors = centMat %*% alpha #Includes overal centering
  size = sum(alpha^2)-1
  if(k==1) { return(c(lag, centerFactors, size))}
  ortho = tcrossprod(alphaK ,alpha)
  c(lag, centerFactors, size, ortho)
}
#--------------------------------------#
# A function that returns the Jacobian of the likelihood ratio
LR_nb_Jac <- function(Alpha, X, CC, responseFun = c("linear", "quadratic", "nonparametric"), cMat, psi, NB_params, NB_params_noLab, d, alphaK, k, nLambda, nLambda1s, ...){
  # @param Alpha: a vector of length d + k*(2+(k-1)/2), the environmental gradient plus the lagrangian multipliers
  # @param X: the n-by-p count matrix
  # @param CC: a n-by-d covariate vector
  # @param responseFun: a character string indicating the type of response function
  # @param NB_params_start: Starting values for the NB_params
  # @param NB_params_start_noLab: Starting values for the NB_params without label
  # @param d: an integer, the number of covariate parameters
  # @param k: an integer, the current dimension

  # @return: The jacobian, a d-by-d symmetric matrix
  #Extract the parameters
  alpha = Alpha[seq_len(d)]
  lambda1s = Alpha[d+nLambda1s] #Multiple centering requirements now!
  lambda2 = Alpha[d+nLambda1s+1]
  lambda3 = if(k==1) {0} else {Alpha[(d+nLambda1s+2):(d+nLambda)]}
  
  Jac = matrix(0, nrow= d + nLambda, ncol = d + nLambda)
  #dLag/dalpha_{yk}dlambda_{1k}
  Jac[1:d,(d+1)] = colWeights
  #Jac[1:(p*k),(p*k+1):((p+1)*k)] = sapply(1:k, function(K){c(rep(0,(K-1)*p),colWeights,rep(0,(k-K)*p))})
  Jac[1:d,d+2] = colWeights*2 *cMat

  tmp = (1+X/thetas)*mu/(1+mu/thetas)^2

    #dLag/ds_{ik}dlambda_{3kk'}
  if(k>1){
    Jac[1:p,(p+3):(p+nLambda)] = t(cMatK)*colWeights
  }

  #Symmetrize
   Jac = Jac + t(Jac)
  #dLag/dr_{ik}
    diag(Jac[1:p,1:p]) = if(constrained) {
          -colSums(tmp * reg^2) + 2*beta[p+2]*colWeights
    } else {
    -crossprod(tmp, reg^2) + 2*beta[p+2]*colWeights
    }
  Jac
  
}

#--------------------------------------#
# A function to estimate the taxon-wise NB-params
estNBparams = function(design, thetas, muMarg, cReg, X, nleqslv.control, ncols, initParam,v){
  
vapply(seq_len(ncols), FUN.VALUE = vector("numeric",v), function(i){
     nleq = try(nleqslv(initParam[,i] , reg = design,  fn = dNBllcol_constr, theta = thetas[i], muMarg = muMarg[,i], cReg = cReg[i], X = X[,i], control = nleqslv.control, jac = JacCol_constr)$x, silent=TRUE)  # , ... Fit the taxon-by taxon NB with given overdispersion parameters and return predictions
     # if(class(nleq)=="try-error" |anyNA(nleq) | any(is.infinite(nleq))){
     # nleq = try(nleqslv(initParam[,i] , fn = dNBllcol_constr, reg = design, theta = thetas[i], muMarg = muMarg[,i], cReg = cReg[i], X = X[,i], control = nleqslv.control)$x,silent=TRUE) # If fails try with numeric jacobian
     # }
     if(class(nleq)=="try-error" |anyNA(nleq) | any(is.infinite(nleq))){nleq = rep.int(0,v)}
     return(nleq)
})
  
  }

#--------------------------------------#
# A function to estimate the NB-params ignoring the taxon labels
estNBparamsNoLab = function(design, thetas, muMarg, cReg, X, nleqslv.control, initParam, n, v){
     #Without taxon Labels
nleqslv(x = initParam , reg = design,  fn = dNBllcol_constr_noLab, thetas = thetas, muMarg = muMarg, cReg = cReg, X = X, control = nleqslv.control, jac = JacCol_constr_noLab, n=n, v=v)$x
}

#--------------------------------------#
# A function to evaluate the score function for 1 taxon at the time
dNBllcol_constr = function(betas, X, reg, theta, muMarg, cReg) {
  # @param beta: a vector of regression parameters (\boldsymbol{beta}_j)
  # @param X: the nx1 data vector
  # @param reg: a matrix of regressors with the same length as beta (\mathbf{C} \boldsymbol{\alpha})
  # @param theta: The dispersion parameter
  # @param mumarg: offset of length n
  # @param cReg:a scalar psi_k * s_{jk}

  # @return The evaluation of the score functions

  mu = exp(c(reg %*% betas)* cReg) * muMarg
  crossprod((X-mu)/(1+mu/theta) , reg) * cReg

}
#--------------------------------------#
# Try this for all taxa combined
dNBllcol_constr_mat = function(betas, aX, reg, theta, muMarg, cReg) {
  # @param beta: a (deg+1)*p matrix of regression parameters with deg the degree of the response function
  # @param X: the nxp data matrix
  # @param reg: a matrix of regressors with the dimension (deg+1)*n (\mathbf{C} \boldsymbol{\alpha})
  # @param theta: The dispersion parameter
  # @param mumarg: offset matrix of size nxp
  # @param cReg: a vector of lentgh p psi_k * s_{jk}

  # @return The evaluation of the score functions, a matrix of size p*(deg+1)

  mu = exp(t(t(reg %*% betas)* cReg)) * muMarg
  score =  crossprod((aX-mu)/(1+t(t(mu)/theta)) , reg) * cReg
  return(c(t(score))) #Taxon per taxon
}
#--------------------------------------#
# The jacobian of this function
JacCol_constr_mat = function(betas, aX, reg, theta, muMarg, cReg) {
  # @param beta: a (deg+1)*p matrix of regression parameters with deg the degree of the response function
  # @param X: the nxp data matrix
  # @param reg: a vector of regressors with the dimension (deg+1)*n (\mathbf{C} \boldsymbol{\alpha})
  # @param theta: The dispersion parameter
  # @param mumarg: offset matrix of size nxp
  # @param cReg: a vector of lentgh p psi_k * s_{jk}

  # @return The jacobian, a (deg+1)*p x (deg+1)*p matrix, a block correlation matrix
v=ncol(reg);p=ncol(aX)
  mu = exp(t(t(reg %*% betas)* cReg)) * muMarg
  tmp = t(t((1+t(t(X)/theta))*mu/(1+t(t(mu)/theta))^2)*cReg^2)
  tmp2 =  vapply(seq_len(v), FUN.VALUE = tmp, function(x){reg[,x]*tmp})
  tmp3 =  vapply(seq_len(v), FUN.VALUE = betas, function(x){crossprod(reg,tmp2[,,x])})
  Jac = matrix(0, p*v, p*v)
  ind = as.logical(Reduce(f=adiag,lapply(seq_len(p),function(x){matrix(TRUE,v,v)})))
  Jac[ind] = apply(tmp3,2,identity)
  return(-Jac)
}
#--------------------------------------#
# The jacobian of this function
JacCol_constr = function(betas, X, reg, theta, muMarg, cReg) {
  # @param beta: a vector of regression parameters (\boldsymbol{beta}_j)
  # @param X: the nx1 data vector
  # @param reg: a matrix of regressors with the same length as beta (\mathbf{C} \boldsymbol{\alpha})
  # @param theta: The dispersion parameter
  # @param mumarg: offset of length n
  # @param cReg:  a scalar psi_k * s_{jk}

  # @return The jacobian

  mu = exp(c(reg %*% betas)* cReg) * muMarg
  tmp = (1+X/theta)*mu/(1+mu/theta)^2
  -crossprod(tmp*reg,reg) * cReg^2 #Don't forget to square cReg!

}
#--------------------------------------#
# A function to evaluate the score function ignoring taxon labels
dNBllcol_constr_noLab = function(betas, X, reg, thetas, muMarg, cReg, ...) {
  # @param beta: a vector of regression parameters with length v (\boldsymbol{beta}_j)
  # @param X: the nxp data matrix
  # @param reg: a matrix of regressors of dimension nxv (\mathbf{C} \boldsymbol{\alpha})
  # @param thetas: The dispersion parameters (a vector of length p)
  # @param muMarg: offset matrix of dimension nxp
  # @param cReg: a vector of length p with the column scores * importance parameter

  # @return The evaluation of the score functions (a vector length v)

  mu = exp(outer(c(reg %*% betas), cReg)) * muMarg
  colSums(crossprod((X-mu)/(1+(mu/thetas)),reg) * cReg)

}
#--------------------------------------#
# The jacobian of this function
JacCol_constr_noLab = function(betas, X, reg, thetas, muMarg, cReg, n ,v) {
  # @param beta: a vector of regression parameters with length v (\boldsymbol{beta}_j)
  # @param X: the nxp data matrix
  # @param reg: a matrix of regressors of dimension nxv (\mathbf{C} \boldsymbol{\alpha})
  # @param thetas: The dispersion parameters (a vector of length p)
  # @param muMarg: offset matrix of dimension nxp

  # @return The jacobian (a v-by-v matrix)

  mu = exp(outer(c(reg %*% betas), cReg)) * muMarg
  tmp = t(t((1+(X/thetas))*mu/(1+(mu/thetas))^2) * cReg^2) #Don't forget to square cReg!
  -crossprod(reg, vapply(seq_len(v), FUN.VALUE = vector("numeric", n), function(x){rowSums(reg[,x]*tmp)}))
  }

#--------------------------------------#
# A function to estimate alpha based on the LR-criterion
estAlpha = function(Alpha, X, CC, d, k, ncols, centMat, n, v, nLambda1s, nLambda, muMarg, thetas, cReg, cMat, psi, alphaK, maxItAlpha = 50, responseFun = c("linear", "quadratic", "nonparametric"),  control.outer = list(trace=FALSE), control.optim=list(trace=FALSE), nleqslv.control = list(), convNorm = 2, tol=1e-3, NB_params = NULL, NB_params_noLab = NULL, analytic = FALSE){
  
  # @param alpha: a vector of length d, the environmental gradient
  # @param alphaK: a matrix with d columns d, the environmental gradient of the previous dimensions
  # @param ncols: a scalar, the number of columns of X
  # @param k: a scalar, dimension index
  # @param v: an integer, the number of parameters per taxon of the response function
  # @param nLambdas1: an integer, number of centering equations
  # @param nLambdas: an integer, total number of lambda parameters
  # @param X: the n-by-ncols count matrix
  # @param C: the n-by-d covariate matrix
  # @param responseFun: a character string indicating the type of response function
  # @param muMarg: an n-by-ncols offset matrix
  # @param thetas: nxp matrix with with estimated dispersion parameters (they will not be re-estimated within the alpha estimation to keep computation time down)
  # @param cReg: a vector of length p, column scores * importance parameter
  # @param maxItAlpha: an integer, the maximum number of iterations
  
  # @return: a vector of length  with the new alpha estimate
  
  iterA = 1
  
  while ((iterA == 1) || ((iterA <= maxItAlpha) &&                           
             (!convergenceA))) { 
    
    AlphaOld = Alpha
  cat("Environmental gradient iteration ",iterA, "\n")
  #New strategy: estimate the parameters and then optimize alpha, iterate between both.
  sampleScore = CC %*% Alpha #A linear combination of the environmental variables yields the sampleScore (the rMat vector)
  design = switch(responseFun,
                     linear = model.matrix(~sampleScore),
                     quadratic = model.matrix(~sampleScore + I(sampleScore^2))) #With intercept
  
NB_params = estNBparams(design = design, thetas = thetas[1,], muMarg = muMarg, cReg = cReg, X = X, nleqslv.control = nleqslv.control, ncols =ncols, initParam = NB_params, v = v)

NB_params_noLab = estNBparamsNoLab(design = design, thetas = thetas, muMarg = muMarg, cReg = cReg, X = X, nleqslv.control = nleqslv.control, initParam = NB_params_noLab, n = n, v = v)
  
 if(analytic){
  AlphaTmp = nleqslv(x = c(Alpha,lambdasAlpha), fn = dLR_nb, X = X, CC = CC, responseFun = responseFun, cMat = cMat, psi = psi, NB_params = NB_params, NB_params_noLab = NB_params_noLab, alphaK = alphaK, k = k, d = d, centMat = centMat, nLambda = nLambda, nLambda1s = nLambda1sE)$x
  Alpha = AlphaTmp[seq_len(d)]
  lambdasAlpha = AlphaTmp[d+(1:nLambdas)]
} else {
   Alpha = constrOptim.nl(par = Alpha, fn = LR_nb, gr = NULL, heq = heq_nb, heq.jac = heq_nb_jac, alphaK = alphaK, X=X, CC=CC, responseFun = responseFun, muMarg = muMarg, cReg = cReg, d = d, ncols=ncols, control.outer = control.outer, control.optim = control.optim, nleqslv.control = nleqslv.control, k =k, centMat = centMat, NB_params = NB_params, NB_params_noLab = NB_params_noLab, v = v)$par #
}
  
  iterA = iterA + 1
  convergenceA = sum((AlphaOld - Alpha)^convNorm)^(1/convNorm) < tol
  } 
  
  return(list(Alpha = Alpha, NB_params = NB_params, NB_params_noLab = NB_params_noLab, convergence = convergenceA, lambdasAlpha = lambdasAlpha))
}
#--------------------------------------#
# A linear equality constraints function for estAlpha (see ?alabama)
heq_nb = function(Alpha, alphaK, d, k,  centMat, ...){
  centerFactors = centMat %*% Alpha #Includes overal centering
  size = sum(Alpha^2)-1
  if(k==1) { return(c( centerFactors, size))}
  ortho = alphaK %*% Alpha
  c(centerFactors, size, ortho)
}
#--------------------------------------#
# It's jacobian, returns a matrix with d columns and at least 2 rows (numerically checked with nloptr::check.derivatives)
heq_nb_jac = function(Alpha, alphaK, d, k, centMat, ...){
  if(k==1) {return(rbind( centMat, 2*Alpha))
    } else {rbind(centMat, 2*Alpha, alphaK)}
}
#-------------------------------------------#
# A function to obtained the matrix of row scores for the constrained case
getRowMatConstr = function(v, NB_params, alpha, covariates, ...){
  # @param v: an integer, the number of parameters per taxon: the degree of the polynomial + 1
  # @param NB_params: a v-by-p matrix with parameters of the taxon-wise response functions 
  # @param alpha: a d-by-k vector of environmental gradients
  # @param covariates: a n-by-d matrix of covariates

  # @return: a n-by-p offset matrix
  
  envir = covariates %*% alpha
  desMat = model.matrix( switch(as.character(v), 
                               "2"=as.formula(~envir),
                               "3" = as.formula(~envir + I(envir^2)), 
                               stop("Only linear and quadratic response functions supported so far! \n")))
  desMat %*% NB_params
}
```

## Wrapper for all distributions

```{r Wrapper all dists, purl = TRUE}
#A simple wrapper function for phyloseq objects and matrices, for all posible distributions it passes all argument onto the correct function.
#Trim on prevalence and total abundance to avoid instability of the algorithm. We cannot conlcude much anyway on lowly abundant taxa
RCM = function(dat, k, round=FALSE, distribution= "NB", prevCutOff = 0.025, minFraction = 0.1, rowWeights = "uniform", colWeights = "marginal", covariates = NULL, confounders = NULL, prevFit = NULL, ...){
  # @param dat: an nxp count matrix or a phyloseq object with an otu_table slot
  # @param k: an integer, the number of dimensions of the RCM solution
  # @param round (optional): a boolean, whether to round to nearest integer. Defaults to FALSE.
  # @param distribution (optional): a character string, the error distribution of the RC(M) model. Defaults to NB
  # @param prevCutoff (optional): a scalar, the prevalance cutoff for the trimming. Defaults to 2.5e-2
  # @param minFraction (optional): a scalar, each taxon's total abundance should equal at leat the number of samples n times minFraction, otherwise it is trimmed. Defaults to 10%
  # @param rowWeights, colWeights (optional): character strings, the weighting procedures for the normalization of row and column scores. Defaults to "uniform" and "marginal" respectively
  # @param covariates (optional): In case "dat" is a phyloseq object, the names of the sample variables to be used as covariates in the constrained analysis, or "all" to indicate all variables to be used. In case dat is a matrix, a nxf matrix or dataframe of covariates. Character variables will be converted to factors, with a warning. Defaults to NULL, in which case an unconstrained analysis is carried out.
  # @param confounders (optional): In case "dat" is a phyloseq object, the names of the sample variables to be used as confounders to be filtered out. In case dat is a matrix, a nxf matrix or dataframe of confounders Character variables will be converted to factors, with a warning. Defaults to NULL, in which case no filtering occurs.
  # @param prevFit(optional) An object with a previous fit, normally from a lower dimension, that should be extended.
  
  classDat = class(dat) #The class provided
  
  ##The count data##
  if (classDat=="matrix"){
  X = dat
  }else  if(classDat=="phyloseq"){
  X = if (taxa_are_rows(dat)) t(otu_table(dat)@.Data) else otu_table(dat)@.Data
  } else {stop("Please provide a matrix or a phyloseq object! \n")}
  p=ncol(X); n=nrow(X)
  
  if(is.null(colnames(X))){colnames(X)=1:ncol(X)}
  if(is.null(rownames(X))){rownames(X)=1:nrow(X)}
  colNames = colnames(X); rowNames =rownames(X)
  # names(rowWeightsNum) =rowNames; names(colWeightsNum) = colNames
  if(round) {X=round(X, 0) }#Round to integer
  
  #Check X type
  if(!all(sapply(X, function(x){(x%%1)==0}))){stop("Please provide integer count matrix! \n")
    } else{X=matrix(as.integer(X), ncol=ncol(X), nrow=nrow(X))}

  colnames(X)=colNames; rownames(X)=rowNames
  X=X[rowSums(X)>0, ] #Remove empty samples
  X=X[, (colMeans(X==0)<(1-prevCutOff)) & (colSums(X)>(n*minFraction))] #Remove txa with low prevalence and those who do not meet the minFraction requirement
  if (distribution %in% c("ZIP","ZINB")) X = X[rowSums(X==0)>0, colSums(X==0)>0] #For a zero-inflated model, make sure every row and column has zeroes
  
  ##Build confounder matrix if applicable. For the preliminary trimming, we do not include an intercept, but we do include all the levels of the factors using contrasts=FALSE: we want to do the trimming in every subgroup, so no hidden reference levels. For the filtering we just use a model with an intercept and treatment coding, here the interest is only in adjusting the offset##
  if(!is.null(confounders)){
    if(class(confounders) %in% c("vector","matrix")){
      if(NROW(X)!=NROW(confounders)){ #Check dimensions
        stop("Data and confounder matrix do not have the same number of samples! \n")
      }
      if(is.vector(confounders)){
      confounders = as.matrix(confounders) #Convert to matrix if only 1 variable
      }
      if(is.null(colnames(confounders))){ #assign names if needed
        colnames(confounders) = paste("var",seq_len(NCOL(confounders)))
      }
      if(class(confounders[,1]) %in% c("character", "factor")){ #If character or factor, build model matrix
        if(class(confounders[,1])=="character"){
          warning("Converting character vector to factor! \n")
        }
        confModelMatTrim = model.matrix(object = as.formula(paste("~" , paste(colnames(confounders), collapse="+"),"-1")), contrasts.arg = apply(colnames(confounders),2,contrasts, contrasts=FALSE)) #No intercept for preliminary trimming
        confModelMat = model.matrix(object = as.formula(paste("~", paste(colnames(confounders), collapse="+"))), contrasts.arg = apply(colnames(confounders),2,contrasts, contrasts=TRUE)) #With intercept for filtering
      } else {
        confModelMat = confModelMatTrim = confounders
      }
    } else if(class(confounders) == "data.frame"){
      if(NROW(X)!=NROW(confounders)){ #Check dimensions
        stop("Data and confounder matrix do not have the same number of samples! \n")
      }
      confModelMatTrim = model.matrix( #No intercept for preliminary trimming
        object = as.formula(paste("~", paste(names(confounders), collapse="+"),"-1")),
        data = datFrame, 
        contrasts.arg = lapply(confounders[sapply(confounders, is.factor)],
                               contrasts, contrasts=FALSE))
      confModelMat = model.matrix( #With intercept for filtering
        object = as.formula(paste("~", paste(names(confounders), collapse="+"))), 
        data = datFrame, 
        contrasts.arg = lapply(confounders[sapply(confounders, is.factor)],
                               contrasts, contrasts = TRUE))
    } else if(class(confounders)=="character"){
      if(classDat != "phyloseq"){
        stop("Providing confounders through variable names is only allowed if phyloseq object is provided! \n")
      }
      datFrame = data.frame(sample_data(dat)) # The dataframe with the confounders
      if(anyNA(datFrame)){stop("Confounders contin missing values!\n")}
    confModelMatTrim = model.matrix(
        object = formula(paste("~", paste(confounders, collapse="+"),"-1")), 
        data = datFrame, 
        contrasts.arg = lapply(datFrame[,sapply(datFrame, is.factor), drop=FALSE][, confounders,drop=FALSE],
                               contrasts, contrasts=FALSE)) 
    confModelMat = model.matrix(
        object = formula(paste("~", paste(confounders, collapse="+"))), 
        data = datFrame, 
        contrasts.arg = lapply(datFrame[,sapply(datFrame, is.factor), drop=FALSE][, confounders,drop=FALSE],
                               contrasts, contrasts = TRUE)) 
    } else{
      stop("Please provide the confounders either as matrix, dataframe, or character string! \n")
    }
  } else{
    confModelMat = confModelMatTrim = NULL
  }

  ##Build covariate matrix if applicable##
  # In this case we will 1) Include dummy's for every level of the categorical variable, and force them to sum to zero. This is needed fro plotting and required for normalization. 2) Exclude an intercept. The density function f() will provide this already. See introduction.
  if(!is.null(covariates)){
    if(class(covariates) == "data.frame"){
      if(NROW(X)!=NROW(covariates)){ #Check dimensions
        stop("Data and covariate matrix do not have the same number of samples! \n")
      }
    datFrame = covariates
    covariatesNames = names(covariates)
    } else if(class(covariates)=="character"){
      if(classDat != "phyloseq"){
        stop("Providing covariates through variable names is only allowed if phyloseq object is provided! \n")
      }
      if(covariates[[1]]=="all"){covariates = sample_variables(dat)} #Enable the "all" option if phyloseq object is provided
      datFrame = data.frame(sample_data(dat))[,covariates] # The dataframe with the covariates
      covariatesNames = covariates
    } else{
      stop("Please provide the covariates either as matrix, dataframe, or character string! \n")
    }
    
    if(any(sapply(datFrame, is.logical))){
      datFrame[,sapply(datFrame, is.logical)] = sapply(datFrame[,sapply(datFrame, is.logical)], as.factor) #Convert logicals to factors
    warning("Logicals converted to factors! \n", immediate. = TRUE)
    }
    if(any(sapply(datFrame, is.character))){
      warning("Character vectors treated as factors! \n", immediate. = TRUE)
    }
    nFactorLevels = sapply(datFrame, function(x){if(is.factor(x)) nlevels(x) else 1}) #Number of levels per factor
    datFrame[,sapply(datFrame, is.factor) & (nFactorLevels < 2)] = NULL #Drop factors with one level
    if(any(sapply(datFrame, is.factor) & (nFactorLevels < 2))){
        warning("The following variables were not included in the analyses because they are factors with only one level: \n", paste(covariates[sapply(datFrame, is.factor) & (nFactorLevels < 2)], sep = " \n"),immediate. = TRUE)
      }
      if(anyNA(datFrame)){
        NArows = apply(datFrame, 1, anyNA)
        X = X[!NArows,]
        datFrame = datFrame[!NArows,]
       if(!is.null(confModelMat)){
          confModelMat = confModelMat[!NArows,]
          confModelMatTrim = confModelMatTrim[!NArows,]
        }
        warning(paste("Some covariates contain missing values. We removed samples \n", paste(which(NArows), collapse = ", "), "\n prior to analysis." ),immediate. = TRUE)
      }
      
      covModelMat = model.matrix(
        object = formula(paste("~", paste(covariatesNames, collapse="+"),"-1")), 
        data = datFrame, 
        contrasts.arg = lapply(datFrame[sapply(datFrame, is.factor)],
                               contrasts, contrasts=FALSE)) 
    #Already prepare the matrix that defines the equations for centering the coefficients of the dummy variables
    centMat  = t(sapply(seq_along(nFactorLevels), function(i){
      c(rep.int(0, sum(nFactorLevels[seq(0,i-1)])), #Zeroes before
        rep.int(if(nFactorLevels[i]==1) 0 else 1, nFactorLevels[i]), #Ones within the categorical variable
        rep.int(0, sum(nFactorLevels[-seq(1,i)]))) #Zeroes after
    })) 

    centMat = centMat[rowSums(centMat)>0,] #Remove zero rows, corresponding to the non-factors
    centMat = rbind(1, centMat) # Add a row of ones for the overal sum
  } else {
    covModelMat = centMat = NULL
  }
  
tic = proc.time() #Time the calculation
  tmp = switch(distribution, 
               NB=RCM_NB(X, rowWeights = rowWeights, colWeights = colWeights, k = k, 
                         confounders  = list(confounders = confModelMat, confoundersTrim = confModelMatTrim), 
                         covariates = covModelMat, prevCutOff = prevCutOff, minFraction = minFraction, centMat = centMat, NBRCM = prevFit,...),
               ZIP=RCM_ZIP(X, rowWeights = rowWeights, colWeights = colWeights, k = k, 
                            confounders  = list(confounders = confModelMat, confoundersTrim = confModelMatTrim), 
                           covariates = covModelMat, prevCutOff = prevCutOff, minFraction = minFraction, ZIPRCM = prevFit,...),
               ZINB=RCM_ZINB(X,  rowWeights = rowWeights, colWeights = colWeights, k = k, 
                              confounders  = list(confounders = confModelMat, confoundersTrim = confModelMatTrim), 
                             covariates = covModelMat, prevCutOff = prevCutOff, minFraction = minFraction, ZINBRCM = prevFit, ...))
  if(classDat=="phyloseq"){tmp$physeq = dat} 
  within(tmp, {
    runtimeInMins = if(is.null(prevFit)) {0} else {prevFit$runtimeInMins} + (proc.time()-tic)[1]/60 # Sum the runtimes
    k = k #Store number of dimensions
  })
}

#Likelioods-------------------------------------
# A function to calculate the likelihoods of
#-The independence model
#-The saturated model
#-The fitted model
#-All models with dimension k 0<k<K
#Which overdispersions to use is a non-trivial problem.  One option would be to use the estimated dispersions of the full model for all calculations. Another is to estimate the overdispersions of the independence and lower dimensional models separately and use them. The problem is that if we use the edgeR machinery again, we get stable estimates but not MLE's, so that the likelihood of the independence model can sometimes be larger than that of a RC model. We provide three options, specified through the Disp parameter:
# - "MLE" Use the MLE's of the separate models where possible,
# - "edgeR" Use the edgeR robust estimate separately for every model
liks = function(rcm, Disp=c("edgeR","MLE")){
  require(MASS)
  #@param rcm: a list, the output of the outerLoop function
  
  #@return a list with components
    #-indLL: likelihood of the indepence model
    #-LL1,..., LL[K-1]: likelihood of intermediate models
    #-LLK: The likelihood of the fitted model
  #Independence model
  C = colSums(rcm$X)
  R = rowSums(rcm$X)
  onesn =rep.int(1, nrow(rcm$X))
  onesp = rep.int(1, ncol(rcm$X))
  E = diag(R) %*% outer(onesn ,onesp) %*% diag(C)/sum(C)
  
  if(Disp=="MLE"){
    
  #Estimate dispersions for the independence model
  # thetasInd = estDisp(rcm$X, cMat=matrix(0,ncol=ncol(rcm$X)), rMat=matrix(0,nrow=nrow(rcm$X)), libSizes=rowSums(rcm$X), abunds=colSums(rcm$X)/sum(rcm$X), psis=0)
  thetasInd =sapply(1:ncol(rcm$X), function(i){theta.ml(y=rcm$X[,i],mu=E[,i])})
  #The overdispersions of the independence model are larger: This is logical since less variation has been explained in this model
  
    #Estimate the overdispersions for the intermediate models and the Full RC model
    LLintDisp = vapply(1:(ncol(rcm$rMat)),FUN.VALUE=rep(0, ncol(rcm$X)), function(k){
      mu=E * exp(rcm$rMat[, 1:k, drop=FALSE] %*% (rcm$cMat[1:k,,drop=FALSE]*rcm$psis[1:k]))
    sapply(1:ncol(rcm$X), function(i){theta.ml(y=rcm$X[,i],mu=mu[,i])})
  })

  } else if(Disp=="edgeR"){
  #Independence model
  trended.dispersion.ind  <- estimateGLMTrendedDisp(y = t(rcm$X), design = NULL, method = "bin.loess",offset=t(log(E)))
  thetasInd = 1/estimateGLMTagwiseDisp(y = t(rcm$X), design = NULL,  offset=t(log(E)), dispersion = trended.dispersion.ind)
  
  #RCM models
  thetasInt = vapply(1:(ncol(rcm$rMat)),FUN.VALUE=rep(0, ncol(rcm$X)), function(k){
  mu=E * exp(rcm$rMat[, 1:k, drop=FALSE] %*% (rcm$cMat[1:k,,drop=FALSE]*rcm$psis[1:k]))
  trended.dispersion  <- estimateGLMTrendedDisp(y = t(rcm$X), design = NULL, method = "bin.loess",offset=t(log(mu)))
  1/estimateGLMTagwiseDisp(y = t(rcm$X), design = NULL,  offset=t(log(mu)), dispersion = trended.dispersion)
  })
  LLintDisp=cbind(thetasInd, thetasInt)
  }  else{stop("No valid dispersion estimation paradigm provided! Choose either MLE or edgeR")}
    names(LLintDisp) = paste0("dispLL",1:(ncol(rcm$rMat)-1))
#Now we have the overdispersions, estimate the likelihoods
  
  #Estimate the likelihoods
  LLintList = mapply(1:(ncol(rcm$rMat)-1),1:(ncol(LLintDisp)-2), FUN = function(k, ThetasI){
    sum(dnbinom(rcm$X, mu = E * exp(rcm$rMat[, 1:k, drop=FALSE] %*% (rcm$cMat[1:k,,drop=FALSE]*rcm$psis[1:k])), size=LLintDisp[,ThetasI], log=TRUE))
  })
  names(LLintList) = paste0("LL",1:(ncol(rcm$rMat)-1))
  indLL = sum(dnbinom(rcm$X, mu=E, size=thetasInd, log=TRUE))
  
#Full RC model
  LLK = sum(dnbinom(rcm$X, mu=E * exp(rcm$rMat %*% (rcm$cMat*rcm$psis)), size=LLintDisp[, ncol(LLintDisp)], log=TRUE))
    
  c(indLL=indLL, LLintList,  LLK=LLK)
}
```

# Demonstration

## Test the software

Some preliminary test to make sure everything runs without errors

```{r tests}
n=50;p=200
counts = matrix(rpois(n*p, 5), n, p)
covars = data.frame(age = rnorm(n, 35,5), gender = sample(c(TRUE, FALSE), n , replace = TRUE), country = factor(sample(c("France","Belgium","Germany"),n, replace=TRUE)), BMI = rnorm(n , 27,6))
physeq = phyloseq(otu_table(counts, taxa_are_rows = FALSE), sample_data(covars))
confounderVec = get_variable(physeq, "country")

#Unconstrained one dimension matrix
un1Dmat = RCM(counts, k=1)
# Add two dimensions
un3Dmat = RCM(dat = counts, prevFit = un1Dmat, k=3)

#Unconstrained one dimension phyloseq
un1DmatPhy = RCM(physeq, k=1)
# Add two dimensions (even accepts a matrix!)
un3DmatPhy = RCM(dat = counts, prevFit = un1DmatPhy, k=3)

#Unconstrained one dimension matrix with confounder filtering
un1DmatConf = RCM(counts, k=1, confounders = data.frame(confounderVec))
# Add two dimensions
un3DmatConf = RCM(dat = counts, prevFit = un1DmatConf, k=3)

#Check some requirements
str(un3DmatConf$confParams)
colSums(un3Dmat$rMat^2)
crossprod(un3Dmat$rMat)
colSums(t(un3Dmat$cMat) * un3Dmat$abunds)
rowSums(un3Dmat$cMat^2)
sum(un3Dmat$cMat[1,]* un3Dmat$cMat[2,] * un3Dmat$abunds)
# All is well!

#Unconstrained one dimension physeq with confounder filtering
un1DmatConfPhy = RCM(physeq, k=1, confounders = "country")
# Add two dimensions
un3DmatConfPhy = RCM(dat = counts, prevFit = un1DmatConf, k=3)

#Constrained one dimension matrix without confounder filtering
con1Dmat = RCM(physeq, k=1, covariates =  sample_variables(physeq))
# Add two dimensions
con3Dmat = RCM(dat = physeq, prevFit = con1Dmat, k=3, covariates =  sample_variables(physeq))
```

## Toy data

### Negative binomial

We generate some data as before with the NB distribution but differing library sizes, apply our algorithm and plot the results.

#### NB without signal

##### Create the data

```{r NB no signal, purl = FALSE}
#Negative binomial, no signal. Set parameters
NsamplesNBNS = 300
NtaxaNBNS = 1100
thetasNBNS = sample(thetas,NtaxaNBNS)
thetasNBNS = thetasNBNS[1/thetasNBNS<60]
rhosNBNS = rhos[names(thetasNBNS)]
rhosNBNS = rhosNBNS/sum(rhosNBNS)
NtaxaNBNS = length(rhosNBNS)
libSizesNBNS = c(rep(1e4, floor(NsamplesNBNS/2)), rep(1e5, floor(NsamplesNBNS/2)))

#Create means and overdispersion matrices
meanNBNS = outer(libSizesNBNS, rhosNBNS)
thetaMatNBNS =  matrix(thetasNBNS, nrow=NsamplesNBNS, ncol=NtaxaNBNS, byrow=TRUE)

#Define a function to make NB data
makeNBdata = function(meanMat, thetaMat){apply(array(data = c(meanMat, thetaMat), dim=c(nrow(meanMat), ncol(meanMat), 2)), c(1,2), function(x){rnbinom(1,mu=x[1], size=x[2])})}

#Generate the data
dataMatNBNS = makeNBdata(meanNBNS, thetaMatNBNS)
```

##### Fit the RC(M) model

```{r fit RCM, purl = FALSE}
#Set control parameters
nleqslv.control = list(trace=FALSE, maxit = 500, cndtol=.Machine$double.eps)
#Fit the RC(M) model
if(!file.exists("toyDataNS.RData")){

      syntNBNSmargmarg_3Job = mcparallel(RCM(dataMatNBNS, distribution="NB", k=3, nleqslv.control= nleqslv.control, maxItOut=1e3, colWeights = "marginal",rowWeights ="marginal", prevCutOff=0.01))
  syntNBNSmargmarg_3 = mccollect(syntNBNSmargmarg_3Job, FALSE)[[1]]
  
      syntNBNSunifmarg_3Job = mcparallel(RCM(dataMatNBNS, distribution="NB", k=3, nleqslv.control= nleqslv.control, maxItOut=1e3, colWeights = "marginal",rowWeights ="uniform",prevCutOff=0.01))
  syntNBNSunifmarg_3 = mccollect(syntNBNSunifmarg_3Job, FALSE)[[1]]
  
      syntNBNSunifunif_3Job = mcparallel(RCM(dataMatNBNS, distribution="NB", k=3, nleqslv.control= nleqslv.control, maxItOut=1e3, colWeights = "uniform",rowWeights ="uniform", prevCutOff=0.01))
  syntNBNSunifunif_3 = mccollect(syntNBNSunifunif_3Job, FALSE)[[1]]
  
      syntNBNSmargunif_3Job = mcparallel(RCM(dataMatNBNS, distribution="NB", k=3, nleqslv.control= nleqslv.control, maxItOut=1e3, rowWeights = "marginal",colWeights ="uniform", prevCutOff=0.01))
  syntNBNSmargunif_3 = mccollect(syntNBNSmargunif_3Job, FALSE)[[1]]
  
   syntNBNSmargmarg_2JobMLE = mcparallel(RCM(dataMatNBNS, distribution="NB", k=2, nleqslv.control= nleqslv.control, maxItOut=1e3, colWeights = "marginal",rowWeights ="marginal", prevCutOff=0.01, marginEst = "MLE"))
  syntNBNSmargmarg_2MLE = mccollect(syntNBNSmargmarg_2JobMLE, FALSE)[[1]]
  
      syntNBNSunifmarg_2JobMLE = mcparallel(RCM(dataMatNBNS, distribution="NB", k=2, nleqslv.control= nleqslv.control, maxItOut=1e3, colWeights = "marginal",rowWeights ="uniform",prevCutOff=0.01, marginEst = "MLE"))
  syntNBNSunifmarg_2MLE = mccollect(syntNBNSunifmarg_2JobMLE, FALSE)[[1]]
  
      syntNBNSunifunif_2JobMLE = mcparallel(RCM(dataMatNBNS, distribution="NB", k=2, nleqslv.control= nleqslv.control, maxItOut=1e3, colWeights = "uniform",rowWeights ="uniform", prevCutOff=0.01, marginEst = "MLE"))
  syntNBNSunifunif_2MLE = mccollect(syntNBNSunifunif_2JobMLE, FALSE)[[1]]
  
      syntNBNSmargunif_2JobMLE = mcparallel(RCM(dataMatNBNS, distribution="NB", k=2, nleqslv.control= nleqslv.control, maxItOut=1e3, rowWeights = "marginal",colWeights ="uniform", prevCutOff=0.01, marginEst = "MLE"))
  syntNBNSmargunif_2MLE = mccollect(syntNBNSmargunif_2JobMLE, FALSE)[[1]]

  #Save the results
  save( syntNBNSunifmarg_3,syntNBNSmargunif_3,syntNBNSmargmarg_3,syntNBNSunifunif_3, dataMatNBNS,test1, test2, test3, test1unif, test2unif, test3unif, syntNBNSunifunif_2MLE, syntNBNSunifmarg_2MLE, syntNBNSmargunif_2MLE, syntNBNSmargmarg_2MLE ,file="toyDataNS.RData" ) #syntNBNSunif,syntNBNSunif, yntNBNSmarg,syntNBNSmarg,
} else {load("toyDataNS.RData")}
```

##### Plot the results

```{r Plot results no signal, purl=FALSE, include=FALSE}
solListNS = list("unifunif" = syntNBNSunifunif_3, "unifmarg" = syntNBNSunifmarg_3, "margunif" = syntNBNSmargunif_3, "margmarg" = syntNBNSmargmarg_3, "unifunifMLE" = syntNBNSunifunif_2MLE, "unifmargMLE" = syntNBNSunifmarg_2MLE, "margunifMLE" = syntNBNSmargunif_2MLE, "margmargMLE" = syntNBNSmargmarg_2MLE)# "unifmargunifRmarg" = test1,  "margmargunifRmarg"=test2, "margunifunifRmarg" = test3, "unifmargunifRunif" = test1unif, "margmargunifRunif"=test2unif, "margunifunifRunif" = test3unif,
#Runtimes and convergence
sapply(solListNS,function(x){x$converged})
sapply(solListNS,function(x){x$runtime})
```

###### Sample plots

Sample plots with the libsizes in colour. The first abbreviation refers to the weighting scheme for the rows(samples), the second to the weighting scheme for the columns. E.g "margunif" means $w_i = x_{i.}$ and $z_j = 1/p$. The "MLE"" epitheton indicates that the independence model was estimated by MLE. Note that in this design the library sizes are either $10^4$ or $10^5$.

```{r SamplePlotsNoSignal, results='hide', purl=FALSE}
par(mfrow=c(2,4))
#unifmarg
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {
  rMatPsi = rMat %*% diag(psis)
  dfCol = data.frame(Dim1=rMatPsi[,1], Dim2=rMatPsi[,2], col=rowSums(X))
  ggplot(data=dfCol, aes(x=Dim1, y=Dim2, col=col)) +geom_point(size=2) +ggtitle(Y) + scale_colour_continuous(name="Library sizes", low="red",high = "green")
    })
  })
```

There definitely is a trend: larger library sizes, larger dim1 scores. Plot the loadings as functions of the library sizes. 

Dimension 1

```{r loadings vs. library sizes dim1, results='hide', purl=FALSE}
par(mfrow=c(2,4))
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,rMat[,1] *psis[1] ,rowSums(X), ylab="Library sizes",xlab="Dim1", sub = paste0("Cor = ",round(cor(rMat[,1], rowSums(X)),2)) )})})
```

Dimension 2

```{r loadings vs. library sizes dim2, results='hide', purl = FALSE}
par(mfrow=c(2,4))
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,rMat[,2] *psis[2],rowSums(X), ylab="Library sizes",xlab="Dim2", sub = paste0("Cor = ",round(cor(rMat[,2], rowSums(X)),2)) )})})
```

Dimension 3

```{r loadings vs. library sizes dim3, results='hide', purl = FALSE}
par(mfrow=c(2,4))
lapply(names(solListNS), function(Y){try(with(solListNS[[Y]], {plot(main=Y,rMat[,3] *psis[3],rowSums(X), ylab="Library sizes",xlab="Dim3", sub = paste0("Cor = ",round(cor(rMat[,3], rowSums(X)),2)))}))})
```

In all weighing schemes, the row scores of the first dimension increase with library size (not in absolute values but they form a gradient). This is likely a result of a discrepancy between the MLE estimates of the sequencing depths under the NB model and the library size (row sums of our count table $\sum_{j=1}^px_{ij}$, which are also just an _estimate_ of the sequencing depth, which does not account for different variances between species but treats them all equally).  The sum of NB-distributed variables is also approximately NB-distributed, although this is not mathematically guaranteed. The first dimension of the MLE row scores estimates then tries to correct for this aberration so that its effect disappears in the higher dimensions. We plot the estimates of the sequencing depths of the two different estimators

```{r plotSequencingDepthEstimators, purl = FALSE}
par(mfrow=c(1,1), pty="s")
with(solListNS[["unifunifMLE"]], {plot(rowSums(X),libSizesMLE, xlab ="Library sizes", ylab = "MLE sequencing depth",log="xy")})
abline(0,1);abline(h=c(1e4,1e5), col="red");abline(v=c(1e4,1e5), col="red")
MSElibs = mean((rowSums(solListNS[["unifunifMLE"]]$X)-rep(c(1e4,1e5), each=150))^2)
MSEmle = mean((solListNS[["unifunifMLE"]]$libSizesMLE-rep(c(1e4,1e5), each=150))^2)
```

There exist considerable differences between both estimation strategies, the MLE one seems less variable. Still the library sizes lead to the smallest MSE.

###### Taxon plots

Now let's take a look at the taxon plots

```{r NBNS taxon plots, results='hide', purl = FALSE}
par(mfrow=c(2,4))
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,t(cMat), ylab="Dim2",xlab="Dim1")}) })
```

The taxon plots of the fits with uniform weights for the taxa are used ($z_j = 1/p$) suffer from outliers when the abundances are estimated as column sums. For the other weighting schemes it is hard to establish anay effect for now. Contrarily to my expectations it are the weights for the centering rather than the normalization requirement that cause the outliers. With MLE estimation of the abundances, the outliers are gone in all weighting schemes!

The taxon plot of second and third dimension

```{r taxonPlot2and3D, results='hide', purl = FALSE}
par(mfrow=c(2,4))
lapply(names(solListNS), function(Y){try(with(solListNS[[Y]], {plot(main=Y,t(cMat[2:3,]), xlab = "Dim2",ylab="Dim3")}))})
```

No more outliers in the higher dimensions

Are the taxon scores related to the abundances in any way?

```{r taxonScesvsAbunds1D, results='hide', purl = FALSE}
#Abundances
par(mfrow=c(2,4))
sapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,abs(cMat[1,]), colSums(X), log="y", cex=0.5, ylab="Mean  abundances",xlab="abs(Dim1)", sub = paste0("Cor = ",round(cor(abs(cMat[1,]), log(colSums(X))),2)))})})
```

Marginal centering weights causes the column scores to slightly correlate with the abundances

```{r taxonScesvsAbunds23D, results='hide', purl = FALSE}
par(mfrow=c(2,4))
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,abs(cMat[2,]), colSums(X), log="y", cex=0.5, ylab="Mean  abundances",xlab="abs(Dim2)", sub = paste0("Cor = ",round(cor(abs(cMat[2,]), log(colSums(X))),2)))})})
par(mfrow=c(2,4))
lapply(names(solListNS), function(Y){try(with(solListNS[[Y]], {plot(main=Y,cMat[3,], colSums(X), log="y", cex=0.5, ylab="Mean  abundances",xlab="abs(Dim3)", sub = paste0("Cor = ",round(cor(abs(cMat[3,]), log(colSums(X))),2)))}))})
```

No more problems in higher dimensions.

Do the taxon scores relate to the overdispersions?

```{r taxonScesvsthetas, results='hide', purl = FALSE}
par(mfrow=c(2,4))
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y, abs(cMat[1,]), thetas, log="y", cex=0.5, ylab="Overdispersion", xlab="abs(Dim1)", sub = paste0("Cor = ",round(cor(abs(cMat[1,]), log(thetas)),2)))})})
par(mfrow=c(2,4))
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y, cMat[2,], thetas, log="y", cex=0.5, ylab="Overdispersion", xlab="abs(Dim2)", sub = paste0("Cor = ",round(cor(abs(cMat[2,]), log(thetas)),2)))})})
```

Large overdispersions get larger scores in all dimensions. This need not be a problem, and may stand in relationship to abundances. Taxa ill fit by the NB may try to compensate this by a large OD and more extreme scores?

How do the MLE estimates for the mean abundances differ from the column sums?

```{r MLEvsColsums, results='hide', purl = FALSE}
par(mfrow=c(1,1))
with(solListNS[["unifunifMLE"]], {plot(main="MLE abundances vs column sum abundances", colSums(X)/sum(X), abundsMLE, log="xy", cex=0.5, ylab="MLE abundances", xlab="Column sum abundance")})
abline(0,1)
```

There are subtle differences but less than with the library sizes. Why is the MLE less different from the marginal sum in this case? We can think of the following mathematical explanation: when calculating library sizes or abundances by row and column sums, each observations receives the same weight. However, when we estimate the margins through ML, we solve the following score equations:

$$\frac{\partial L(\mathbf{X}_i|\mathbf{u}_{i}, \mathbf{u}_j, \boldsymbol{\theta})}{\partial u_{i}} = \sum_{j=1}^p \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}}$$

$$\frac{\partial L(\mathbf{X}_i|\mathbf{u}_{i}, \mathbf{u}_j, \boldsymbol{\theta})}{\partial u_{j}} = \sum_{i=1}^n \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}}.$$

Note that since we are estimating offsets the value of the regressor is 1. In this case the difference of $y_{ij}$ with the expected value $\mu_{ij}$ is weighted by a factor $\frac{1}{1+\frac{\mu_{ij}}{\theta_j}} = \frac{\theta_j}{\theta_j+\mu_{ij}}$. When estimating $u_j$, $\theta$ is a constant but when estimating $u_i$ it is different for every observation $y_{ij}$. Hence the weights put on every observation differ much more when estimating the sample offsets than when estimating the taxon offsets. That is the reason why the MLE differs more from the marginal sum for the library size than for the abundances.

##### Influence function

Take a look at the influence function values for the influence on the first psi parameters

```{r NS influence measures: psis, results="hide", purl = FALSE}
infl1 = lapply(solListNS, function(x, i){
  infl=with(x, NBpsiInfl(psi = psis[i], X = X, cMat = cMat[i,,drop=FALSE], rMat = rMat[,i, drop=FALSE], muMarg = outer(rowSums(X), colSums(X)/sum(X)), theta = thetas))
  id=abs(infl) > quantile( abs(infl),0.995)
  Xid = x$X[id]
  abunds = colSums(x$X)/sum(x$X)
  libsizes = rowSums(x$X)
  list(infl = infl, id = id, Xid = Xid, abunds=abunds, libsizes=libsizes)
},1)
# par(mfrow=c(3,5))
# lapply(names(infl1), function(x){
#   with(infl1[[x]], plot(abunds, colSums(id), log="x", main=x, ylab= "Number of influential observations"))
# })
par(mfrow=c(2,4))
lapply(names(infl1), function(x){
  with(infl1[[x]], plot(log(abs(c(t(infl))))~rep(cut(log(abunds),8),nrow(infl)), main=x, ylab = "log(|Influence|)",xlab = "log Abundance")) #Very heavy!
})
```

Larger abundances, larger influences in all weighting schemes (remember it is a log(abs())-plot).

Influence vs libSizes

```{r Influence vs libSizes, results="hide", purl = FALSE}
par(mfrow=c(2,4))
lapply(names(infl1), function(x){
  with(infl1[[x]], plot(log(abs(c(infl)))~rep(cut(log(libsizes),8),ncol(infl)), main=x, ylab = "log(|Influence|)",xlab = "log library size")) #Very heavy!
})
```

No clear trend of influence in function of the library sizes

How are the most influential observations as compared to the bulk of the observations?

```{r Influence vs non-influential observations, purl = FALSE}
lapply(names(infl1), function(x){
  with(infl1[[x]], {rbind(expectationsAll = quantile(outer(libsizes, abunds)),
       expectationsInfluentialObservation =quantile(outer(libsizes, abunds)[id])) })
})

# sapply(names(infl1), function(x){
#   rbind(zeroesAll = mean(solListNS[[x]]$X==0),
#        zeroesInfluentialObservation = mean(solListNS[[x]]$X[infl1[[x]]$Xid]==0)) })
```

The most influential observations are zero counts (or low counts) in highly abundant species and high libsizes (i.e. high expected values)!

Plot the influence vs. observed abundances

```{r X vs Infl(X), results="hide", purl = FALSE}
#Expectations
par(mfrow=c(2,4))
lapply(names(infl1), function(x){
  id = sample(1:prod(dim(solListNS[[x]]$X)), size=10000)
plot(log10(solListNS[[x]]$X+1)[id], infl1[[x]]$infl[id], ylab= "Influence", xlab = "log10(Counts +1)", main=x)
abline(v=0, col="red")
})
```

Versus expected abundances

```{r Infl vs observed abundances, results="hide", purl = FALSE}
par(mfrow=c(2,4))
lapply(names(infl1), function(x){
  id = sample(1:prod(dim(solListNS[[x]]$X)), size=10000)
plot(outer(rowSums(solListNS[[x]]$X), colSums(solListNS[[x]]$X)/sum(solListNS[[x]]$X))[id], infl1[[x]]$infl[id], ylab = "Influence", xlab = "Expected counts", main=x)
  abline(v=0, col="red")
})
```

Versus Raw residuals (Observed - Expected count)

```{r vs Pearson residuals, results = "hide", purl = FALSE}
par(mfrow=c(2,4))
lapply(names(infl1), function(x){
   id = sample(1:prod(dim(solListNS[[x]]$X)), size=10000) #Take a susbet for computability
plot((solListNS[[x]]$X-outer(rowSums(solListNS[[x]]$X), colSums(solListNS[[x]]$X)/sum(solListNS[[x]]$X)))[id], infl1[[x]]$infl[id], ylab = "Influence", xlab = "Raw residuals", main=x)#sqrt(diag(1/rowSums(solListNS[[x]]$X)))%*%%*%sqrt(diag(1/colSums(solListNS[[x]]$X)))
  abline(v=0, col="red")
})
```


```{r psi2Infl, eval=FALSE, purl = FALSE}
infl2 = lapply(solListNS, function(x, i){
  infl=with(x, NBpsiInfl(psi = psis[i], X = X, cMat = cMat[i,,drop=FALSE], rMat = rMat[,i, drop=FALSE], muMarg = outer(rowSums(X), colSums(X)/sum(X)), theta = thetas))
  id=abs(infl) > quantile( abs(infl),0.995)
  Xid = x$X[id]
  abunds = colSums(x$X)/sum(x$X)
  libsizes = rowSums(x$X)
  list(infl = infl, id = id, Xid = Xid, abunds=abunds, libsizes=libsizes)
},2)
par(mfrow=c(2,2))
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(abunds, colSums(id), log="x", main=x))
})
#In the second dimension, larger abundance means larger influence on the psis too, and no more outliers
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(rep(abunds, nrow(infl)), c(t(infl)), log="x", main=x)) #Very heavy!
}) #Outlier in unifmarg weigthing scheme
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(rep(libsizes, ncol(infl)), c(infl), log="x", main=x)) #Very heavy!
})
#Expectations
lapply(names(infl2), function(x){
  with(infl2[[x]], {rbind(quantile(outer(libsizes, abunds)),
       quantile(outer(libsizes, abunds)[id])) })
})
lapply(names(infl2), function(x){
  cat(x, "\n")
  with(infl2[[x]], table(Xid))
})
```

Under the uniform weighting scheme for the taxa, one single observation will get an enormous influence in the first dimension, this is in all likelihood the outlier. Note that we cen derive this from the influence function that does not even depend on the weights!

Influence on the colScores

```{r NBNS influence colscores, eval=FALSE, purl = FALSE}
inflCol1 = lapply(solListNS, function(x){
  try(with(x, NBcolInfl(X, psis, cMat, rMat,thetas , colWeights , k=1 , lambdaCol)))
})
#Only look at most extreme colscores
#Cannot calculate influence functions for the interesting cases (with the outliers)

```

```{r NBNS influence rowscores, eval=FALSE, purl = FALSE}
inflRowList1 = lapply(solListNS, function(x){
  try(with(x, NBrowInfl(X, psis, cMat, rMat,thetas , rowWeights , k=1 , lambdaRow)))
})
#Only look at most extreme colscores
#Cannot calculate influence functions for the interesting cases (with the outliers)
inflRowList1=inflRowList1[sapply(inflRowList1, class)=="list"]
#The influence on the first row score
inflRow1 = lapply(inflRowList1, function(x){getInflRow(x$score, x$InvJac, 1)}) 
par(mfrow=c(1,2))
libSizes = rowSums(solListNS[[1]]$X)
lapply(inflRow1, function(x){
  plot(y=x[,1], libSizes, log="x")
})
```

##### Conclusions from the null plots

 - No signal in either plot. 
 - For the uniform taxon weighting the first dimension of the plot is dominated by outliers
 - The first dimension row scores are related to the library sizes in all weighting schemes
 - The least abundant species get higher column scores in all weighting schemes
 
#### NB with signal

The signal is introduced by sampling from different distributions in which each time the abundance of a small set of species was modified.

With modified abundances (4 groups)

```{r NB with signal abundance,eval=FALSE , purl = FALSE}
rhosNBSigref = rhosNBSig1 = rhosNBSig2 = rhosNBSig3 = rhosNBSig4 = rhosNBNS

load("/home/stijn/PhD/American Gut/AGphylo.RData")
libSizesAG = sample_sums(AGphylo)

NsamplesSignal1 = NsamplesSignal2 =  20
NsamplesSignal3 = NsamplesSignal4 =  15

NtaxaSignalNB = 40

Signal1NB = 18
Signal2NB = 12
Signal3NB = 10
Signal4NB = 7

idSig1NB = 1:NtaxaNBNS %in% sample(1:NtaxaNBNS, NtaxaSignalNB) #Random sampling should ensure orthogonality
idSig2NB = 1:NtaxaNBNS %in% sample(1:NtaxaNBNS, NtaxaSignalNB)
idSig3NB = 1:NtaxaNBNS %in% sample(1:NtaxaNBNS, NtaxaSignalNB) #Random sampling should ensure orthogonality
idSig4NB = 1:NtaxaNBNS %in% sample(1:NtaxaNBNS, NtaxaSignalNB)
#Apply the signals

rhosNBSig1[idSig1NB] = rhosNBSig1[idSig1NB]*Signal1NB
rhosNBSig2[idSig2NB] = rhosNBSig2[idSig2NB]*Signal2NB
rhosNBSig3[idSig3NB] = rhosNBSig3[idSig3NB]*Signal3NB
rhosNBSig4[idSig4NB] = rhosNBSig4[idSig4NB]*Signal4NB

#Renormalize
renorm=function(x){x/sum(x)}
rhosNBSig1=renorm(rhosNBSig1);rhosNBSig2=renorm(rhosNBSig2);
rhosNBSig3=renorm(rhosNBSig3);rhosNBSig4=renorm(rhosNBSig4);

#Generate data
Nref = (NsamplesNBNS-NsamplesSignal1-NsamplesSignal2-NsamplesSignal3-NsamplesSignal4)
meanMatRefNB = outer(libSizesAG[sample(size=Nref, 1:NsamplesNBNS)], rhosNBSigref)
meanMatSig1NB = outer(libSizesAG[sample(size=NsamplesSignal1, 1:NsamplesNBNS)], rhosNBSig1)
meanMatSig2NB = outer(libSizesAG[sample(size=NsamplesSignal2, 1:NsamplesNBNS)], rhosNBSig2)
meanMatSig3NB = outer(libSizesAG[sample(size=NsamplesSignal3, 1:NsamplesNBNS)], rhosNBSig3)
meanMatSig4NB = outer(libSizesAG[sample(size=NsamplesSignal4, 1:NsamplesNBNS)], rhosNBSig4)
thetaMatSigNB = thetaMatNBNS

dataMatRefNB = makeNBdata(meanMatRefNB, thetaMatSigNB)
dataMatSig1NB = makeNBdata(meanMatSig1NB, thetaMatSigNB)
dataMatSig2NB = makeNBdata(meanMatSig2NB, thetaMatSigNB)
dataMatSig3NB = makeNBdata(meanMatSig3NB, thetaMatSigNB)
dataMatSig4NB = makeNBdata(meanMatSig4NB, thetaMatSigNB)

dataMatSigNBab = rbind(dataMatRefNB, dataMatSig1NB, dataMatSig2NB, dataMatSig3NB,dataMatSig4NB)

#Save signals
sampleSigNBab = factor(c(rep("Reference", Nref), rep("Signal1", NsamplesSignal1), rep("Signal2", NsamplesSignal2), rep("Signal 3", NsamplesSignal3), rep("Signal 4", NsamplesSignal4)))
taxaSigNBtmp = rep("Reference",ncol(dataMatSigNBab))
taxaSigNBtmp[idSig1NB] = "Signal 1"
taxaSigNBtmp[idSig2NB] = "Signal 2"
taxaSigNBtmp[idSig3NB] = "Signal 3"
taxaSigNBtmp[idSig4NB] = "Signal 4"
taxaSigNBtmp[idSig1NB & idSig2NB] = "Signal 1 and 2"
taxaSigNBtmp[idSig1NB & idSig3NB] = "Signal 1 and 3"
taxaSigNBtmp[idSig1NB & idSig4NB] = "Signal 1 and 4"
taxaSigNBtmp[idSig2NB & idSig3NB] = "Signal 2 and 3"
taxaSigNBtmp[idSig2NB & idSig4NB] = "Signal 2 and 4"
taxaSigNBtmp[idSig3NB & idSig4NB] = "Signal 3 and 4"
taxaSigNBtmp[idSig1NB & idSig2NB & idSig3NB] = "Signal 1 and 2 and 3"
taxaSigNBtmp[idSig4NB & idSig2NB & idSig3NB] = "Signal 2 and 3 and 4"
taxaSigNBtmp[idSig1NB & idSig2NB & idSig4NB] = "Signal 1 and 2 and 4"
taxaSigNBtmp[idSig1NB & idSig4NB & idSig3NB] = "Signal 1 and 3 and 4"
taxaSigNBab = factor(taxaSigNBtmp, levels = c("Reference","Signal 1","Signal 2","Signal 3","Signal 4","Signal 1 and 2","Signal 1 and 3","Signal 1 and 4","Signal 2 and 3","Signal 2 and 4","Signal 3 and 4", "Signal 1 and 2 and 3", "Signal 1 and 2 and 4", "Signal 1 and 3 and 4", "Signal 2 and 3 and 4")) 
names(taxaSigNBab) = colnames(dataMatSigNBab) = names(rhosNBSigref)
names(sampleSigNBab) = rownames(dataMatSigNBab) = 1:NsamplesNBNS

save(dataMatSigNBab, taxaSigNBab, sampleSigNBab, file = "abData.RData")
```

#### The abundance based approach: fit

```{r Fit3Dab, eval=FALSE, purl = FALSE}
nleqslv.control.ab = list(trace=TRUE, maxit = 500, cndtol=.Machine$double.eps)
if(!file.exists("toyDataSigab.RData")){

    syntNBSigmargmarg_abJob = mcparallel(RCM(dataMatSigNBab, distribution="NB", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "marginal"))
  syntNBSigmargmarg_ab = mccollect(syntNBSigmargmarg_abJob, FALSE)[[1]] #Check
  
      syntNBSigunifmarg_abJob = mcparallel(RCM(dataMatSigNBab, distribution="NB", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform"))
  syntNBSigunifmarg_ab = mccollect(syntNBSigunifmarg_abJob, FALSE)[[1]] 
  
      syntNBSigmargunif_abJob = mcparallel(RCM(dataMatSigNBab, distribution="NB", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, rowWeights="marginal", colWeights = "uniform"))
  syntNBSigmargunif_ab = mccollect(syntNBSigmargunif_abJob, FALSE)[[1]] 
  
      syntNBSigunifunif_abJob = mcparallel(RCM(dataMatSigNBab, distribution="NB", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, colWeights="uniform", rowWeights = "uniform"))
  syntNBSigunifunif_ab = mccollect(syntNBSigunifunif_abJob, FALSE)[[1]] #Check
  
        syntNBSigmargmarg_abJobMLE = mcparallel(RCM(dataMatSigNBab, distribution="NB", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "marginal", marginEst = "MLE"))
  syntNBSigmargmarg_abMLE = mccollect(syntNBSigmargmarg_abJobMLE, FALSE)[[1]] 
  
      syntNBSigunifmarg_abJobMLE = mcparallel(RCM(dataMatSigNBab, distribution="NB", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform", marginEst = "MLE"))
  syntNBSigunifmarg_abMLE = mccollect(syntNBSigunifmarg_abJobMLE, FALSE)[[1]] 
  
      syntNBSigmargunif_abJobMLE = mcparallel(RCM(dataMatSigNBab, distribution="NB", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, rowWeights="marginal", colWeights = "uniform", marginEst = "MLE"))
  syntNBSigmargunif_abMLE = mccollect(syntNBSigmargunif_abJobMLE, FALSE)[[1]] 
  
      syntNBSigunifunif_abJobMLE = mcparallel(RCM(dataMatSigNBab, distribution="NB", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, colWeights="uniform", rowWeights = "uniform", marginEst = "MLE"))
  syntNBSigunifunif_abMLE = mccollect(syntNBSigunifunif_abJobMLE, FALSE)[[1]] 
 
  save(dataMatSigNBab, syntNBSigunifmarg_ab, syntNBSigunifunif_ab,syntNBSigmargmarg_ab,syntNBSigmargunif_ab,taxaSigNBab, sampleSigNBab, testAb1, testAb2, testAb3, testAb1unif, testAb2unif, testAb3unif,testAbInv,testAbLibs,dataMatSigNBabag,syntNBSigmargmarg_abMLE,syntNBSigunifunif_abMLE,syntNBSigmargunif_abMLE,syntNBSigunifmarg_abMLE, file="toyDataSigab.RData")#syntNBSigunif, syntNBSigmarg, syntNBSigmarg_3,
} else {load("toyDataSigab.RData")}
```

```{r ab plots, include=FALSE, purl = FALSE}
load("toyDataSigABserver.RData") 
# solListWSab = list("unifunif" = syntNBSigunifunif_ab, "unifmarg" = syntNBSigunifmarg_ab, "margunif" = syntNBSigmargunif_ab, "margmarg" = syntNBSigmargmarg_ab, "unifmargunifRmarg" = testAb1,  "margmargunifRmarg"=testAb2, "margunifunifRmarg" = testAb3, "unifmargunifRunif" = testAb1unif, "margmargunifRunif"=testAb2unif, "margunifunifRunif" = testAb3unif, "inverse"= testAbInv, "testEqualLibs" =   testAbLibs, "unifunifMLE" = syntNBSigunifunif_abMLE, "unifmargMLE" = syntNBSigunifmarg_abMLE, "margunifMLE" = syntNBSigmargunif_abMLE, "margmargMLE" = syntNBSigmargmarg_abMLE )
names(sigListAB) = c("unifunif","unifunifMLE","margunif","margunifMLE","unifmarg","unifmargMLE","margmarg","margmargMLE")
solListWSab = sigListAB

solListWSab = solListWSab[sapply(solListWSab, class)=="list"] 
#Runtimes and convergence
sapply(solListWSab,function(x){x$converged})
sapply(solListWSab,function(x){x$runtime})
sapply(solListWSab,function(x){x$iter})
```

The first abbreviation refers to the weighting scheme for the rows(samples), the second to the weighting scheme for the columns. E.g "margunif" means $w_i = x_{i.}$ and $z_j = 1/p$. The MLE epitheton indicates that the independence model was estimated by MLE.

#### Sample plots

\newpage

```{r abSignalPlots, results='hide', fig.show="hold", fig.width = 12, fig.height = 9, purl = FALSE}
cols = c("grey", "red","blue","purple","green","brown","cyan","orange","black",  "magenta","yellow","pink", "darkblue","grey75","cadetblue") 
palette(cols)
par(mfrow=c(1,1), mar = c(4,4,4,4), mai=c(0,0,0,0))
lapply(names(solListWSab), function(Y){plotRCM(solListWSab[[Y]], samColour = sampleSigNBab, main=Y, biplot=FALSE, Dim = c(1,2), libInset=c(-0.1,0))})#, libLeg = Y ==names(solListWSab)[1]
```

##### 1st and 3rd dimensions

\newpage

```{r abSignalPlots23D, results='hide', fig.show="hold", fig.width = 12, fig.height = 9, purl = FALSE}
palette(cols)
par(mfrow=c(1,1))
lapply(names(solListWSab), function(Y){try(plotRCM(solListWSab[[Y]], samColour = sampleSigNBab, main=Y, biplot=FALSE, Dim = c(1,3), libInset=c(-0.8,0)))})
```

Only when the margins are estimated with MLE are the first two dimensions used to separate the signal well. Especially the uniform weighting for the samples and marginal for the columns (as was our first intuition) appears to perform well. Also uniform weighting for both yields good results.

#### Sample scores vs. libsizes

```{r abSignalPlots 13D, results='hide', fig.show="hold", purl = FALSE}
par(mfrow=c(2,4))
#Maybe a function of the library sizes
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {
  rMatPsi = rMat %*% diag(psis)
  dfCol = data.frame(Dim1=rMatPsi[,1], Dim2=rMatPsi[,2], col=log(rowSums(X)))
  ggplot(data=dfCol, aes(x=Dim1, y=Dim2, col=col)) +geom_point(size=3) +ggtitle(Y) + scale_colour_continuous(name="Library sizes", low="red",high = "green")
    })
  })
```

Look at the scores' relationship to the library sizes more directly.

```{r ab libsizes, results='hide', fig.show="hold", eval=FALSE, purl = FALSE}
#Look at the loadings in function of the library sizes and abunds
par(pty = "m", mfrow = c(1,1), mar = c(5,4,4,4))
logP="x"
#Libsizes
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,rMat[,1] *psis[1],x=rowSums(X), log=logP, xlab ="Dim1",ylab = "Library sizes", sub = paste0("Cor = ",round(cor(rMat[,1] ,rowSums(X)),2) ) )})})
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,rMat[,2] *psis[2],x=rowSums(X), log=logP, xlab ="Dim2", ylab = "Library sizes", sub = paste0("Cor = ",round(cor(rMat[,2] ,rowSums(X)),2) ) )})})
# lapply(names(solListWSab), function(Y){try(with(solListWSab[[Y]], {plot(main=Y,rMat[,3] *psis[3],x=rowSums(X), log=logP, xlab ="Dim3", ylab = "Library sizes", sub = paste0("Cor = ",round(cor(rMat[,1] ,rowSums(X)),2) ) )}))})
```

On these plots it is clear that when the margins are estimated using the library sizes, the scores correlate with the library sizes.

In the MLE framework for the offsets, dependence on the library sizes has disappeared.

Compare MLE and rowSums estimators for the sequencing depth

```{r plotSequencingDepthEstimatorsAb, purl = FALSE}
par(mfrow=c(1,1), pty="s")
with(solListWSab[["unifunifMLE"]], {plot(rowSums(X),libSizesMLE, xlab ="Library sizes", ylab = "MLE sequencing depth",log="xy")})
 abline(0,1);#abline(h=c(1e4,1e5), col="red");abline(v=c(1e4,1e5), col="red")
# MSElibs = mean((rowSums(solListWSab[["unifunifMLE"]]$X)-rep(c(1e4,1e5), each=150))^2)
# MSEmle = mean((solListWSab[["unifunifMLE"]]$libSizesMLE-rep(c(1e4,1e5), each=150))^2)
```

The MLE estimate is generally larger than the rowSums estimate (presumably since the former puts less weight on zero counts (large overdispersion)), this may explain the gradient of the row scores in function of the library sizes.

##### Taxa plots

Now let's take a look at the taxon plots

```{r NBab taxon plots, results='hide', eval=FALSE, purl = FALSE}
par(mfrow=c(1,1), pty="s")
palette(cols)
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,t(cMat), ylab="Dim2",xlab="Dim1", col = taxaSigNBab)}); legend("topright",legend = levels(taxaSigNBab), col=cols,pch= 1, cex=0.7, inset = c(-0.80,0), xpd=TRUE) })
```

For the uniform weighting of the samples we see outliers for the taxa

Are the taxon scores related to the abundances in any way?

```{r taxonScesvsAbunds1Dab, results='hide', eval=FALSE, purl = FALSE}
#Abundances
par(mfrow=c(1,1), mar=c(5,4,4,4))
sapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,cMat[1,], colSums(X), log="y", cex=0.5, ylab="Mean  abundances",xlab="Dim1", sub = paste0("Cor = ", round(cor(cMat[1,], colSums(X)),2)))})})
```

The MLE offset estimation does reduce correlation with the mean abundances.

```{r taxonScesvsAbunds23Dab, results='hide', eval=FALSE, purl = FALSE}
# Second and third dimensions
par(mfrow=c(1,1), mar=c(5,4,4,4))
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,cMat[2,], colSums(X), log="y", cex=0.5, ylab="Mean  abundances",xlab="Dim2", sub = paste0("Cor = ", round(cor(cMat[2,], colSums(X)),2)))})})
par(mfrow=c(1,1), mar=c(5,4,4,4))
lapply(names(solListWSab), function(Y){try(with(solListWSab[[Y]], {plot(main=Y,cMat[3,], colSums(X), log="y", cex=0.5, ylab="Mean  abundances",xlab="Dim3", sub = paste0("Cor = ", round(cor(cMat[3,], colSums(X)),2)))}))})
```

No more problems with correlation in higher dimensions.

Do the taxon scores relate to the overdispersions?

```{r taxonScesvsthetas1, results='hide', eval=FALSE, purl = FALSE}
par(mfrow=c(1,1))
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,abs(cMat[1,]), thetas, log="y", cex=0.5, ylab="Overdispersion", xlab="Dim1", sub = paste0("Cor = ", round(cor(abs(cMat[1,]), thetas),2)))})})
par(mfrow=c(1,1))
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,cMat[2,], thetas, log="y", cex=0.5, ylab="Overdispersion", xlab="Dim2", sub = paste0("Cor = ", round(cor(abs(cMat[2,]), thetas),2)))})})
```

No real relationship with overdispersions

```{r lagrange multipliers, eval=FALSE, purl = FALSE}
##### Lagrange multipliers
par(mfrow=c(2,4))
lapply(names(solListWSab), function(Y){cat(Y, "\n");solListWSab[[Y]]$lambdaCol})
```

##### Biplots

First two dimensions

```{r ab biplots, results='hide', fig.show="hold", fig.width = 12, fig.height = 9, purl = FALSE}
#par(mfrow=c(2,4), mai = c(2,2,2,4))
par(mfrow=c(1,1))
palette(cols)
lapply(names(solListWSab), function(Y){plotRCM(solListWSab[[Y]], samColour = sampleSigNBab, main=Y, biplot=TRUE, Dim = c(1,2), taxColour=taxaSigNBab, taxLegPos = "bottomright", abundLeg=TRUE, taxCol  =cols, taxInset = c(-0.0,-0.0), libCex = 0.65, libInset = c(0.0,-0.0), arrowFrac = 0.05)})
```

Fisrt and third dimension

```{r ab23 biplots, results='hide', fig.show="hold", fig.width = 12, fig.height = 9, purl = FALSE}
#par(mfrow=c(2,4), mai = c(2,2,2,4))
par(mfrow=c(1,1))
palette(cols)
lapply(names(solListWSab), function(Y){plotRCM(solListWSab[[Y]], samColour = sampleSigNBab, main=Y, biplot=TRUE, Dim = c(1,3), taxColour=taxaSigNBab, taxLegPos = "bottomright", abundLeg=TRUE, taxCol  =cols, taxInset = c(-0,-0), libCex = 0.65, libInset = c(-0.0,-0.0), arrowFrac = 0.05)})
```

##### Influence function

Take a look at the influence function values on the psis

```{r Ab influence measures: psis, eval=FALSE, purl = FALSE}
infl1 = lapply(solListWSab, function(x, i){ 
  infl=with(x, NBpsiInfl(psi = psis[i], X = X, cMat = cMat[i,,drop=FALSE], rMat = rMat[,i, drop=FALSE], muMarg = outer(rowSums(X), colSums(X)/sum(X)), theta = thetas))
  id=abs(infl) > quantile( abs(infl),0.995)
  Xid = x$X[id]
  abunds = colSums(x$X)/sum(x$X)
  libsizes = rowSums(x$X)
  list(infl = infl, id = id, Xid = Xid, abunds=abunds, libsizes=libsizes)
},1)
par(mfrow=c(1,2))
lapply(names(infl1), function(x){
  with(infl1[[x]], plot(abunds, colSums(id), log="x", main=x, ylab="Number of very influential observations"))
}) #High abundances, larger influence
lapply(names(infl1), function(x){
  idTmp = sample(seq_along(infl1[[x]]$abunds), 200) #For speed
  with(infl1[[x]], plot(rep(abunds[idTmp], nrow(infl)), c(t(infl[,idTmp])), log="x", main=x, xlab="Abundance",ylab = "Influence")) #Very heavy!
})
lapply(names(infl1), function(x){
  idTmp = sample(seq_along(infl1[[x]]$libsizes), 200) #For speed
  with(infl1[[x]], plot(rep(libsizes[idTmp], ncol(infl)), c(infl[idTmp,]), log="", main=x, xlab ="Library sizes",ylab = "Influence")) #Very heavy!
})
#Expectations
lapply(names(infl1), function(x){
  with(infl1[[x]], {rbind(quantile(outer(libsizes, abunds)),
       quantile(outer(libsizes, abunds)[id])) })
}) #Influential observations have high expectations
lapply(names(infl1), function(x){
  cat(x, "\n")
  with(infl1[[x]], table(Xid))
}) # ... but are very often zero!

infl2 = lapply(solListWSab, function(x, i){
  infl=with(x, NBpsiInfl(psi = psis[i], X = X, cMat = cMat[i,,drop=FALSE], rMat = rMat[,i, drop=FALSE], muMarg = outer(rowSums(X), colSums(X)/sum(X)), theta = thetas))
  id=abs(infl) > quantile( abs(infl),0.995)
  Xid = x$X[id]
  abunds = colSums(x$X)/sum(x$X)
  libsizes = rowSums(x$X)
  list(infl = infl, id = id, Xid = Xid, abunds=abunds, libsizes=libsizes)
},2)
par(mfrow=c(1,2))
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(abunds, colSums(id), log="x", main=x))
})
#In the second dimension, larger abundance means larger influence on the psis too, and no outliers
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(rep(abunds, nrow(infl)), c(t(infl)), log="x", main=x, xlab="Abundance",ylab = "Influence")) #Very heavy!
}) #Outlier in unifmarg weigthing scheme
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(rep(libsizes, ncol(infl)), c(infl), log="x", main=x, xlab="Library size",ylab = "Influence")) #Very heavy! #No trend in terms of library sizes visible from here, but their distribution should be log-normal perhaps
})
#Expectations
lapply(names(infl2), function(x){
  with(infl2[[x]], {rbind(quantile(outer(libsizes, abunds)),
       quantile(outer(libsizes, abunds)[id])) })
})
lapply(names(infl2), function(x){
  cat(x, "\n")
  with(infl2[[x]], table(Xid))
})
```

The most influential observations are zero counts in highly abundant species and high libsizes (i.e. high expectations)! Note that we cen derive this from the influence function that does not even depend on the weights!

Influence on the colScores

```{r Ab influence colscores, eval=FALSE, purl = FALSE}
inflColList1 = lapply(solListWSab, function(x){
  try(with(x, NBcolInfl(X, psis, cMat, rMat,thetas , colWeights = if(is.matrix(colWeights)) colWeights else{ cbind(colWeights, colWeights, colWeights)} , k=1 , lambdaCol)))
})
#Look at the signal from the first group
Id1 = which(taxaSigNBab=="Signal 1")[1]

inflCol1 = lapply(inflColList1, function(x){getInflCol(x$score, x$InvJac, Id1)}) 
lapply(inflCol1, function(x){
  boxplot(x[,Id1]~sampleSigNBab, las=2, ylab="Influence")
})
```

As expected, the samples with the signal have the highest impact on the score

```{r Ab influence rowscores Ab, eval=FALSE, purl = FALSE}
inflRowList1 = lapply(solListWSab, function(x){
  try(with(x, NBrowInfl(X, psis, cMat, rMat,thetas , rowWeights , k=1 , lambdaRow)))
})
#Only look at most extreme colscores
#Cannot calculate influence functions for the interesting cases (with the outliers)
inflRowList1=inflRowList1[sapply(inflRowList1, class)=="list"]
#The influence on the first row score
 
par(mfrow=c(1,2))
idRow = 10
libSizes = rowSums(solListWSab[[1]]$X)
inflRow1 = lapply(inflRowList1, function(x){getInflRow(x$score, x$InvJac, idRow)})
lapply(names(inflRow1), function(x){
  plot(y=inflRow1[[x]][,idRow], libSizes, log="x", main=x, col=(libSizes ==libSizes[idRow])+1, ylab="Influence")
})
```

Evidently, the observation from the sample itself has the largest influence. In the marginal weighting scheme for the libsizes, the larger libsizes sometimes get a larger influence, although the effect may be small.


```{r tolerances, eval=FALSE, purl = FALSE}
#Achieved tolerances
tolAchPsi = with(syntNBSigmargmarg_ab, lapply(1:k, function(y){sapply( 2:iter[y], function(x){abs(1-psiRec[y,x]/psiRec[y,x-1])})}))
par(mfrow=c(1,3))
lapply(tolAchPsi, function(x){plot(x, log="y");abline(h=0.01, col="red");abline(h=0.001, col="blue")})

with(syntNBSigmargmarg_ab,plot(psiRec[3,1:iter[3]])) #Weird jumps, don't stop too early!
```

#### A log-uniform approach

We sample abundances and library sizes log-uniformly to get a better view of the scores in function of them

```{r NB with signal lu, purl = FALSE}
makeNBdata=function(meanMat, thetaMat){apply(array(data= c(meanMat, thetaMat), dim=c(nrow(meanMat), ncol(meanMat), 2)), c(1,2), function(x){rnbinom(1,mu=x[1], size=x[2])})}
NtaxaLogUnif = 1000
NsamplesLogUnif = 300
rhosRefLU = rhosSig1LU = rhosSig2LU = rhosSig3LU = rhosSig4LU = 10^(-runif(NtaxaLogUnif, 2.5,6))

NsamplesSignal1LU = NsamplesSignal2LU =  25
NsamplesSignal3LU = NsamplesSignal4LU =  20

NtaxaSignalNBLU = 40

Signal1NBLU = 10
Signal2NBLU = 8
Signal3NBLU = 7.5
Signal4NBLU = 7

idSig1NBLU = 1:NtaxaLogUnif %in% sample(1:NtaxaLogUnif, NtaxaSignalNBLU) #Random sampling should ensure orthogonality
idSig2NBLU = 1:NtaxaLogUnif %in% sample(1:NtaxaLogUnif, NtaxaSignalNBLU)
idSig3NBLU = 1:NtaxaLogUnif %in% sample(1:NtaxaLogUnif, NtaxaSignalNBLU) #Random sampling should ensure orthogonality
idSig4NBLU = 1:NtaxaLogUnif %in% sample(1:NtaxaLogUnif, NtaxaSignalNBLU)
#Apply the signals

rhosSig1LU[idSig1NBLU] = rhosSig1LU[idSig1NBLU]*Signal1NBLU
rhosSig2LU[idSig2NBLU] = rhosSig2LU[idSig2NBLU]*Signal2NBLU
rhosSig3LU[idSig3NBLU] = rhosSig3LU[idSig3NBLU]*Signal3NBLU
rhosSig4LU[idSig4NBLU] = rhosSig4LU[idSig4NBLU]*Signal4NBLU

#Renormalize
renorm=function(x){x/sum(x)}
rhosSig1LU=renorm(rhosSig1LU);rhosSig2LU=renorm(rhosSig2LU);
rhosSig3LU=renorm(rhosSig3LU);rhosSig4LU=renorm(rhosSig4LU);

libSizesNBLU = 10^runif(NsamplesLogUnif, 3.5,5.5)

#Generate data
Nref = (NsamplesLogUnif-NsamplesSignal1LU-NsamplesSignal2LU-NsamplesSignal3LU-NsamplesSignal4LU)
meanMatRefNBLU = outer(libSizesNBLU[sample(size=Nref, 1:NsamplesLogUnif)], rhosRefLU)
meanMatSig1NBLU = outer(libSizesNBLU[sample(size=NsamplesSignal1LU, 1:NsamplesLogUnif)], rhosSig1LU)
meanMatSig2NBLU = outer(libSizesNBLU[sample(size=NsamplesSignal2LU, 1:NsamplesLogUnif)], rhosSig2LU)
meanMatSig3NBLU = outer(libSizesNBLU[sample(size=NsamplesSignal3LU, 1:NsamplesLogUnif)], rhosSig3LU)
meanMatSig4NBLU = outer(libSizesNBLU[sample(size=NsamplesSignal4LU, 1:NsamplesLogUnif)], rhosSig4LU)
load("/home/stijn/PhD/American Gut/AGpars.RData")
thetaMatSigNBLU = matrix(sample(thetas, NtaxaLogUnif), byrow=TRUE, ncol=NtaxaLogUnif, nrow=NsamplesLogUnif)

dataMatRefNBLU = makeNBdata(meanMatRefNBLU, thetaMatSigNBLU)
dataMatSig1NBLU = makeNBdata(meanMatSig1NBLU, thetaMatSigNBLU)
dataMatSig2NBLU = makeNBdata(meanMatSig2NBLU, thetaMatSigNBLU)
dataMatSig3NBLU = makeNBdata(meanMatSig3NBLU, thetaMatSigNBLU)
dataMatSig4NBLU = makeNBdata(meanMatSig4NBLU, thetaMatSigNBLU)

dataMatSigNBLU = rbind(dataMatRefNBLU, dataMatSig1NBLU, dataMatSig2NBLU, dataMatSig3NBLU, dataMatSig4NBLU)

#Save signals
sampleSigNBLU = factor(c(rep("Reference", Nref), rep("Signal1", NsamplesSignal1LU), rep("Signal2", NsamplesSignal2LU), rep("Signal 3", NsamplesSignal3LU), rep("Signal 4", NsamplesSignal4LU)))
taxaSigNBLUtmp = rep("Reference",ncol(dataMatSigNBLU))
taxaSigNBLUtmp[idSig1NBLU] = "Signal 1"
taxaSigNBLUtmp[idSig2NBLU] = "Signal 2"
taxaSigNBLUtmp[idSig3NBLU] = "Signal 3"
taxaSigNBLUtmp[idSig4NBLU] = "Signal 4"
taxaSigNBLUtmp[idSig1NBLU & idSig2NBLU] = "Signal 1 and 2"
taxaSigNBLUtmp[idSig1NBLU & idSig3NBLU] = "Signal 1 and 3"
taxaSigNBLUtmp[idSig1NBLU & idSig4NBLU] = "Signal 1 and 4"
taxaSigNBLUtmp[idSig2NBLU & idSig3NBLU] = "Signal 2 and 3"
taxaSigNBLUtmp[idSig2NBLU & idSig4NBLU] = "Signal 2 and 4"
taxaSigNBLUtmp[idSig3NBLU & idSig4NBLU] = "Signal 3 and 4"
taxaSigNBLUtmp[idSig1NBLU & idSig2NBLU & idSig3NBLU] = "Signal 1 and 2 and 3"
taxaSigNBLUtmp[idSig4NBLU & idSig2NBLU & idSig3NBLU] = "Signal 2 and 3 and 4"
taxaSigNBLUtmp[idSig1NBLU & idSig2NBLU & idSig4NBLU] = "Signal 1 and 2 and 4"
taxaSigNBLUtmp[idSig1NBLU & idSig4NBLU & idSig3NBLU] = "Signal 1 and 3 and 4"
taxaSigNBLUtmp[idSig1NBLU & idSig4NBLU & idSig2NBLU & idSig3NBLU] = "Signal 1,2,3 and 4"
taxaSigNBLU = factor(taxaSigNBLUtmp) 
names(taxaSigNBLU) = colnames(dataMatSigNBLU) = names(rhosRefLU)
names(sampleSigNBLU) = rownames(dataMatSigNBLU) = 1:NsamplesLogUnif
```

Fit the RC(M) model to the logUnif data

```{r Fit3Dlu, eval=FALSE, purl = FALSE}
nleqslv.control.lu = list(trace=TRUE, maxit = 500, cndtol=.Machine$double.eps)
if(!file.exists("toyDataSigLU.RData")){

    syntNBSigmargmarg_LUJob = mcparallel(RCM(dataMatSigNBLU, distribution="NB", k=2, nleqslv.control= nleqslv.control.lu, maxItOut=2e4, prevCutOff=0.01, colWeights="marginal", rowWeights = "marginal", marginEst="MLE"))
  syntNBSigmargmarg_LU = mccollect(syntNBSigmargmarg_LUJob, FALSE)[[1]] 
  
      syntNBSigunifmarg_LUJob = mcparallel(RCM(dataMatSigNBLU, distribution="NB", k=2, nleqslv.control= nleqslv.control.lu, maxItOut=2e4, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform", marginEst="MLE"))
  syntNBSigunifmarg_LU = mccollect(syntNBSigunifmarg_LUJob, FALSE)[[1]] 
  
      syntNBSigmargunif_LUJob = mcparallel(RCM(dataMatSigNBLU, distribution="NB", k=2, nleqslv.control= nleqslv.control.lu, maxItOut=2e4, prevCutOff=0.01, rowWeights="marginal", colWeights = "uniform", marginEst="MLE"))
  syntNBSigmargunif_LU = mccollect(syntNBSigmargunif_LUJob, FALSE)[[1]] 
  
      syntNBSigunifunif_LUJob = mcparallel(RCM(dataMatSigNBLU, distribution="NB", k=2, nleqslv.control= nleqslv.control.lu, maxItOut=2e4, prevCutOff=0.01, colWeights="uniform", rowWeights = "uniform", marginEst="MLE"))
  syntNBSigunifunif_LU = mccollect(syntNBSigunifunif_LUJob, FALSE)[[1]] 
  
#     Dim = dim(dataMatSigNBLU)
#   unifWeightsLU = rep(1/Dim[2], Dim[2])
#   margWeightsLU = colSums(dataMatSigNBLU)/sum(dataMatSigNBLU)
#   colWeightsTestLU1 = cbind(unifWeightsLU, margWeightsLU, unifWeightsLU)
#   colWeightsTestLU2 = cbind(margWeightsLU, margWeightsLU, unifWeightsLU)
#   colWeightsTestLU3 = cbind(margWeightsLU, unifWeightsLU, unifWeightsLU)

      syntNBSigmargmarg_LUJobMLE = mcparallel(RCM(dataMatSigNBLU, distribution="NB", k=3, nleqslv.control= nleqslv.control.lu, maxItOut=2e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "marginal", marginEst = "MLE"))
  syntNBSigmargmarg_LUMLE = mccollect(syntNBSigmargmarg_LUJobMLE, FALSE)[[1]] 
  
      syntNBSigunifmarg_LUJobMLE = mcparallel(RCM(dataMatSigNBLU, distribution="NB", k=3, nleqslv.control= nleqslv.control.lu, maxItOut=2e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform", marginEst = "MLE"))
  syntNBSigunifmarg_LUMLE = mccollect(syntNBSigunifmarg_LUJobMLE, FALSE)[[1]] 
  
      syntNBSigmargunif_LUJobMLE = mcparallel(RCM(dataMatSigNBLU, distribution="NB", k=3, nleqslv.control= nleqslv.control.lu, maxItOut=2e3, prevCutOff=0.01, rowWeights="marginal", colWeights = "uniform", marginEst = "MLE"))
  syntNBSigmargunif_LUMLE = mccollect(syntNBSigmargunif_LUJobMLE, FALSE)[[1]] 
  
      syntNBSigunifunif_LUJobMLE = mcparallel(RCM(dataMatSigNBLU, distribution="NB", k=3, nleqslv.control= nleqslv.control.lu, maxItOut=2e3, prevCutOff=0.01, colWeights="uniform", rowWeights = "uniform", marginEst = "MLE"))
  syntNBSigunifunif_LUMLE = mccollect(syntNBSigunifunif_LUJobMLE, FALSE)[[1]] 
 
  save(dataMatSigNBLU, syntNBSigunifmarg_LU, syntNBSigunifunif_LU,syntNBSigmargmarg_LU,syntNBSigmargunif_LU, sampleSigNBLU, taxaSigNBLU,  file="toyDataSigLU.RData")#syntNBSigunif, syntNBSigmarg, syntNBSigmarg_3,testLU1, testLU2, testLU3, testLU1unif, testLU2unif, testLU3unif,testLUInv,testLULibs,
} else {load("toyDataSigLU.RData")}
```

LU plots

```{r LU plots, include=FALSE, purl = FALSE}
load("toyDataSigLUserver.RData")
# solListWSLU = list("unifunif" = syntNBSigunifunif_LU, "unifmarg" = syntNBSigunifmarg_LU, "margunif" = syntNBSigmargunif_LU, "margmarg" = syntNBSigmargmarg_LU, "unifmargunifRmarg" = testLU1,  "margmargunifRmarg"=testLU2, "margunifunifRmarg" = testLU3, "unifmargunifRunif" = testLU1unif, "margmargunifRunif"=testLU2unif, "margunifunifRunif" = testLU3unif, "inverse"= testLUInv, "testEqualLibs" =   testLULibs, "unifunifMLE" = syntNBSigunifunif_LUMLE, "unifmargMLE" = syntNBSigunifmarg_LUMLE, "margunifMLE" = syntNBSigmargunif_LUMLE, "margmargMLE" = syntNBSigmargmarg_LUMLE )
names(sigListLU) = c("unifunif","unifunifMLE","margunif","margunifMLE","unifmarg","unifmargMLE","margmarg","margmargMLE")
solListWSLU = sigListLU

solListWSLU = solListWSLU[sapply(solListWSLU, class)=="list"] 
#Runtimes and convergence
sapply(solListWSLU,function(x){x$converged})
sapply(solListWSLU,function(x){x$runtime})
sapply(solListWSLU,function(x){x$iter})
```

The first Abreviation refers to the weighting scheme for the rows(samples), the second to the weighting scheme for the columns. E.g "margunif" means $w_i = x_{i.}$ and $z_j = 1/p$. The MLE epitheton indicates that the independence model was estimated by MLE.

##### Sample plots

```{r LUSignalPlots, eval=FALSE, purl = FALSE}
cols = c("grey", "red","blue","purple","green","brown","cyan","black", "orange", "magenta","yellow","pink", "olive","grey75","grey85") 
palette(cols)
par(mfrow=c(2,4))
lapply(names(solListWSLU), function(Y){plotRCM(solListWSLU[[Y]], samColour = sampleSigNBLU, main=Y, biplot=FALSE, Dim = c(1,2), libInset=c(-0.8,0))})
```

```{r LUSignalPlots23D, eval=FALSE, purl = FALSE}
par(mfrow=c(2,4))
lapply(names(solListWSLU), function(Y){try(plotRCM(solListWSLU[[Y]], samColour = sampleSigNBLU, main=Y, biplot=FALSE, Dim = c(1,3), libInset=c(-0.8,0)))})
```

Only when the margins are estimated with MLE are the first two dimensions used to separate the signal well. Especially the uniform weighting for the samples and marginal for the columns (as was our first intuition) appears to perform well. Also uniform weighting for both yields good results.

```{r LUSignalPlots 13D, eval=FALSE, purl = FALSE}
par(mfrow=c(2,4))
#Maybe a function of the library sizes
lapply(names(solListWSLU), function(Y){with(solListWSLU[[Y]], {
  rMatPsi = rMat %*% diag(psis)
  dfCol = data.frame(Dim1=rMatPsi[,1], Dim2=rMatPsi[,2], col=log(rowSums(X)))
  ggplot(data=dfCol, aes(x=Dim1, y=Dim2, col=col)) + geom_point(size=3) + ggtitle(Y)+ scale_colour_continuous(name = "Library sizes", low="red",high = "green")
    })
  })
```

On these plots it is clear that when the margins are estimated using the library sizes, the scores correlate with the library sizes.

Look at the scores' relationship to the library sizes more directly.

```{r LU libsizes, eval=FALSE, results = "hide", purl = FALSE}
#Look at the loadings in function of the library sizes and abunds
par(pty = "m", mfrow = c(2,4), mar=c(5,4,4,4))
logP="y"
#Libsizes
lapply(names(solListWSLU), function(Y){with(solListWSLU[[Y]], {plot(main=Y,rMat[,1] *psis[1],rowSums(X), log=logP, xlab ="Dim1", ylab = "Library sizes", sub = paste0("Cor = ", round(cor(rMat[,1], rowSums(X)),2)) )})})
lapply(names(solListWSLU), function(Y){with(solListWSLU[[Y]], {plot(main=Y,rMat[,2] *psis[2],rowSums(X), log=logP, xlab ="Dim2", ylab = "Library sizes" , sub = paste0("Cor = ", round(cor(rMat[,2], rowSums(X)),2)))})})
lapply(names(solListWSLU), function(Y){try(with(solListWSLU[[Y]], {plot(main=Y,rMat[,3] *psis[3],rowSums(X), log=logP, xlab ="Dim3", ylab = "Library sizes" , sub = paste0("Cor = ", round(cor(rMat[,3], rowSums(X)),2)))}))})
```

In the MLE framework for the offsets, dependence on the library sizes has disappeared

##### Taxa plots

Now let's take a look at the taxon plots

```{r NBLU taxon plots, results='hide', eval=FALSE, purl = FALSE}
par(mfrow=c(2,4))
lapply(names(solListWSLU), function(Y){with(solListWSLU[[Y]], {plot(main=Y,t(cMat), ylab="Dim2",xlab="Dim1", col = taxaSigNBLU)}); legend("topright",legend = levels(taxaSigNBLU), col=cols,pch= 1, cex=0.7, inset = c(-0.80,0), xpd=TRUE) })
```

For the uniform weighting of the samples we see outliers for the taxa

Are the taxon scores related to the abundances in any way?

```{r taxonScesvsabunds1DLU, results='hide', eval=FALSE, purl = FALSE}
#abundances
par(mfrow=c(2,4), mar=c(5,4,4,4))
sapply(names(solListWSLU), function(Y){with(solListWSLU[[Y]], {plot(main=Y,cMat[1,], colSums(X), log="y", cex=0.5, ylab="Mean  abundances",xlab="Dim1", sub = paste0("Cor = ", round(cor(cMat[1,], colSums(X)),2)))})})
```

The MLE offset estimation does reduce correlation with the mean abundances.

Second and third dimensions

```{r taxonScesvsabunds23DLU, results='hide', eval=FALSE, purl = FALSE}
par(mfrow=c(2,4), mar=c(5,4,4,4))
lapply(names(solListWSLU), function(Y){with(solListWSLU[[Y]], {plot(main=Y,cMat[2,], colSums(X), log="y", cex=0.5, ylab="Mean  abundances",xlab="Dim2", sub = paste0("Cor = ", round(cor(cMat[2,], colSums(X)),2)))})})
par(mfrow=c(2,4), mar=c(5,4,4,4))
lapply(names(solListWSLU), function(Y){try(with(solListWSLU[[Y]], {plot(main=Y,cMat[3,], colSums(X), log="y", cex=0.5, ylab="Mean  abundances",xlab="Dim3", sub = paste0("Cor = ", round(cor(cMat[3,], colSums(X)),2)))}))})
```

No more problems with correlation in higher dimensions.

Do the taxon scores relate to the overdispersions?

```{r taxonScesvsthetas2, results='hide', eval=FALSE, purl = FALSE}
par(mfrow=c(2,4))
lapply(names(solListWSLU), function(Y){with(solListWSLU[[Y]], {plot(main=Y,abs(cMat[1,]), thetas, log="y", cex=0.5, ylab="Overdispersion", xlab="Dim1", sub = paste0("Cor = ", round(cor(abs(cMat[1,]), thetas),2)))})})
par(mfrow=c(2,4))
lapply(names(solListWSLU), function(Y){with(solListWSLU[[Y]], {plot(main=Y,cMat[2,], thetas, log="y", cex=0.5, ylab="Overdispersion", xlab="Dim2", sub = paste0("Cor = ", round(cor(abs(cMat[2,]), thetas),2)))})})
```

No real relationship with overdispersions

```{r lagrange multiplierslu, eval=FALSE, purl = FALSE}
##### Lagrange multipliers
par(mfrow=c(2,4))
lapply(names(solListWSLU), function(Y){cat(Y, "\n");solListWSLU[[Y]]$lambdaCol})
```

##### Biplots

```{r LU biplots, results='hide', eval=FALSE, purl = FALSE}
par(mfrow=c(2,4), mai = c(2,2,2,4))
par(mfrow=c(1,1))
lapply(names(solListWSLU), function(Y){plotRCM(solListWSLU[[Y]], samColour = sampleSigNBLU, main=Y, biplot=TRUE, Dim = c(1,3), taxColour=tmp, taxLegPos = "bottomright", abundLeg=TRUE, taxCol  =cols, taxInset = c(-1.45,-0.75), libCex = 0.65, libInset = c(-0.85,0), arrowFrac = 0.05)})
```

##### Influence function

Take a look at the influence function values on the psis

```{r LU influence measures: psis, eval=FALSE, purl = FALSE}
infl1 = lapply(solListWSLU, function(x, i){ 
  infl=with(x, NBpsiInfl(psi = psis[i], X = X, cMat = cMat[i,,drop=FALSE], rMat = rMat[,i, drop=FALSE], muMarg = outer(rowSums(X), colSums(X)/sum(X)), theta = thetas))
  id=LUs(infl) > quantile( LUs(infl),0.995)
  Xid = x$X[id]
  abunds = colSums(x$X)/sum(x$X)
  libsizes = rowSums(x$X)
  list(infl = infl, id = id, Xid = Xid, abunds=abunds, libsizes=libsizes)
},1)
par(mfrow=c(1,2))
lapply(names(infl1), function(x){
  with(infl1[[x]], plot(abunds, colSums(id), log="x", main=x, ylab="Number of very influential observations"))
}) #High abundances, larger influence
lapply(names(infl1), function(x){
  idTmp = sample(seq_along(infl1[[x]]$abunds), 200) #For speed
  with(infl1[[x]], plot(rep(abunds[idTmp], nrow(infl)), c(t(infl[,idTmp])), log="x", main=x, xlab="abundance",ylab = "Influence")) #Very heavy!
})
lapply(names(infl1), function(x){
  idTmp = sample(seq_along(infl1[[x]]$libsizes), 200) #For speed
  with(infl1[[x]], plot(rep(libsizes[idTmp], ncol(infl)), c(infl[idTmp,]), log="", main=x, xlab ="Library sizes",ylab = "Influence")) #Very heavy!
})
#Expectations
lapply(names(infl1), function(x){
  with(infl1[[x]], {rbind(quantile(outer(libsizes, abunds)),
       quantile(outer(libsizes, abunds)[id])) })
}) #Influential observations have high expectations
lapply(names(infl1), function(x){
  cat(x, "\n")
  with(infl1[[x]], table(Xid))
}) # ... but are very often zero!

infl2 = lapply(solListWSLU, function(x, i){
  infl=with(x, NBpsiInfl(psi = psis[i], X = X, cMat = cMat[i,,drop=FALSE], rMat = rMat[,i, drop=FALSE], muMarg = outer(rowSums(X), colSums(X)/sum(X)), theta = thetas))
  id=LUs(infl) > quantile( LUs(infl),0.995)
  Xid = x$X[id]
  abunds = colSums(x$X)/sum(x$X)
  libsizes = rowSums(x$X)
  list(infl = infl, id = id, Xid = Xid, abunds=abunds, libsizes=libsizes)
},2)
par(mfrow=c(1,2))
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(abunds, colSums(id), log="x", main=x))
})
#In the second dimension, larger abundance means larger influence on the psis too, and no outliers
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(rep(abunds, nrow(infl)), c(t(infl)), log="x", main=x, xlab="abundance",ylab = "Influence")) #Very heavy!
}) #Outlier in unifmarg weigthing scheme
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(rep(libsizes, ncol(infl)), c(infl), log="x", main=x, xlab="Library size",ylab = "Influence")) #Very heavy! #No trend in terms of library sizes visible from here, but their distribution should be log-normal perhaps
})
#Expectations
lapply(names(infl2), function(x){
  with(infl2[[x]], {rbind(quantile(outer(libsizes, abunds)),
       quantile(outer(libsizes, abunds)[id])) })
})
lapply(names(infl2), function(x){
  cat(x, "\n")
  with(infl2[[x]], tLUle(Xid))
})
```

The most influential observations are zero counts in highly abundant species and high libsizes (i.e. high expectations)! Note that we cen derive this from the influence function that does not even depend on the weights!

Influence on the colScores

```{r LU influence colscores, eval=FALSE, purl = FALSE}
inflColList1 = lapply(solListWSLU, function(x){
  try(with(x, NBcolInfl(X, psis, cMat, rMat,thetas , colWeights = if(is.matrix(colWeights)) colWeights else{ cbind(colWeights, colWeights, colWeights)} , k=1 , lambdaCol)))
})
#Look at the signal from the first group
Id1 = which(taxaSigNBLU=="Signal 1")[1]

inflCol1 = lapply(inflColList1, function(x){getInflCol(x$score, x$InvJac, Id1)}) 
lapply(inflCol1, function(x){
  boxplot(x[,Id1]~sampleSigNBLU, las=2, ylab="Influence")
})
```

As expected, the samples with the signal have the highest impact on the score

```{r LU influence rowscores LU, eval=FALSE, purl = FALSE}
inflRowList1 = lapply(solListWSLU, function(x){
  try(with(x, NBrowInfl(X, psis, cMat, rMat,thetas , rowWeights , k=1 , lambdaRow)))
})
#Only look at most extreme colscores
#Cannot calculate influence functions for the interesting cases (with the outliers)
inflRowList1=inflRowList1[sapply(inflRowList1, class)=="list"]
#The influence on the first row score
 
par(mfrow=c(1,2))
idRow = 10
libSizes = rowSums(solListWSLU[[1]]$X)
inflRow1 = lapply(inflRowList1, function(x){getInflRow(x$score, x$InvJac, idRow)})
lapply(names(inflRow1), function(x){
  plot(y=inflRow1[[x]][,idRow], libSizes, log="x", main=x, col=(libSizes ==libSizes[idRow])+1, ylab="Influence")
})
```

Evidently, the observation from the sample itself has the largest influence. In the marginal weighting scheme for the libsizes, the larger libsizes sometimes get a larger influence, although the effect may be small.

Achieved tolerances

```{r toleranceslu, eval=FALSE, purl = FALSE}
tolAchPsi = with(syntNBSigmargmarg_LU, lapply(1:k, function(y){sapply( 2:iter[y], function(x){LUs(1-psiRec[y,x]/psiRec[y,x-1])})}))
par(mfrow=c(1,3))
lapply(tolAchPsi, function(x){plot(x, log="y");LUline(h=0.01, col="red"); abline(h=0.001, col="blue")})

with(syntNBSigmargmarg_LU,plot(psiRec[3,1:iter[3]])) #Weird jumps, don't stop too early!
```

#### Real data examples

Finally we also apply the method to real datasets

```{r RC(M)_NB real datasets, purl = FALSE}
realNames = c("AGP", "Hard.palate","Buccal.mucosa","Anterior.nares","Left.Antecubital.fossa", "Posterior.fornix", "Mid.vagina", "Left.Retroauricular.crease" )
load("/home/stijn/PhD/Simulations/data/physeqListV13.RData")
load("/home/stijn/PhD/Simulations/data/AGphylo.RData")
fileNames = paste(realNames, "RCM.RData")#"Kostic", "zeller16S", "zellerMeta",
resListRCM = lapply(fileNames, function(x){
  load(x)
  RCMres
})

names(resListRCM) = realNames

sapply(resListRCM,function(x){x$converged}) # All converged
sapply(resListRCM,function(x){x$runtime})
sapply(resListRCM,function(x){x$iter})
```

The HMP dataset

```{r HMP plot, purl = FALSE}
SClevels  = unique(c(unlist(sapply(physeqListV13, function(x){
  levels(get_variable("RUNCENTER",physeq=x))
}))))
#Give similar colours for the same sequencing center
cols = c("cadetblue","cyan", "darkblue","blue","magenta","orange","yellow","brown","black",  "grey80","grey70","pink") 
palette(cols)
physeqListV13_SC = lapply(physeqListV13, function(x){
  sample_data(x)[["RUNCENTER"]] = factor(sample_data(x)[["RUNCENTER"]], levels = SClevels, labels = SClevels)
  x
})

phyList = c(AGP = AGphylo, physeqListV13_SC)

resListRCM2 = lapply(realNames, function(x){
  tmp = resListRCM[[x]]
  tmp$physeq = prune_samples(rownames(tmp$X),prune_taxa(colnames(tmp$X), phyList[[x]]))
  tmp
}) 

names(resListRCM2) = realNames

par(mfrow = c(2,4))
lapply(names(resListRCM2)[-1], function(x){plotRCM(resListRCM2[[x]], biplot=FALSE, samColour = "sex", main=x)})
par(mfrow = c(2,4))
lapply(names(resListRCM2)[-1], function(x){plotRCM(resListRCM2[[x]], biplot=FALSE, samColour = "RUNCENTER", main=x, libInset = c(-0.9,-0.1))})
```

We've discovered a clear impact of the runcenter!
Now let's filter out the impact of this variable

```{r HMP filter out variable effect, purl=FALSE}
K=2
nleqslv.control = list()
#Group the runcenters together

RCMhmpFilt = lapply(physeqListV13, function(x){
  runcenters = data.frame(sample_data(x))[ "RUNCENTER"]
runcenters[["RUNCENTER"]][runcenters[["RUNCENTER"]] %in% c("BCM","BI,BCM","BCM,BI","BCM,WUGC", "BCM,JCVI")] = "BCM"
runcenters[["RUNCENTER"]][runcenters[["RUNCENTER"]] %in% c("JCVI","JCVI,BI","JCVI,WUGC")] = "JCVI"
runcenters[["RUNCENTER"]][runcenters[["RUNCENTER"]] %in% c("WUGC,JCVI","WUGC","WUGC,BCM")] = "WUGC"
sample_data(x) = data.frame(sample_data(x) ,RUNCENTER2 = runcenters[[1]])
x})

physeq = RCMhmpFilt[["Buccal.mucosa"]]
conf = get_variable(physeq, "RUNCENTER2")
confDrop = droplevels(conf)[[1]]

# antNarJob = mcparallel(  RCM(RCMhmpFilt[["Anterior.nares"]], distribution="NB", k = K, nleqslv.control= nleqslv.control, maxItOut=5e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform", marginEst = "MLE", round=TRUE, confounders = "RUNCENTER2"))
# antNar = mccollect(antNarJob, wait=FALSE)[[1]]
# save(antNar, file="antNarConfounders.RData")

if(!file.exists(file="/home/stijn/PhD/Biplots/filtResHMP.RData")){
filtResJob = mcparallel(lapply(RCMhmpFilt, function(x){
  RCM(x, distribution="NB", k = K, nleqslv.control= nleqslv.control, maxItOut=5e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform", marginEst = "MLE", round=TRUE, confounders = "RUNCENTER2")
  }))
filtRes = mccollect(filtResJob, wait=FALSE)[[1]]
save(filtRes, file="/home/stijn/PhD/Biplots/filtResHMP.RData")
} else {load(file="/home/stijn/PhD/Biplots/filtResHMP.RData")}
par(mfrow = c(3,3))
lapply(names(filtRes), function(x){plotRCM(filtRes[[x]], biplot=FALSE, samColour = "RUNCENTER", main=x, libInset = c(-1.9,-0.1))})
lapply(names(filtRes), function(x){plotRCM(filtRes[[x]], biplot=FALSE, samColour = "RUNCENTER2", main=x, libInset = c(-0.9,-0.1))})
par(mfrow = c(1,1))
```


```{r filterTwoConfounders, purl = FALSE}
#With two confounders
confTestJob = mcparallel(RCM(RCMhmpFilt[["Anterior.nares"]], distribution="NB", k = K, nleqslv.control= nleqslv.control, maxItOut=5e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform", marginEst = "MLE", round=TRUE, confounders = c("sex","RUNCENTER2")))
confTest = mccollect(confTestJob, wait=FALSE)[[1]]
save(confTest, file="/home/stijn/PhD/Biplots/confTestHMP.RData")
```

The AGP dataset

```{r AGPplots, purl = FALSE}
par(mfrow = c(3,3))
AGPvars = c("IBD","AGE","SEX","PREGNANT","LACTOSE","ASTHMA","DIABETES", "DIET_TYPE","COUNTRY")
sapply(AGPvars, function(x){
plotRCM(resListRCM[["AGP"]], biplot=FALSE, samColour = x, main=x, libInset = c(-2.9,-0.1), libLeg=TRUE)
})
```

The AGP dataset is presumably too noisy to find much of a signal

```{r fitting process, purl = FALSE}
lapply(names(resListRCM), function(x){with(resListRCM[[x]],{ plot(main=x, rowRec[2,1,1:iter[1]])})})
```

AGP is known to be very noisy and does not yield anything.

The Zeller data

```{r load zeller RC(M) NB results, purl = FALSE}
load("/home/stijn/PhD/Simulations/data/zellerData.RData")

zellers = c("zellerMeta","zeller16S")
# zellerRCM  = lapply(paste(zellers, "RCM.RData"), function(x){
#   load(x)
#   RCMres
# })
if(!file.exists("zellerSphyRCM.RData")){
zellerSphyRCM = RCM(zellerSphy, k=2, round = TRUE) # Less than one minute!
save(zellerSphyRCM, file="zellerSphyRCM.RData")
} else {load("zellerSphyRCM.RData")}

if(!file.exists("zellerMphyRCM.RData")){
zellerMphyRCM = RCM(zellerMphy, k=2, round = TRUE, prevCutOff = 0.05, minFraction = 0.15) # Less than one minute! More trimming needed here
save(zellerMphyRCM, file="zellerMphyRCM.RData")
} else {load("zellerMphyRCM.RData")}

zellerRCM  =list(zellerMphyRCM, zellerSphyRCM)
names(zellerRCM) = zellers

sapply(zellerRCM,function(x){x$converged}) # All converged
sapply(zellerRCM,function(x){x$runtime})
sapply(zellerRCM,function(x){x$iter})

# zellerRCM[[1]]$physeq = zellerMphy 
# zellerRCM[[2]]$physeq = zellerSphy 

cols = c("darkblue","orange","darkgreen", "brown","black", "blue","magenta","cadetblue","cyan", "yellow", "grey80","grey70","pink") 
palette(cols)

par(mfcol = c(2,3))
sapply(c("Country","Diagnosis","Gender"), function(y){
sapply(names(zellerRCM), function(x){plotRCM(zellerRCM[[x]], biplot=FALSE, samColour = y, main=paste(x,y, collapse="_"))})
})
```

For the 16S data we see signals for Country and Cancer diagnosis. For the metagenomics data we do not see a trend. Notice we are doing indirect gradient analysis again, but this is just to look for confirmation that the unconstrained analysis is detecting true groups.

```{r plotZellerSphy, purl = FALSE}
PCbiplot(zellerMphyRCM, colour = "Age")
PCbiplot(zellerMphyRCM, colour = "BMI")
```

### Zero-inflated poisson

#### ZIP without signal

##### Generate data

```{r ZIP without signal, eval=FALSE, purl = FALSE}
load("/home/stijn/PhD/American Gut/AGphylo.RData")
#First estimate the ZIP parameters

if(!file.exists("AGzipParams.RData")){
  otuTab = otu_table(AGphylo)@.Data
  logLibs = log(sample_sums(AGphylo))
  ZIPfits = mclapply(mc.cores=4,1:ncol(otuTab),  function(i){
    zeroinfl(otuTab[,i]~offset(logLibs)| 1)
  })
  zeroProbs = sapply(ZIPfits, function(x){expit(x$coef$zero)})
  ZIPmeans = sapply(ZIPfits, function(x){exp(x$coef$count)})
  save(zeroProbs, ZIPmeans, file="AGzipParams.RData")
} else {load(file="AGzipParams.RData")}
#ZeroProbs are very high

#Define parameters
zeroProbs = zeroProbs[zeroProbs<0.9]
NsamplesZIPnoSig= 200
NtaxaZIPnoSig = 800
idSampleZIP = sample(size=NtaxaZIPnoSig, 1:length(ZIPmeans))
lambdasZIPnoSig = ZIPmeans[idSampleZIP]
lambdasZIPnoSig= lambdasZIPnoSig/sum(lambdasZIPnoSig)
zeroesZIPnoSig = sample(zeroProbs, NtaxaZIPnoSig)
libSizesZIPnoSig =c(rep(1e5, floor(NsamplesZIPnoSig/2)), rep(1e6, floor(NsamplesZIPnoSig/2)))

#Mean and zero matrices
meanMatZIPnoSig = outer(libSizesZIPnoSig, lambdasZIPnoSig)
zeroMatZIPnoSig = matrix(zeroesZIPnoSig, nrow=NsamplesZIPnoSig, ncol=NtaxaZIPnoSig, byrow = TRUE)

#Data generation
dataMatZIPnoSig = matrix(rzipois(n=prod(dim(meanMatZIPnoSig)),lambda = meanMatZIPnoSig, pstr0 = zeroMatZIPnoSig), ncol=NtaxaZIPnoSig, nrow=NsamplesZIPnoSig)
save(dataMatZIPnoSig, file = "zipNSData.RData")
```

##### Fit RC(M)

```{r RC(M) fit, eval=FALSE, purl = FALSE}
nleqslv.controlZIP = list(trace=TRUE, maxit=250, cndtol = .Machine$double.eps, allowSingular=TRUE)
if(!file.exists("syntZIPnoSig.RData")){
  syntZIPnoSigUnifUnifJob = mcparallel(RCM(dataMatZIPnoSig, method = "ZIP", k=2, nleqslv.control = nleqslv.controlZIP, colWeights = "uniform", rowWeights = "uniform", maxItOut=5e0))
  syntZIPnoSigUnifUnif =  mccollect(syntZIPnoSigUnifUnifJob, FALSE)[[1]] 
  
  syntZIPnoSigUnifMargJob = mcparallel(RCM(dataMatZIPnoSig, method = "ZIP",k=2, nleqslv.control = nleqslv.controlZIP, colWeights = "marginal", rowWeights = "uniform", maxItOut=5e2))
  syntZIPnoSigUnifMarg =  mccollect(syntZIPnoSigUnifMargJob, FALSE)[[1]]
  
  syntZIPnoSigMargUnifJob = mcparallel(RCM(dataMatZIPnoSig, method = "ZIP",k=2, nleqslv.control = nleqslv.controlZIP, colWeights = "uniform", rowWeights = "marginal", maxItOut=5e2))
  syntZIPnoSigMargUnif =  mccollect(syntZIPnoSigMargUnifJob, FALSE)[[1]]
  
   syntZIPnoSigMargMargJob = mcparallel(RCM(dataMatZIPnoSig, method = "ZIP",k=2, nleqslv.control = nleqslv.controlZIP, colWeights = "marginal", rowWeights = "marginal", maxItOut=5e1))
  syntZIPnoSigMargMarg =  mccollect(syntZIPnoSigMargMargJob, FALSE)[[1]]
  
  save(syntZIPnoSigUnifMarg, syntZIPnoSigUnifUnif, syntZIPnoSigMargMarg, syntZIPnoSigMargUnif, dataMatZIPnoSig, file="syntZIPnoSig.RData")
} else {load("syntZIPnoSig.RData")}
#No convergence, mean cannot be fitted
```

##### Plot the results

```{r RC(M) zip, eval=FALSE, purl = FALSE}
load(file="toyDataNoSigZIPserver.RData")
solListWSZIP = noSigListZIP[sapply(noSigListZIP, class)=="list"]
#Runtimes and convergence
sapply(solListWSZIP,function(x){x$converged})
sapply(solListWSZIP,function(x){x$runtime})
sapply(solListWSZIP,function(x){x$iter})

par(mfrow=c(2,2))
sapply(solListWSZIP,function(x){with(x, plot(psiRec[1,1:iter[1]]))})
```

#### ZIP with signal

```{r ZIP with signal, eval=FALSE, purl = FALSE}
load("/home/stijn/PhD/American Gut/AGphylo.RData")
#First estimate/load the ZIP parameters

if(!file.exists("AGzipParams.RData")){
  otuTab = otu_table(AGphylo)@.Data
  logLibs = log(sample_sums(AGphylo))
  ZIPfits = mclapply(mc.cores=4,1:ncol(otuTab),  function(i){
    zeroinfl(otuTab[,i]~offset(logLibs)| 1)
  })
  zeroProbs = sapply(ZIPfits, function(x){expit(x$coef$zero)})
  ZIPmeans = sapply(ZIPfits, function(x){exp(x$coef$count)})
  save(zeroProbs, ZIPmeans, file="AGzipParams.RData")
} else {load(file="AGzipParams.RData")}
#ZeroPorbs are very low

#Defina parameters
NsamplesZIPsig= 150
NtaxaZIPsig = 500
lambdasZIPref = lambdasZIPsig1 =lambdasZIPsig2 = lambdasZIPsig12 =  sample(ZIPmeans, NtaxaZIPsig)
zeroesZIPref = zeroesZIPsig1 = zeroesZIPsig2 = zeroesZIPsig12 =sample(zeroProbs, NtaxaZIPsig)
libSizesZIPsig =c(rep(1e4, floor(NsamplesZIPsig/2)), rep(1e5, floor(NsamplesZIPsig/2)))

#Define the signal
NtaxaSignal1ZIP = 20
NtaxaSignal2ZIP = 20

NtaxaSignal1zeroZIP = 20
NtaxaSignal2zeroZIP = 20

Signal1taxZIP = 4
Signal2taxZIP = 3

Signal1zeroTaxZIP = 5
Signal2zeroTaxZIP = 3

NsamplesSignal1ZIP = 20
NsamplesSignal2ZIP = 20
NsamplesSignal12ZIP = 20

NsamplesSignal1zeroZIP = 20
NsamplesSignal2zeroZIP = 20
NsamplesSignal12zeroZIP = 20

idSig1TaxZIP = 1:NtaxaSignal1ZIP
idSig2TaxZIP = sample(1:NtaxaZIPsig,NtaxaSignal2ZIP)

lambdasZIPsig1[idSig1TaxZIP] = lambdasZIPsig1[idSig1TaxZIP] * Signal1taxZIP
lambdasZIPsig2[idSig2TaxZIP] = lambdasZIPsig2[idSig2TaxZIP] * Signal2taxZIP
lambdasZIPsig12[c(idSig1TaxZIP, idSig2TaxZIP)] = c(lambdasZIPsig1[1:NtaxaSignal1ZIP],lambdasZIPsig2[idSig2TaxZIP])

idSig1TaxZIPzeroes = 1:NtaxaSignal1zeroZIP + NtaxaSignal1ZIP/2
idSig2TaxZIPzeroes = idSig1TaxZIPzeroes + NtaxaSignal1zeroZIP/2 + NtaxaSignal2zeroZIP/2

zeroesZIPsig1[idSig1TaxZIPzeroes] = expit(logit(zeroesZIPsig1[idSig1TaxZIPzeroes]) * Signal1zeroTaxZIP)
zeroesZIPsig2[idSig2TaxZIPzeroes] = expit(logit(zeroesZIPsig2[idSig2TaxZIPzeroes]) * Signal1zeroTaxZIP)
zeroesZIPsig12[c(idSig1TaxZIPzeroes, idSig2TaxZIPzeroes)] = c(zeroesZIPsig1[idSig1TaxZIPzeroes],zeroesZIPsig2[idSig2TaxZIPzeroes])

NsamplesZIPsigRef =  NsamplesZIPsig - NsamplesSignal1ZIP - NsamplesSignal2ZIP-  NsamplesSignal12ZIP

#Define mean and zero matrices
meanMatrefZIP = outer(sample(libSizesZIPsig, NsamplesZIPsigRef), lambdasZIPref)
meanMatSig1ZIP = outer(sample(libSizesZIPsig, NsamplesSignal1ZIP), lambdasZIPsig1)
meanMatSig2ZIP = outer(sample(libSizesZIPsig, NsamplesSignal2ZIP), lambdasZIPsig2)
meanMatSig12ZIP = outer(sample(libSizesZIPsig, NsamplesSignal12ZIP), lambdasZIPsig12)

zeroMatrefZIP = matrix(zeroesZIPref, nrow=nrow(meanMatrefZIP),ncol=ncol(meanMatrefZIP), byrow = TRUE)
zeroMatSig1ZIP = matrix(zeroesZIPsig1, nrow=nrow(meanMatSig1ZIP),ncol=ncol(meanMatSig1ZIP), byrow = TRUE)
zeroMatSig2ZIP = matrix(zeroesZIPsig2, nrow=nrow(meanMatSig2ZIP),ncol=ncol(meanMatSig2ZIP), byrow = TRUE)
zeroMatSig12ZIP = matrix(zeroesZIPsig12, nrow=nrow(meanMatSig12ZIP),ncol=ncol(meanMatSig12ZIP), byrow = TRUE)

#Generate the data
dataMatZIPref = matrix(rzipois(n=prod(dim(meanMatrefZIP)),lambda = meanMatrefZIP, pstr0 = zeroMatrefZIP), ncol=ncol(meanMatrefZIP), nrow=nrow(meanMatrefZIP))
dataMatZIPsig1 = matrix(rzipois(n=prod(dim(meanMatSig1ZIP)),lambda = meanMatSig1ZIP, pstr0 = zeroMatSig1ZIP), ncol=ncol(meanMatSig1ZIP), nrow=nrow(meanMatSig1ZIP))
dataMatZIPsig2 = matrix(rzipois(n=prod(dim(meanMatSig2ZIP)),lambda = meanMatSig2ZIP, pstr0 = zeroMatSig2ZIP), ncol=ncol(meanMatSig2ZIP), nrow=nrow(meanMatSig2ZIP))
dataMatZIPsig12 = matrix(rzipois(n=prod(dim(meanMatSig12ZIP)),lambda = meanMatSig12ZIP, pstr0 = zeroMatSig12ZIP), ncol=ncol(meanMatSig12ZIP), nrow=nrow(meanMatSig12ZIP))

#Bind the data
dataMatZIPSig = rbind(dataMatZIPref, dataMatZIPsig1, dataMatZIPsig2, dataMatZIPsig12)
#mean(dataMatZIPsig==0) #Correct zero fraction

#Save signals
sampleSigZIP = factor(c(rep("Reference",NsamplesZIPsigRef), rep("Signal1", NsamplesSignal1ZIP), rep("Signal2", NsamplesSignal2ZIP), rep("Signal 1 and 2", NsamplesSignal12ZIP)))
taxaSigZIPtmp = rep("Reference",ncol(dataMatSigNB))
taxaSigZIPtmp[idSig1TaxZIP] = "Signal 1"
taxaSigZIPtmp[idSig2TaxZIP] = "Signal 2"
taxaSigZIP =factor(taxaSigZIPtmp) 
names(taxaSigZIP) = colnames(dataMatZIPSig) = names(rhosNBSigref)
names(sampleSigZIP) = rownames(dataMatZIPSig) = 1:NsamplesZIPsig
```

#### Apply the RC(M) algorithm

```{r ZIP signal fit, eval=FALSE, purl=FALSE}
if(!file.exists("syntZIPSig.RData")){
    syntZIPSigUnifJob = mcparallel(RCM(dataMatZIPSig, method = "ZIP",k=2, nleqslv.control = list(trace=TRUE, maxit=250), colWeights = "uniform"))
  syntZIPSigUnif =  mccollect(syntZIPSigUnifJob, FALSE)[[1]] 
  
  syntZIPSigMargJob = mcparallel(RCM(dataMatZIPSig, method = "ZIP",k=2, nleqslv.control = list(trace=TRUE, maxit=250), colWeights = "marginal"))
  syntZIPSigMarg =  mccollect(syntZIPSigMargJob, FALSE)[[1]] 
  
  save(syntZIPSigUnif, syntZIPSigMarg, sampleSigZIP, taxaSigNB, file="syntZIPSig.RData")
} else {load("syntZIPSig.RData")}
```

##### Plot the results

### Zero-inflated negative binomial

#### Without signal

##### Generate data

```{r ZINB without signal, eval=FALSE, purl=FALSE}
load("/home/stijn/PhD/American Gut/AGphylo.RData")
#First estimate the ZIP parameters
expit=function(x){
  tmp = exp(x)/(1+exp(x))
  tmp[is.na(tmp)]=1 #Adjust for overflow
  tmp}

if(!file.exists("AGzinbParams.RData")){
  otuTab = otu_table(AGphylo)@.Data
 # otuTab = otuTab[, colMeans(otuTab==0)<0.95]
  logLibs = log(sample_sums(AGphylo))
  ZINBfits = mclapply(mc.cores=4,1:ncol(otuTab),  function(i){
    try(zeroinfl(otuTab[,i]~offset(logLibs)| 1, dist="negbin"), silent=TRUE)
  })
  zeroProbsNB = sapply(ZINBfits, function(x){if(is.list(x)) expit(x$coef$zero) else NA})
  ZINBmeans = sapply(ZINBfits, function(x){if(is.list(x)) exp(x$coef$count) else NA})
  thetasZINB = sapply(ZINBfits, function(x){if(is.list(x)) x$theta else NA})
  naID=is.na(zeroProbsNB)
  zeroProbsNB= zeroProbsNB[!naID]
  ZINBmeans= ZINBmeans[!is.na(ZINBmeans)]
  thetasZINB= thetasZINB[!is.na(thetasZINB)]
  save(thetasZINB, zeroProbsNB, ZINBmeans, file="AGzinbParams.RData")
} else {load(file="AGzinbParams.RData")}
#ZeroProbs are lower than for the ZIP

#Define parameters
zeroProbsNB = zeroProbsNB[zeroProbsNB<0.9]
NsamplesZINBnoSig= 200
NtaxaZINBnoSig = 800
idSampleZINB = sample(size=NtaxaZINBnoSig, 1:length(ZINBmeans))
MeansZINB = ZINBmeans[idSampleZINB]
ThetasZINB = thetasZINB[idSampleZINB]
zeroesZINBnoSig = sample(zeroProbsNB, NtaxaZINBnoSig)
libSizesZINBnoSig =c(rep(1e5, floor(NsamplesZINBnoSig/2)), rep(1e6, floor(NsamplesZINBnoSig/2)))

#Mean, OD and zero matrices
meanMatZINBnoSig = outer(libSizesZINBnoSig, MeansZINB)
ODmatZINB = outer(rep(1,NsamplesZINBnoSig) ,ThetasZINB )
zeroMatZINBnoSig = matrix(zeroesZINBnoSig, nrow=NsamplesZINBnoSig, ncol=NtaxaZINBnoSig, byrow = TRUE)

#Data generation
dataMatZINBnoSig = matrix(rzinegbin(n=prod(dim(meanMatZINBnoSig)),munb = meanMatZINBnoSig, size = ODmatZINB, pstr0 = zeroMatZINBnoSig), ncol=NtaxaZINBnoSig, nrow=NsamplesZINBnoSig)
save(dataMatZINBnoSig, file = "zinbNoSigData.RData")
```

##### Apply the RC(M) algorithm with the ZINB distribution

```{r ZINB signal fit, eval=FALSE, purl=FALSE}
nleqslv.control.zinb = list(trace = TRUE, maxit = 250, cndtol = .Machine$double.eps, allowSingular=TRUE)
if(!file.exists("syntZINBnoSig.RData")){
    syntZINBSigunifunifJob = mcparallel(RCM(dataMatZINBnoSig, method = "ZINB", k=3, nleqslv.control = nleqslv.control.zinb, colWeights = "uniform", rowWeights = "uniform", twoCores=FALSE, maxItOut = 5e0))
  syntZINBSigunifunif =  mccollect(syntZINBSigunifunifJob, FALSE)[[1]] 
  
  syntZINBSigunifmargJob = mcparallel(RCM(dataMatZINBnoSig, method = "ZINB", k=3, nleqslv.control = nleqslv.control.zinb, colWeights = "marginal", rowWeights = "uniform", twoCores=FALSE))
  syntZINBSigunifmarg =  mccollect(syntZINBSigunifmargJob, FALSE)[[1]]
  
  save(syntZINBSigunifunif, syntZINBSigunifmarg, file="syntZINBnoSig.RData")
} else {load("syntZINBnoSig.RData")}
```

##### Plot the ZINB results

```{r RC(M) ZINB, eval=FALSE, purl=FALSE}
load(file="toyDataNoSigZINBserver.RData")
solListWSZINB = noSigListZINB[sapply(noSigListZINB, class)=="list"]
#Runtimes and convergence
sapply(solListWSZINB,function(x){x$converged})
sapply(solListWSZINB,function(x){x$runtime})
sapply(solListWSZINB,function(x){x$iter})
par(mfrow=c(2,2))
sapply(solListWSZINB,function(x){with(x, plot(psiRec[1,1:iter[1]]))})
```

#### Restore defaults

```{r restore defaults, eval=FALSE, purl = FALSE}
palette(palStore)
```

### Constrained RC(M): tests

Test this method in one dimension, linear response function. Default should probably be: linear for the dummy's of the categorical variables, quadratic or non-parametric for continuous ones.

```{r LRnbchack, purl=FALSE}
#Some synthetic data
n = 40
p = 200
d = 3
var1 = rnorm(n) # an unrelated variable
var2 = rnorm(n) # A meaningful variable
var3 = rnorm(n, sd=2) # An even more meaningful variable
psi = 1
cVec = rnorm(p)*psi
libSizes = 10^runif(n, 3, 5)
abunds = 10^runif(p, -5, -3)
abunds = abunds/sum(abunds)
muMarg = outer(libSizes, abunds)
alphaK = c(0.2,1,1.5)
alphaK = alphaK-mean(alphaK)
alphaK = alphaK/sqrt(sum(alphaK^2))
regMat = rbind(var1, var2,var3)
mu = muMarg * exp(outer(c(alphaK %*% regMat),cVec))
thetasCon = thetas[1:p]
prevCut = 0.1

X0 = matrix(rnbinom(n*p, size=thetasCon, mu = t(mu)), ncol = p, byrow = TRUE)
idCol = colSums(X0)>0 & (colMeans(X0!=0) >prevCut)
X = X0[rowSums(X0)>0,idCol ]
cVec2 = cVec[idCol]
muMargObs = outer(rowSums(X), colSums(X)/sum(X))

testLR = LR_nb(alphaK, X, t(regMat), responseFun ="linear", muMarg = muMargObs, thetas = thetasCon[idCol], cReg = cVec2, d= nrow(regMat), ncols=ncol(X),v=2)
centMat = rbind(1, matrix(0,3,3))

testAlpha = estAlpha(c(0.01,0.01,0.01), X, t(regMat), responseFun ="linear", muMarg = muMargObs, thetas = matrix(thetasCon[idCol],nrow(X),ncol(X), byrow=TRUE), cReg = cVec2, d = nrow(regMat), ncols=ncol(X), alphaK = NULL, centMat = centMat, n=nrow(X), k=1)

#Works really well, only a trifle slow!
testRCMconstrJob = mcparallel(RCM_NB(X = X, k = 1, covariates = t(regMat)))
testRCMconstr = mccollect(testRCMconstrJob, wait=FALSE)[[1]]

save(testRCMconstr,X, regMat, cVec2, alphaK, file="testRCMconstr.RData")
plot(c(testRCMconstr$cMat), cVec2)
plot(testRCMconstr$alpha, alphaK) #Not bad!!
plot(alphaK %*% regMat, c(testRCMconstr$alpha) %*% regMat)
```

Now a bit more complicated: two dimensions, dummy's and quadratice relationship

```{r LRnbchack2, purl=FALSE}
#Some synthetic data
n = 40
p = 200
d = 5
var1 = rnorm(n) # an unrelated variable
var2 = rnorm(n) # A meaningful variable
var3 = rnorm(n, sd=2) # An even more meaningful variable
var4 = factor(sample(c(1,1:3), n, replace=TRUE)) # A categorical variable
var5 = factor(sample(c(1,1,1:3), n, replace=TRUE)) # Another categorical variable
psi = 2
cVec = rnorm(p)*psi
libSizes = 10^runif(n, 3, 5)
abunds = 10^runif(p, -5, -3)
abunds = abunds/sum(abunds)
muMarg = outer(libSizes, abunds)
alphaK = c(0.2,
           1,
           1.5,
           1.2,0.9,-3.1,
           1.1,-2.5,1.4) #The categorical variables with zero-sum coding (they center around zero by default. Remember to estimate with treatment coding but normalize based on zero-sum coding.)
alphaK = alphaK-mean(alphaK)
alphaK = alphaK/sqrt(sum(alphaK^2))
regMat = t(
  cbind(model.matrix(~var1+var2+var3+var4+var5-1, contrasts.arg = lapply(list(var4 = var4,var5 = var5), contrasts, contrasts=FALSE))
                  ))

mu = muMarg * exp(outer(c(alphaK %*% regMat),cVec))
thetasCon = thetas[1:p]
prevCut = 0.1

X0 = matrix(rnbinom(n*p, size=thetasCon, mu = t(mu)), ncol = p, byrow = TRUE)
idCol = colSums(X0)>0 & (colMeans(X0!=0) >prevCut)
X = X0[rowSums(X0)>0,idCol ]
cVec2 = cVec[idCol]
muMargObs = outer(rowSums(X), colSums(X)/sum(X))
#centMat

testAlpha2 = estAlpha(rep(0.01,9), X, t(regMat), responseFun ="gaussian", muMarg = muMargObs, thetas = thetasCon[idCol], cReg = cVec2, d = nrow(regMat), ncols=ncol(X), alphaK = NULL, centMat = centMat, k=1)

testRCMconstr2Job = mcparallel(
  RCM(dat = X, k = 2, covariates = data.frame(var1 = var1, var2 = var2, var3 = var3, var4 = var4, var5 = var5), responseFun = "quadratic")
  )
testRCMconstr2 = mccollect(testRCMconstr2Job, wait=FALSE)[[1]]
```

### Triplots

### Real data application

```{r realDataRCM, purl = FALSE}
load("/home/stijn/PhD/Simulations/data/physeqListV13.RData")
antNarConstr = RCM(physeqListV13[["Anterior.nares"]], k=2, covariates  = c("sex", "RUNCENTER"))
load("/home/stijn/PhD/Simulations/data/zellerData.RData")
zellerConstr = RCM(zellerMphy, k=2, covariates  = c("Diagnosis", "BMI", "Age","Gender","Country"), round=TRUE)
```

# Method evaluation

After implementing the method and proving its concept, it is time to compare it with existing methodologies. We need to define a way to obtain datasets, and a way to evaluate the ordinations in a high-throughput way.

## Unconstrained RC(M)

### Generating datasets

As ways of generating test datasets we see three different options

  1) Parametric simulation with the negative binomial with known abundances. 
  2) Non-parametric simulation with SimSeq
  3) Real datasets with biological signal allegedly known
  
  Parametric simulation is needed as a first check but evidently favours our method (if we use the negative binomial distribution). SimSeq is a reasonably neutral tool, we will only use WMW to obtain the lfdr. Real datasets will need to complement our analysis but it may not be possible to evaluate them in a high-throughput way, this will need to happen empirically.
  
#### Parametric simulations

##### Negative binomial

The negative binomial is the assumed distribution of our method. As a result we should outperform all other methods in this setting.

###### Simulate from different distributions

In this setting we will simulate data from 4 unrelated negative binomial distributions. Because of this clustering should be very good.

```{r NBunrelatedDistr, purl = FALSE}
p = 1000 #Number of taxa
n = 60 #Number of samples
nPop = 4 #Number of underlying populations
nPerPop = n/nPop #Number of samples per population
reps = 100 #replicates
nDim  = 3 #Number of dimensions

load("/home/stijn/PhD/Simulations/data/MLES.RData") #Load observed parameter values
rhoEst = unlist(rhoMLEs) #observed relative abundances, pooled
thetaEst = 1/unlist(phiMLEs) #observed dispersions abundances, pooled

groupInd = rep(1:nPop, each = nPerPop) #Group indicator, as integers
groupFactor = factor(groupInd, levels = 1:nPop, labels =  paste("Population", 1:nPop), ordered = TRUE) #as factor

#Library sizes
load("/home/stijn/PhD/Simulations/data/physeqListV13.RData")
libSizesEst = unlist(sapply(physeqListV13, sample_sums)) #Resample

if(!file.exists(file = "simRCMunconstr1.RData")){
datList1 = mclapply(mc.cores=4, 1:reps, function(i){
## Sample parameters
rhosSampled = sapply(1:nPop, function(x){sample(rhoEst, p)})[,groupInd]
thetasSampled = sample(thetaEst, p) #Sampled dispersions are the same in every population, this is the ideal case assumed by our method.
libSizesSampled = sample(libSizesEst, n) #Sampled libSizes

## Create dataset
meanMat = libSizesSampled * t(rhosSampled)
dataMat = matrix(rnbinom(prod(dim(meanMat)), mu = meanMat, size = matrix(thetasSampled,n,p)),n,p)
list(dataMat = dataMat, group = groupFactor, libSizesSampled = libSizesSampled, thetasSampled = thetasSampled, rhosSampled = rhosSampled)
})

RCMresList1 = mclapply(mc.cores=4, datList1, function(y){
  RCM(y$dataMat, k = nDim, distribution = "NB")
})
save(RCMresList1, groupFactor, file = "simRCMunconstr1.RData")
} else {load(file = "simRCMunconstr1.RData")}
```
  
### Evaluating the biplot

For the simulation we will need multiple repetitions to prove the superiority of our method, so we need an automatic evaluation of the clustering accuracy.

#### Within vs. overall distance

A first intuitive approach would be to compare the within cluster distances on the plot to the distances to the origin. Clusters are defined as samples generated with the same taxon distribution, either parametrically or non-parametrically. Say we have $m = 1,...,l$ groups with (multidimensional) means $\mathbf{M}_m$.  The within distance is then defined as $d_{m, within} = \frac{1}{n_m}\sum_{i=1}^{n_m} d_{euclidean}(\mathbf{M}_m, Y_i)$, with $d_{euclidean}$ the euclidean distance. The overall distance of a group then equals $d_{m, overall} = \frac{1}{n_m}\sum_{i=1}^{n_m} d_{euclidean}(\mathbf{0}, Y_i)$ with $\mathbf{0}$ the origin. This calculation can be done for each dimension separately or in the multidimensional space.

We could perhaps look for the distance based method that minimizes $\frac{d_{m, within}}{d_{m, overall}}$?

This approach can be applied to the row scores (samples) as well as the column scores (taxa).

#### Correlation with library sizes

The motivating problem to develop the whole method was to find an approach that would not show correlation between the row scores and the library sizes, which is a technical artefact.

(Absolute values of) Pearson and/or Spearman correlations of row scores with library sizes could be another criterion to evaluate the quality of the biplot.

<!---
The between distance between groups m and m' is then defined as $d_{mm'} = d_{euclidean}(\mathbf{M}_m, \mathbf{M}_{m'})$ with $d_{euclidean}$ the euclidean distance.
--->

## Constrained RC(M)

# Competing methods

## Unconstrained RC(M)

 - PCoA
   * Bray-Curtis dissimilarity (Forest ecology)
   * UniFrac distance (Microbial ecology, uses phylogenetic information)
   * See Warton _et al._ (2012): Distance-based multivariate analyses confound
location and dispersion effects
 - Corresponence analysis (CA) (Statistics)
 - CoDa: PCoA of log-ratio transformed data (Gloor _et al._ (2016), Gloor and Reid(2016)
  * Dependence on pseudocounts?
 - Bayesian Nonparametric Ordination (Ren _et al._ (2016))
   
## Constrained

  - Constrained PCoA?
  - Constrained correspondence analysis (CCA)
    * Sparse canonical correlation analysis (ssCCA) (Chen et al. 2013)

\newpage

#### Some comments on analysing microbiome count data

##### Do not log-transform count data

- log-transforms come with arbitrary pseudocounts for the many zeroes
- models based on transformed data are hard to interpret in funcion of the untransformed data
- log-transformation does not stabilize the variance
- Accept non-normality and heteroscedasticity instead of trying to transform them away!
- Log-transformation is more robust though and provides better type I error control (see papers O'Hara 2010, Ives 2015, Warton 2016 and this [post](http://stats.stackexchange.com/questions/114848/negative-binomial-glm-vs-log-transforming-for-count-data-increased-type-i-erro?rq=1) )

##### Do not rely on residuals-based approaches

- Most residual based approaches rely on normally behaved residuals
- Microbiome residuals are skewed to the right 
- The expectations should be larger than or equal to 5 for this approximation to hold, which is not the case for the majority of microbiome entries

##### Use a GLM

- Normalizing to relative or rarefied abundances throws away information on the variance and valuable counts
- Leave the data untransformed and use a proper GLM for counts! This will properly model mean and variance and allows incorporation of covariate information