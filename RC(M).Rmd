---
title: "RC(M)"
author: "Stijn"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---

```{r check-install-load-packages, warning=FALSE, message=FALSE, echo=FALSE, purl=TRUE}
WD = "/home/stijn/PhD/Biplots"
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE,  warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE, tidy = TRUE, fig.width=11, fig.height=8, root.dir=WD, purl=FALSE)
setwd(WD)
# The required package list:
reqpkg <- c("phyloseq","MASS", "parallel","nleqslv", "edgeR", "VGAM", "HMP", "ggplot2", "pscl")
# Load all required packages and show version
for(i in reqpkg)
{
#   print(i) 
#   print(packageVersion(i))
  library(i, quietly=TRUE, verbose=FALSE, warn.conflicts=FALSE, character.only=TRUE)
} 
if(file.exists("/home/stijn/PhD/American Gut/AGpars.RData")) {load("/home/stijn/PhD/American Gut/AGpars.RData")}
par(pty="s") #Make sure the biplots are square!
palStore = palette()
```

# Introduction

The aim of this notebook is to wrap the codes to fit the RC(M) model with the negative binomial, zero-inflated poisson and zero-inflated negative binomial models. Also an overview of the theory is given here.

# Theory

## Correspondence analysis

Suppose we have a $nxp$ count data matrix $\mathbf{X}$ with $n$ samples and $p$ taxa, with $i$ and $j$ row respectively column indices.

### Independence model

Under independence between rows and columns we model the counts in a contingency table as 

$$E(X_{ij}) = a_i b_j$$

whereby usually $a_i=x_{i.}=\sum_{j=1}^px_{ij}$ (the library sizes) and $b_j=\frac{x_{.j}}{x_{..}}=\frac{\sum_{i=1}^nx_{ij}}{\sum_{j=1}^p\sum_{i=1}^nx_{ij}}$ (the average relative abundances).

### Reconsitution formula of Correspondence Analysis (CA)

A more extended model is

$$E(X_{ij}) = a_i  b_j + c_i d_j \sum_{k=1}^K \omega_k v_{ki} w_{jk}$$.

For $a_i = c_i = x_{i.}$ and $b_j = d_j = \frac{x_{.j}}{x_{..}}$ this is regular correspondence analysis, usually with truncation at $K=2$. The second term in the equation represents deviation from independence. This is called the *reconstitution formula* since it decomposes the observed average count into its expectation under independence and a residual. The residual is then further decomposed into $k$ pieces. The expected count can then be written as

$$E(X_{ij}) = \frac{x_{i.}x_{.j}}{x_{..}} \big(1 + \sum_{k=1}^K \omega_k v_{ki} w_{jk}\big)$$.

Regular corrsepondence analysis is done through singular value decomposition(SVD) of the matrix of weighted (Pearson) residuals

$$R^{-1/2}(X-E)C^{-1/2}=U\Sigma V$$

with $\Sigma$ a diagonal matrix and $R$ and $C$ diagonal matrices with row and column sums.

In matrix notation the reconstitution formula becomes

$$X = E_{independence} + RU\Sigma VC$$

with $\mathbf{1'}R^{1/2}U=\mathbf{0}$ and $\mathbf{1'}C^{1/2}V=\mathbf{0}$ (weighted means equal zero) and $U'R^{1/2}U=\mathbf{1}$ and $V'C^{1/2}V=\mathbf{1}$ (weighted variances equal one) (see VanDerHeijden et al., 1985).

### Skewness

Because of the skewness of the distribution, the residuals are also very skewed. If we have an expectation of around 1, it is very likely to observe a zero, but much less likely to observe a 2. Still both deviations are weighted equally. I think a residual-based approach is ill suited to asymmetric, skewed data. Also the library size correction will never be appropriate, so that part of the biplot will reflect differences in library sizes. We considered decomposing

$$R^{-1}(X-E)C^{-1}$$

in the past to account for the fact that the data are not Poisson ($Var(X) = E(X)$) but rather negative binomial ($Var(X) = E(X) + \phi *E(X)^2$) and thus the variance for large means proportional to the squared expectation. Even more general we considered.

$$R^{-a}(X-E)C^{-b}$$

and estimate $a$ and $b$ by maximum likelihood but we did not continue with this idea.

## Hellinger distance

With PCoA, any distance metric can be used to produce a distance matrix, which can be decomposed to make biplots.

The Hellinger distance divides by the square root of the observation as means of normalization:

$$\frac{X}{\sqrt{Var(X)}} \approx \frac{X}{\sqrt{X}} = \sqrt{X}$$

The rationale is that each observation x is the best estimate of its expectation E(X), and under the Poisson model also of its variance. Still also these plots are susceptible to differences in library sizes.

## Log-linear analysis

Another modelling approach is to use log-linear modelling an thereby introduce the negative binomial as error term. We know from previous goodness of fit testing that this is an appropriate error model for microbiome data.

In log-linear analysis the logged expected count $l_{ij}$ is modelled as

$$log(E(x_{ij})) = l_{ij} = u + u_i + u_j + u_{ij}$$.

For $u_{ij} \neq 0$ this is the saturated model, which provides a perfect fit. If $u_{ij} = 0$ this is the independence model presented above.

### Relationship between CA and log-linear analysis

According to Escoufier, 1985 if $a =\sum_{k=1}^K \omega_k v_{ki} w_{jk}$ is small (i.e. the deviation from independence is small) then $log(1+a) \approx a$ and

$$log(E(x_{ij})) = log(x_{i.}) + log(x_{.j}) - log(x_{..}) + log\big(1 + \sum_{k=1}^K \omega_k v_{ki} w_{jk}\big) \approx log(x_{i.}) + log(x_{.j}) - log(x_{..}) + \sum_{k=1}^K \omega_k v_{ki} w_{jk}$$

Since the same restricitions apply to the scores $v_{ki}$ and $w_{jk}$ as to U and V, we can state that  $\psi_k \approx \omega_k$.

### The RC(2)-association model

For the purpose of making biplots, we might consider the following model (proposed by Leo Goodman, 1985. See also VanDerHeijden 1994)

$$log(E(x_{ij})) = l_{ij} = u + u_i + u_j + \psi_1r_{1i}s_{1j} + \psi_2r_{2i}s_{2j}$$.

Constraints are needed to render this model identifiable, Goodman proposes

Centering:

$$\sum_{i=1}^nx_{i.}r_{ki} = 0$$

with k=1,2 and

Normalization($k=k'$) and orthogonality ($k \neq k'$)

$$\sum_{i=1}^nx_{i.}r_{ki}r_{k'i} = I(k=k')$$

$$\sum_{j=1}^px_{.j}s_{kj} = 0$$

$$\sum_{j=1}^px_{.j}s_{kj}s_{k'j} = I(k=k')$$

Goodman thus uses _weighted_ constraints, to retain the relationship with correspondence analysis.

However, this allows the scores of samples with small library sizes or taxa with small abundances to grow larger than the others, which is not what we want (see RC2NB.Rmd). Therefor, we choose

$$\sum_{i=1}^nr_{ki} = 0$$

with k=1,2 and

$$\sum_{i=1}^nr_{ki}r_{k'i} = I(k=k')$$

$$\sum_{j=1}^ps_{kj} = 0$$

$$\sum_{j=1}^ps_{kj}s_{k'j}  = I(k=k')$$

i.e. weighted constraints for the column scores and unweighted for the row scores. The idea was that the relative abundances are biologically meaningful and should have an impact on the fitting procedures, whereas this is not true for the library sizes, which are assumed to be technical artefacts. 

__Added__: but actually, does it make sense that the _weighted_ scores are orthogonal, if we fit and plot the unweighted scores in the end? From our results it seems like a good idea to keep the normalization weighted, but what about the centering? We do have a problem in our first dimension currently ... __Update__: actually the weighted centering for the column scores is what avoids the outliers. Do we need other ways ways to restric thte scores?

The models are usually fitted by ML. See page 55 of Goodman 1985 for details. Initial values can be obtained through a singular value decomposition. In our case we use the singular value decomposition of

$$R^{-1}(X-E)C^{-1}$$

so that the initial scores already obey the centering restrictions above.

Goodman assumed a Poisson distribution, but it might be possible to use a negative binomial model with unique dispersions for each taxon $j$ as well.

We will use $u=-log(x_{..}$, $u_i = log(x_{i.})$ and $u_j = log(x{.j})$, and then we'll have to iterate between fitting the overdispersions, the imporance parameters $\psi$, the $r's$ and the $s's$. 

The centering, normalization and orthogonality of the row and column scores is enforced through Lagrange multipliers. This makes the systems of equations much harder to solve but assures independence of the dimensions.

Note that this is a case of __indirect gradient__ analysis: the dimensions are estimated without incorporating gradient information (measured covariates) to maximally represent variability in the data. In a next step these obtained gradients can be compared with measured covariates (often graphically through a colour code) to see how well these covariates explain the gradients. 

## Fitting algorithm for the RC(2) association model with a NB error structure

See Goodman 1985 (appendix), Becker 1988 for information on estimation. The code below is generic but we take k=2 for simplicity.

Parameters of interest to estimate are $\psi$, $r_1$, $r_2$, $c_1$, $c_2$. In the process we'll also need the nuisance parameters $\mathbf{\theta}$. Assume $E(X_{ij}) = \mu_{ij}$ and  $Var(X_{ij})=\mu_{ij} + \frac{\mu_{ij}^2}{\theta}$

1. Obtain a singular value decomposition as $(X-E) = U\Sigma V$. This gives us initial values $[r_{11}^{init}, r_{12}^{init},..., r_{1i}^{init}] = U_{(1.)}$, the first row of $U$, and correspondingly $\Sigma$ and $V$ give initial values for $\mathbf{\psi}$ and $\mathbf{s}$.

The scores have  means equal to zero already so that

$$\sum_{i=1}^nr_{ki} = 0$$

and

$$\sum_{j=1}^ps_{kj} = 0$$.

We still need to ensure that the variances equal 1, so we set

$$r_{ki}^{init} = \big(\frac{r_{ki}^{SVD}}{\sum_{i=1}^n{r^{SVD}_{ki}}^2}\big)^{1/2}$$

and

$$s_{ik}^{init} = \big(\frac{s_{ik}^{SVD}}{\sum_{i=1}^n{s^{SVD}_{ik}}^2}\big)^{1/2}$$

2. Estimate overdispersions

Theoretically we might estimate the dispersions by ML, but the estimates are very variable, which dramatically affects the rest of the fitting procedure.

 - Maximum likelihood estimation (not used) 
 
$$log(E(x_{ij})) = l_{ij} = offset\big(log(\frac{x_{i.}x_{.j}}{x_{..}}  \big) + \psi_1 (s_{1i}^{init}r_{1j}^{init}) + \psi_2 (s_{2i}^{init}r_{2j}^{init})\big)$$

to obtain estimates for the overdispersions $\theta_j$, whereby the means $\mu_{ij}$ are assumed known

 $$\sum_{i=1}^n \sum^{y_i^*}_{t=0} \big( \frac{t}{1+\frac{t}{\theta_j} } \big) + \theta_j^2 log(1+\frac{\mu_{ij}}{\theta_j})- \mu_{ij}\frac{y_{ij}+\theta_j}{1+\frac{\mu_{ij}}{\theta_j} }$$
 
 For this we can use the theta.ml() function in the _MASS_ package
 
 - edgeR robust
 
To get more stable estimates of the dispersions, RNA-seq packages such as _DESeq_ and _edgeR_ use an empirical Bayes approach to shrink dispersion estimates towards a fitted mean-dispersion trend. Here I use the machinery of edgeR robust (version 3.14.0) to estimate the dispersions
 
To reduce the computational cost the estimation of the overdispersions is not repeated in every iteration

3. Estimate the psi parameters
 
 Solve the system of score equations (see Lawless 1987) for $\psi_1$ and $\psi_2$, assuming $\theta_j$'s given
 
 $$\sum_{i=1}^n \sum_{j=1}^p r_{im}s_{mj} \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}} = 0$$ for all m=1,...,k
 
 with
 
 $$E(X_{ij}) = \mu_{ij} = exp\big(log(\frac{x_{i.}x{.j}}{x_{..}}) + \psi_1^{MLE} s_{1j}^{init}r_{1i}^{init} + \psi_2^{MLE} s_{2j}^{init}r_{2i}^{init} \big)$$.
 
 These are non-linear equations for which we use the _nleqslv_ package, with its default the Broyden method. This is a variation of the Newton-Raphson method (or Fisher scoring in case of ML) whereby the Jacobian is recycled and not recalculated at every iteration. The Jacobian is a square matrix of dimension k*(k-1)/2. The diagonal entry at position $kk'$ is:
 
 $$\frac{\delta^2 L_{NB}(\mu, \theta)}{\delta \psi_k \delta \psi_{k'}} = -\sum_{i=1}^n \sum_{j=1}^p r_{ik}r_{ik'}s_{kj}s_{k'j}\frac{\mu_{ij}(1+\frac{x_{ij}}{\theta_j})}{(1+\frac{\mu_{ij}}{\theta_j})^2}$$
 
4. To estimate the $r_{i}$'s we would really like to use weighted least squares to minimize

$$\sum_{j=1}^p\Big(log(x_{ij}) - \big(u + u_i + u_j +\sum_{k=1}^M r_{ik} \big( \hat{\psi_ks_{jk}} \big)\Big)^2$$

for every $i$. Unfortunately we cannot calculate $log(x_{ij})$ because of the zeroes.

We'll have to use another way to estimate them, maximum likelihood in our case. I don't like using weighted least squares on the non-transformed counts because of the skewness of the residuals as explained above.

To enforce the constraints on the scores mentioned above we use Lagrange multipliers and thus look for the maximum of the Lagrangian function

$$Lag(r_{ik}, \mathbf{\lambda}) = L_{NB}(\mathbf{r}) + \sum_{k=1}^M \big( \lambda_{1k} \sum_{i=1}^n r_{ik} \big) + \sum_{k=1}^M \big( \lambda_{2k} ( \sum_{i=1}^n r_{ik}^2 ) - 1 \big) + \sum_{k \neq k'} \big(\lambda_{3kk'} \sum_{i=1}^n (r_{ik}r_{ik'}) \big)$$

with $L_{NB}(\mathbf{r})$ the log-likelihood function of the negative binomial regression. The derivatives of this function are

$$\frac{\delta Lag(r_{ik}, \mathbf{\lambda})}{\delta r_{ik}} = \sum_{j=1}^p \hat{s}_{jk} \hat{\psi}_k \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}} + \sum_{k=1}^M \lambda_{1k}  + \sum_{k=1}^M \big(2 \lambda_{2k} r_{ik}\big) +  \sum_{k' \neq k} r_{ki} \lambda_{kk'} = 0$$

and 

$$\frac{\delta Lag(r_{ik}, \mathbf{\lambda})}{\delta \lambda_{1k}} = \sum_{i=1}^n r_{ik} = 0$$

and 

$$\frac{\delta Lag(r_{ik}, \mathbf{\lambda})}{\delta \lambda_{2k}} = (\sum_{i=1}^n r_{ik}^2) - 1 = 0$$

and 

$$\frac{\delta Lag(r_{ik}, \mathbf{\lambda}))}{\delta \lambda_{3kk'}} = (\sum_{i=1}^n r_{ik}r_{ik'}) - 1 = 0$$

. The latter three groups of equations represent the original centering, normalization and orthogonality constraints.

There are additional equations in the system now, but also as many extra $\lambda$ parameter to optimize. These $\lambda$ parameters are also stored and used as initial values in the next iteration run.

We now truly have a system of equations to solve, which we do with the _nleqslv_ package. It is straightforward but a bit cumbersome to also provide the analytical Jacobian, which greatly speeds up the iterations (otherwise it has to be approximated numerically). The Jacobian is sparse and symmetric.

 $$\frac{\delta^2 Lag(\mu, \theta)}{ \delta r_{ik} \delta r_{ik'}} = -\sum_{i=1}^n \sum_{j=1}^p \psi_{k}\psi_{k'}s_{kj}s_{k'j}\frac{\mu_{ij}(1+\frac{x_{ij}}{\theta_j})}{(1+\frac{\mu_{ij}}{\theta_j})^2}=0$$
 
  $$\frac{\delta^2 Lag(\mu, \theta)}{\delta r_{ik} \delta \lambda_{1k}} = 1$$
  
  $$\frac{\delta^2 Lag(\mu, \theta)}{\delta r_{ik} \delta \lambda_{2k}} = 2r_{ik}$$
  
  $$\frac{\delta^2 Lag(\mu, \theta)}{\delta r_{ik} \delta \lambda_{3kk'}} = r_{ik'}$$
  
  All other entries are zero.
  
5. Repeat step 4 but now estimate $s_{jk}$ column scores in the restrictions, e.g. the Lagrangian becomes:

$$Lag(s_{jk}, \mathbf{\lambda}) = L_{NB}(s_{jk}) + \sum_{k=1}^M \big( \lambda_{1k} \sum_{j=1}^p s_{jk} \big) + \sum_{k=1}^M \big( \lambda_{2k}  ( \sum_{j=1}^p s_{jk}^2 ) - 1 \big) + \sum_{k \neq k'} \big(\lambda_{3kk'} \sum_{j=1}^p (s_{jk}s_{jk'}) - 1\big)$$

6. Repeat steps 2-5 until convergence. Convergence is assumed if between two iterations

 - The $\psi$ parameters change less than $0.01\%$
 
 AND
 
 - None of the square roots of the L2-norms of the relative changes of row and column scores exceeds 0.1%, or
 
 $$\forall \mathbf{r_k}, \mathbf{s_k} \Big(\sqrt{\sum_{i=1}^n (1-\frac{r^{(iter-1)}_{ik}}{r^{(iter)}_{ik}})^2} < 0.001 \Big) \cup \Big( \sqrt{\sum_{j=1}^p (1-\frac{s^{(iter-1)}_{kj}}{s^{(iter)}_{kj}})^2} < 0.001 \Big)$$
 
 and analogously for all the scores. Other norms, such as L1 or infinity norms can also be used. A norm between 1 and 2 will lead to fast convergence.
 
 Overdispersions are not taken into account in determining the convergence since they're merely nuisance parameters.

8. Plot $\psi_1r_{1i}$ vs $\psi_2r_{2i}$ and add $s_{1j}$ vs $s_{2j}$ as arrows to make a biplot.

In the end we'll have estimated p (dispersions) + kxp (column scores) + kxn (row scores) + k (importance parameters) = (k+1)p + kxn + k parameters out of np entries. We have imposed 4k + k(k-1) restrictions, so the final model is still very parsimonious for n and p in the hundreds.

9. Assess the goodness of fit

Since the model is overparametrized, classical ML theory (such as asymptotic behavious of maximum likelihood statistics) does not apply to our solution. Still we can compare the likelihoods of the independence, the RC(2) and the intermediate models to get an idea of the relative importance of the dimensions. The same concept exists for correspondence analysis, where the size of the diagonal elements of $\Sigma$ is proportional to the importance of the corresponding dimension. The log-likelihood of the saturated model is by definition equal to 0.

We can decompose the difference in log-likelihood between the independence and the full RC(K) model as

$$(LL_2 - LL_0) = (LL_1 - LL_0) + (LL_2 - LL_1)$$

with $LL_0$, $LL_1$ and $LL_2$ -2 the log-likelihoods of the independence, RC(1) and RC(2) models respectively. Scaling by $(LL_2 - LL_0)$ will provide interpretable fractions.

# 1B1

Initially all dimension were estimated jointly, but for reasons of sped we moved to a one-by-one approach estimating first the first dimension, then the second given the first, then the third given the first two and so on. By splitting the estimation procedures into smaller parts like this the estimation speeds up considerably, presumably because the systems are easier to solve.

# Zero-inflated poisson

The error distribution is now no longer negative binomial but zero-inflated Poisson (ZIP). The chance on a structural zero is modelled as:

$$logit(P(X_{ij}=0)) = f_j + t_{i1}v_{j1}\chi_1 + t_{i2}v_{j2}\chi_2$$.

The probability of a strucutral zero does not depend on the library size: strucutural zeroes are assumed to be truly absent species and not due to undersampling. Each taxon then has a base logit probability on a structural zero of $f_j$ and the terms $t_{i1}v_{j1}\chi_1 + t_{i2}v_{j2}\chi_2$ indicate in which two independent directions the observed number of zeroes deviates from this base level.

## Restrictions

The same restrictions apply to the scores $t_i$ and $v_j$ as to $r_i$ and $s_j$

## Estimation (see Lambert 1992 for details)

The mean and zero probability are modelled independently, absence of a species is independent of its abundance when it is present. Say $Z=0$ when $X$ is from the Poisson state and $Z=1$ when X is a structural zero. Evidently, when $X=0$, Z is unknown.

EM algorithm: iterate between 

 - E: estimate Z, assuming  $\mathbf{r_i}$, $\mathbf{s_j}$ and $\mathbf{\Psi}$ known, through its expectation
 - M: Maximize the log-likelihood given Z
 
Unlike the RC(2)NB model, $f_j$ has to be estimated as well, there is no obvious candidate here. We estimate it marginally and then keep it fixed during the iterations. This way the score visualize departures in zero inflation from this independence of zero-inflation between rows and columns.

Newton-Raphson would be incredibly complicated here, the derivatives of the log-likelihood functions barely fit on a page!

# Implementation

## Correspondence analysis

```{r Auxfuns Correspondence analysis, purl=TRUE}
## A function to perform correspondence analysis

caSVD = function(X){
  
# @param X: the nxp count matrix
  
# @return: the singular value decomposition of the matrix of pearson residuals
  
  C = colSums(X)
  R = rowSums(X)
  onesn =rep.int(1, nrow(X))
  onesp = rep.int(1, ncol(X))
  E = diag(R) %*% outer(onesn ,onesp) %*% diag(C)/sum(C)
  # Goal = diag(1/sqrt(R)) %*% (X-E) %*% diag(1/sqrt(C))
  #Goal = diag(1/sqrt(R)) %*% (X-E) %*% diag(sqrt(sum(C)/C))
  #Goal = diag(1/R) %*% (X-E) %*% diag(1/C)
  Goal = diag(1/sqrt(R)) %*% (X-E) %*% diag(1/sqrt(C))
  dimnames(Goal) = dimnames(X)
  svd(Goal)
}
#-------------------------------------------------#
```

## Negative binomial

```{r Auxfuns, purl=TRUE}
#All matrices X are considered to be nxp, i.e. samples are rows and taxa are columns

#--------------------------------------#

dNBpsis = function(beta, X, reg, theta,  k, muMarg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param rMat: a nxk matrix with row scores
  # @param cMat: a kxp matrix with column scores
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes : a vector of length n with library sizes

  # @return A vector of length r with the new psi estimates

  mu = exp(arrayprod(reg, beta)) * muMarg 
  
  vapply(1:k,FUN.VALUE=0, function(z){
  sum(reg[,,z]*(X-mu)/(1+t(t(mu)/theta)))
  })

}
#--------------------------------------#
#A jacobian for the psi parameters
NBjacobianPsi = function(beta, X, reg, muMarg, theta, k){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
    mu = exp(arrayprod(reg, beta)) * muMarg
  sapply(1:k, function(K){
    vapply(1:k,FUN.VALUE=0, function(Kinner){
    -sum(reg[,,K]*reg[,,Kinner]*(1+t(t(X)/theta))*mu/(1+t(t(mu)/theta))^2)
  })})
}

#---------------------------------------#
# A function to estimate the overdispersions
estDisp = function(X, cMat, rMat, muMarg, psis, prior.df=10, dispWeights=NULL){
# @param X: the nxp named count matrix
# @param rMat: A nxk matrix with the row weights of every dimension in the columns 
# @param cMat: A kxp matrix with the column weights of every dimension in the rows 
# @param libSizes: a vector of length n with library sizes
# @param abunds: a vector of length p with the abundances
# @param psis: A vector of length k with importance estimates
# @param prior.df (optional): prior degrees of freedom for dispersion estimation, see edgeR documentation

  require(edgeR)
    # A matrix of means
  logMeansMat = t(rMat %*% (cMat*psis) + log(muMarg))
#Use the edgeR machinery to estimate the dispersions
  trended.dispersion  <- estimateGLMTrendedDisp(y = t(X), design = NULL, method = "bin.loess",offset=logMeansMat, weights=dispWeights)
  thetaEsts <- 1/estimateGLMTagwiseDisp(y = t(X), design = NULL, prior.df = prior.df, offset=logMeansMat, dispersion = trended.dispersion, weights=dispWeights)
  if(anyNA(thetaEsts)){
    idNA = is.na(thetaEsts)
    thetaEsts[idNA] = mean(thetaEsts[!idNA])
    warning(paste(sum(idNA), "dispersion estimations did not converge!"))
  }
  return(thetas=thetaEsts)
}
#---------------------------------------#

#An auxiliary R function, kindly provided by Joris, to "matrix-multiply" an array by a vector
arrayprod <- function(x,y){
  xdim <- dim(x)
  if(length(xdim)==2) {return(x*y)}
  outdim <- xdim[1:2]
  outn <- prod(outdim)
 
  yexpand <- rep(y, each = outn)
  outid <- seq_len(outn)
 
  tmp <- x * yexpand
 
  dim(tmp) <- c(outn, xdim[3])
  out <- rowSums(tmp)
 
  dim(out) <- outdim
 
  out
}

#--------------------------------------#
#A score function of the NB for the column scores

dNBllcol = function(beta, X, reg, thetas, muMarg, k, p, n, colWeights, nLambda) {
  # @param beta: a vector of k*p column scores + k within dimensional centering lambda parameters + k within dimensional normalization parameters +  k(k-1)/2 between dimension lambda parameters
  # @param y: the data vector of length n
  # @param reg: a nxk regressor matrix: outer product of rowScores and psis
  # @param thetas: a scalar,  the current dispersion parameters
  # @param logAbund: a scalar with the LOG OF THE abundance parameter
  # @param libSizes : a vector of length n with (known) library sizes
  # @param k: a scalar, the dimension of the RC solution
  # @param p: a scalar, the number of taxa
  # @param n: a scalar, the number of samples

  # @return A vector of length p*k+2*k+ k(k-1)/2 evaluations of the lagrangian
  
  cMat = matrix(beta[1:(k*p)], byrow=TRUE, ncol=p, nrow=k)
  # logMu[,K] = reg[,K]*beta
  mu = exp(reg %*% cMat) * muMarg
  
  lambda1 = beta[(k*p+1):(k*p+k)] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[(k*p+k+1):(k*p+2*k)] #normalization restrictions sum(abunds*r^2_{ik}) = 1
    if(k==1) {lambda3Mat=0
  } else {
  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(p+2)+1):length(beta)]
  }
#   score = as.vector(sapply(1:k, function(K){
#     sapply(1:p, function(P){
#       sum(reg[,K]*(y[,P]-mu[,P])/(1+t(t(mu[,P])/thetas[P])))  + 
#             sum(lambda3Mat[K,]*cMat[,P]*abunds[P]) + lambda1[K]*abunds[P] + lambda2[K]*abunds[P]*
#         if(LASSO) ifelse(cMat[K,P]>0,1,-1) else 2*cMat[K,P]*abunds[P]
#           })})) 
  
  score = c(t(
    crossprod(reg,((X-mu)/(1+t(t(mu)/thetas)))) + 
                        t(colWeights*t(lambda1 + lambda2*2*cMat + (lambda3Mat %*% cMat)))
    ))
  
  centers = colSums(colWeights*t(cMat))
  unitSums = colSums(colWeights*t(cMat^2))-1
  if(k==1) return(c(score,centers, unitSums))
  orthogons = unlist(sapply(1:(k-1), function(K){
    vapply((K+1):k,FUN.VALUE=0, function(Kinner){
      sum(cMat[K,]*cMat[Kinner,]*colWeights)
    })
  }))

  return(c(score,centers, unitSums, orthogons))
}

#--------------------------------------#

# A function to provide an analytical jacobian in the hope to speed up the calculations
NBjacobianCol = function(beta, X, reg, thetas, muMarg, k, n ,p, colWeights, nLambda){
  #@return a symmetric jacobian matrix of size p*k + k(k-1)/2
  cMat = matrix(beta[1:(k*p)], byrow=TRUE, nrow=k, ncol=p)
  lambda3Mat = matrix(0,ncol=k, nrow=k) #Reorder to lower triangular matrix
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(p+2)+1):length(beta)]

  #Calculate the mean
  mu = exp(reg %*% cMat)* muMarg
  
  Jac = matrix(0, nrow= p*k + nLambda, ncol=p*k + nLambda)
  #The suymmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle
#   
  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:(p*k),(p*k+1):((p+1)*k)] = c(rep(c(colWeights, rep.int(0,p*k)),k-1), colWeights)
  #Jac[1:(p*k),(p*k+1):((p+1)*k)] = sapply(1:k, function(K){c(rep(0,(K-1)*p),colWeights,rep(0,(k-K)*p))})
  Jac[1:(p*k),((p+1)*k+1):((p+2)*k)] = vapply(1:k,FUN.VALUE = vector("numeric", p*k), function(K){c(rep.int(0,(K-1)*p),colWeights*2 *cMat[K,],rep.int(0,(k-K)*p))})
     
  tmp= (1+t(t(X)/thetas))*mu/(1+t(t(mu)/thetas))^2
  if(k>1){
  for (K in 1:(k-1)){
      for(Kinner in (K+1):k){

  #dLag²/dr_{ik}dr_{ik'}
            diag(Jac[(p*(K-1)+1):(p*K),(p*(Kinner-1)+1):(p*Kinner)]) =  -crossprod(  tmp, (reg[,K]*reg[,Kinner])) + lambda3Mat[Kinner, K]*colWeights
    }
  }
  
  
    #dLag²/ds_{ik}dlambda_{3kk'}
    Jac[1:(p*k),((p+2)*k+1):(p*k+nLambda)] = unlist(sapply(1:(k-1), function(K){
        vapply((K+1):k,FUN.VALUE = vector("numeric",p*k), function(Kinner){
          c(rep.int(0, p*(K-1)), colWeights*cMat[Kinner,], rep.int(0, p*(Kinner-K-1)), colWeights*cMat[K,],rep.int(0, p*(k-Kinner)))
        })
      }))
  }
  
  #Symmetrize
  Jac = Jac + t(Jac)
#dLag²/dr_{ik}²

    diag(Jac[1:(p*k),1:(p*k)]) = c(-crossprod(tmp, reg^2)) + 2*rep(beta[((p+1)*k+1):((p+2)*k)], each = p)*colWeights
  Jac
}

#-------------------------------------------#
#A score function of the NB for the row scores

dNBllrow= function(beta, X, reg, thetas, muMarg, k, n ,p, rowWeights, nLambda) {
  # @param beta: a vector of k regression parameters to optimize: the k column scores
  # @param y: the data vector of length p
  # @param reg: a kxp regressor matrix: outer product of rowScores and psis
  # @param thetas: a scalar,  the current dispersion parameters
  # @param abunds: a vector of length p with the abundances
  # @param libSizes : a vector of length n with (known) library sizes
  # @param k: a scalar, the dimension of the RC solution
  # @param p: a scalar, the number of taxa
  # @param n: a scalar, the number of samples

    # @return A vector of length p*k+2*k+ k(k-1)/2 evaluations of the lagrangian
  rMat = matrix(beta[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)
  mu = exp(rMat %*% reg)* muMarg
  
  lambda1 = beta[(k*n+1):(k*n+k)] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[(k*(n+1)+1):(k*(n+2))] #normalization restrictions sum(abunds*r^2_{ik}) = 1
#Orthogonality restrictions sum(abunds*r_{ik}*r_{ik'}) = 0. First all of dimension 1, then the remaining of dimension 2 etc. #+2*k
  if(k==1) {lambda3Mat=0
  } else {
  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(n+2)+1):length(beta)]
  }
  score = c(t(tcrossprod(reg, (X-mu)/(1+t(t(mu)/thetas))) + t(rowWeights*t(lambda1 + lambda2* 2*t(rMat) + t(rMat %*% lambda3Mat)))))
  #
  centers = colSums(rMat*rowWeights)
  unitSums = colSums(rMat^2*rowWeights)-1
  if(k==1) return(c(score,centers, unitSums))
  orthogons = unlist(sapply(1:(k-1), function(K){
    vapply((K+1):k,FUN.VALUE=0, function(Kinner){
      sum(rMat[,K]*rMat[,Kinner]*rowWeights)
    })
  }))

  return(c(score,centers, unitSums, orthogons))
}
#-------------------------------------------#
# A function to provide an analytical jacobian in the hope to speed up the calculations

NBjacobianRow = function(beta, X, reg, thetas, muMarg, k, n ,p, nlambda, rowWeights, nLambda){
  #@return a symmetric jacobian matrix of size (n+2) *k + k(k-1)/2
  rMat = matrix(beta[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)
  lambda3Mat = matrix(0,ncol=k, nrow=k) #Reorder to lower triangular matrix
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(n+2)+1):length(beta)]

    mu = exp(rMat %*% reg)* muMarg

  Jac = matrix(0, nrow= n*k + nLambda, ncol=n*k + nLambda)
  #The suymmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:(n*k),(n*k+1):((n+1)*k)] = c(rep(c(rowWeights, rep.int(0,n*k)),k-1), rowWeights) 
  #dLag²/dr_{ik}dlambda_{2k}
  Jac[1:(n*k),((n+1)*k+1):((n+2)*k)] = vapply(1:k,FUN.VALUE = vector("numeric",n*k), function(K){c(rep(0,(K-1)*n),2 *rMat[,K]*rowWeights,rep.int(0,(k-K)*n))})
  tmp = (1+t(t(X)/thetas))*mu/(1+t(t(mu)/thetas))^2
  
  if(k>1){   
  for (K in 1:(k-1)){
      for(Kinner in (K+1):k){

        #dLag²/dr_{ik}dr_{ik'}
      diag(Jac[(n*(K-1)+1):(n*K),(n*(Kinner-1)+1):(n*Kinner)]) = -tcrossprod(reg[K,]*reg[Kinner,],tmp) + lambda3Mat[Kinner, K]*rowWeights
      }
  }
  #dLag²/dr_{ik}dlambda_{3kk'}
    Jac[1:(n*k),((n+2)*k+1):(n*k+nLambda)] = unlist(sapply(1:(k-1), function(K){
        vapply((K+1):k,FUN.VALUE = vector("numeric",n*k), function(Kinner){
          c(rep.int(0, n*(K-1)), rMat[,Kinner]*rowWeights, rep.int(0, n*(Kinner-K-1)), rMat[,K]*rowWeights,rep.int(0, n*(k-Kinner)))
        })
      }))
  }
  #Symmetrize
  Jac = Jac + t(Jac)
#dLag²/dr_{ik}²

    diag(Jac[1:(n*k),1:(n*k)]) = c(t(-tcrossprod(reg^2 ,tmp) + 2*rowWeights*rep(beta[((n+1)*k+1):((n+2)*k)], each = n)))
    
  Jac

}
#-------------------------------------------#
## A wrapper function to calculate the overdispersion, abundance and importance parameters

RCM_NB = function(X, k, tol = 1e-3, maxItOut = 500, Psitol = 1e-3, verbose = TRUE, libSizes = NULL, NBRCM=NULL,  global ="dbldog", nleqslv.control=list(),method=c("Broyden"), dispFrec=10, convNorm = 2, rowWeights, colWeights, weightsChar, prior.df=10){
  
  # @param X: a nxp data matrix
  # @param k: a scalar, number of dimensions in the RC(M) model
  # @param tol(optional): a scalar, the relative convergende tolerance for the row scores and column scores parameters, defaults to 1e-3  
  # @param Psitol(optional): a scalar, the relative convergence tolerance for the psi parameters, defaults to 1e-4
  # @param maxItOut(optional): an integer, the maximum number of iteration in the outer loop, defaults to 50
  # @param verbose(optional): a boolean, should information on iterations be printed? Defaults to TRUE
  # @param method(optional): Method for solving non-linear equations, see nleqslv. Defaults to Broyden. The difference with the newton method is that the Jacobian is not recalculated at every iteration
  # @param global(optional): global strategy for solving non-linear systems , see nleqslv
  # @param nleqslv.control: a list with control options, see nleqslv
  # @param NBRCM: A RNRCM object, typically a previous fit that did not converge. Estimates from this objects will be used as starting values
  # @param dispFreq: a scalar, how many iterations the algorithm should wait before reestimationg the dispersions
  # @param convNorm: a scalar, the norm to use to determine convergence

  # @return A list with elements:
  # @return psis: a vector of length k with estimates for the importance parameters psi
  # @return thetas: a vector of length p with estimates for the overdispersion
  # @return rMat: a nxk matrix with estimated row scores
  # @return cMat: a pxk matrix with estimated column scores
  # @return converged: a boolean indicating if the algorithm converged
  # @return rowRec: a n x k x maxItOut array with a record of all rMat estimates through the iterations 
  # @return colRec: a k x p x maxItOut array with a record of all cMat estimates through the iterations
  # @return psiRec.: a k x maxItOut array with a record of all psi estimates through the iterations
  # @return iter: number of iterations
  # @return X: the original fitting matrix
  
  #Initialize some parameters
  abunds = colSums(X)/sum(X)
  libSizes = rowSums(X)
  muMarg = outer(libSizes, abunds) #The marginals to be used as expectation
  n=NROW(X)
  p=NCOL(X)
  nLambda = 2*k+k*(k-1)/2
  
  #If previous fit provided, use those starting values
  if(!is.null(NBRCM)){
    for(i in c("rMat","cMat","psis","lambdaCol","lambdaRow", "thetas")){
      assign(i, NBRCM[[i]])
    }
  #Otherwise try to use intelligent starting values
    } else{
 ## 1) Initialization
      #Depending on the weighting schemes, use other starting values
  svdX = switch(weightsChar,
                "marginalmarginal" = svd(diag(1/libSizes) %*% (X-muMarg) %*% diag(1/colSums(X))),
                "marginaluniform" = svd(diag(1/libSizes) %*% (X-muMarg)),
                "uniformmarginal" = svd((X-muMarg) %*% diag(1/colSums(X))),
                "uniformuniform" = svd(X-muMarg))
  rMat = svdX$u[,1:k, drop=FALSE]
  cMat = t(svdX$v[,1:k, drop=FALSE])
  psis = svdX$d[1:k]

#Center
  cMat = t(apply(cMat, 1, function(colS){
      colS-sum(colS*colWeights)/sum(colWeights)
  }))
rMat = apply(rMat, 2, function(rowS){
      rowS-sum(rowS*rowWeights)/sum(rowWeights)
  })
   
#Redistribute some weight to fit the constraints 
  psis = c(psis *t(apply(cMat, 1, function(colS){
      sqrt(sum(colWeights * colS^2))
  })) * apply(rMat, 2, function(rowS){
      sqrt(sum(rowWeights * rowS^2))
  }))

#Normalize  
cMat = t(apply(cMat, 1, function(colS){
      colS/sqrt(sum(colWeights * colS^2))
  }))
rMat = apply(rMat, 2, function(rowS){
      rowS/sqrt(sum(rowWeights * rowS^2))
  })
  lambdaRow =  rep.int(0,nLambda)
  lambdaCol =  rep.int(0,nLambda)
    }

  #Initialize iteration count and pre-allocate arrays to track iterations
  iterOut = 1
  rowRec = array(0,dim=c(n,k, maxItOut))
  colRec = array(0,dim=c(k,p, maxItOut))
  thetaRec = matrix(0,ncol=maxItOut, nrow=p)
  psiRec = matrix(0,ncol=maxItOut, nrow=k)

  ## 2) Propagation
  
  while((iterOut ==1) || ((iterOut <= maxItOut) && (!convergence)))
    {
    
  if(verbose && iterOut%%1 == 0){
  cat("\n","Outer Iteration", iterOut, "\n","\n")
    if(iterOut!=1){
  cat("Old psi-estimates: ", psisOld, "\n")
  cat("New psi-estimates: ", psis, "\n")
    }
  }
  ## 2)a. Store old parameters
  psisOld = psis
  rMatOld = rMat
  cMatOld = cMat
 
#Overdispersions (not at every iterations to speed things up, doesn't change a lot anyway)
    if((iterOut %% dispFrec) ==0  || iterOut==1){
  if (verbose) cat("\n Estimating overdispersions \n")
 thetas = estDisp(X = X, rMat = rMat,cMat =  cMat,  muMarg=muMarg, psis = psis, prior.df = prior.df)
  }
    
#Psis
    if (verbose) cat("\n Estimating psis (k=",k,") \n")
regPsis = sapply(1:k, simplify="array", function(K){
    outer(rMat[,K], cMat[K,])})

psisTmp = try(sort(abs(nleqslv(fn = dNBpsis, x = psis, theta = thetas, X = X, reg=regPsis, muMarg=muMarg, k=k, global=global, control = nleqslv.control, jac=NBjacobianPsi, method=method)$x), decreasing=TRUE), silent=TRUE)
if(!inherits(psisTmp, "try-error")) psis = psisTmp

#Column scores
  if (verbose) cat("\n Estimating column scores \n")
regCol = t(t(rMat)*psis)
tmpCol = try(nleqslv(fn = dNBllcol, x = c(t(cMat), lambdaCol), thetas=thetas, X = X, reg = regCol, muMarg=muMarg, k=k,  global = global, control = nleqslv.control, n=n, p=p, jac = NBjacobianCol, method=method, colWeights=colWeights, nLambda=nLambda), silent=TRUE)

if(class(tmpCol)=="list"){
  cat(ifelse(tmpCol$termcd==1, "Column scores converged \n", "Column scores DID NOT converge \n"))
  cMat = matrix(tmpCol$x[1:(k*p)], byrow=TRUE, ncol=p, nrow=k)
  lambdaCol = tmpCol$x[((k*p)+1):(k*p+nLambda)]
}
#Normalize (speeds up algorithm if previous step had not converged)
cMat = t(apply(cMat,1, function(colS){
    colS - sum(colS * colWeights)/sum(colWeights) 
  }))
cMat = t(apply(cMat, 1, function(colS){
      colS/sqrt(sum(colWeights * colS^2))
  }))

#Row scores
  if (verbose) cat("\n Estimating row scores \n")
regRow = cMat*psis
tmpRow = try(nleqslv(fn = dNBllrow, x = c(rMat, lambdaRow), thetas=thetas, X = X, reg = regRow, muMarg=muMarg, k=k,  global = global, control = nleqslv.control, n=n, p=p, jac = NBjacobianRow, method=method, rowWeights=rowWeights, nLambda=nLambda), silent=TRUE)

if(class(tmpRow)=="list"){
  cat(ifelse(tmpRow$termcd==1, "Row scores converged \n", "Row scores DID NOT converge \n"))
  rMat = matrix(tmpRow$x[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)
  lambdaRow = tmpRow$x[((k*n)+1):(k*n+nLambda)]
}

#Normalize (speeds up algorithm if previous step had not converged)
rMat = apply(rMat,2, function(rowS){
    rowS - sum(rowS * rowWeights)/sum(rowWeights) 
  })
rMat = apply(rMat, 2, function(rowS){
      rowS/sqrt(sum(rowWeights * rowS^2))
  })

#Store intermediate estimates
  rowRec[,, iterOut] = rMat
  colRec[,, iterOut] = cMat
  thetaRec [, iterOut] = thetas
  psiRec[, iterOut] = psis

    ## Change iterator
    iterOut = iterOut + 1
    
  ##Check convergence  (any numbered norm for row and column scores)
  convergence = ((iterOut <= maxItOut) && 
                   (all(abs(1-psis/psisOld) < Psitol)) &&
                   ((sum(abs(1-rMat/rMatOld)^convNorm)/n)^(1/convNorm) < tol) && 
                   ((sum(abs(1-cMat/cMatOld)^convNorm)/p)^(1/convNorm) < tol) )
} # END while-loop
  
  ## 3) Termination
  
  rownames(rMat) = rownames(X)
  colnames(cMat) = colnames(X)
  rownames(cMat) = colnames(rMat) = paste0("Dim",1:k)
  
  if(!convergence ){
    warning("Algorithm did not converge! Check for errors or consider changing tolerances or number of iterations")
  }
    return(list(converged = convergence,rMat=rMat, cMat=cMat, thetas = thetas, psis = psis, X=X,
                 rowRec= rowRec, colRec = colRec, psiRec = psiRec, thetaRec = thetaRec, lambdaRow=lambdaRow, lambdaCol=lambdaCol, rowWeights=rowWeights, colWeights=colWeights, iter=iterOut-1, fit="RCM_NB"))
}
#-------------------------------------------#

```

### Negative binomial: one by one

Fitting the dimensions one by one

```{r NBauxFuns_1B1, purl=TRUE}
#-------------------------------------#
#Fit the library sizes by ML
dNBlibSizes = function(beta, X, reg, thetas){
  mu = exp(outer(beta,reg, "+"))
  rowSums((X-mu)/(1+t(t(mu)/thetas)))
}
#------------------------------------#
# The corresponding Jacobian for the libsizes
NBjacobianLibSizes = function(beta, X, reg, thetas){
       mu = exp(outer(beta,reg, "+"))
  diag(-rowSums((1+t(t(X)/thetas))*mu/(1+t(t(mu)/thetas))^2))
}
#-------------------------------------#
#Fit the abundances by ML
dNBabunds = function(beta, X, reg, theta){
    mu = exp(outer(reg,beta, "+"))
  score = colSums((X-mu)/(1+t(t(mu)/thetas)))
}
#------------------------------------#
# The corresponding Jacobian for the libsizes
NBjacobianAbunds = function(beta, X, reg, thetas){
    mu = exp(outer(reg,beta, "+"))
  -diag(colSums((1+t(t(X)/thetas))*mu/(1+t(t(mu)/thetas))^2))
}

#------------------------------#
#A small aux function for the length of the lambdas
seq_k = function(y){
  (y-1)*(2+(y-2)/2) + seq_len(y+1)
}

#--------------------------------------#
dNBpsis_1B1 = function(beta, X, reg, theta,  muMarg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param rMat: a nxk matrix with row scores
  # @param cMat: a kxp matrix with column scores
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution

  # @return A vector of length r with the new psi estimates

  mu = muMarg * exp(reg* beta)
  
  sum(reg*(X-mu)/(1+t(t(mu)/theta)))

}
#--------------------------------------#
#A jacobian for the psi parameters
NBjacobianPsi_1B1 = function(beta, X, reg, muMarg, theta){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  mu = muMarg * exp(reg* beta)
  -sum(reg^2*(1+t(t(X)/theta))*mu/(1+t(t(mu)/theta))^2)

}
#-------------------------------------# 
#The influence function for the psis
NBpsiInfl = function(psi, X, cMat, rMat, muMarg, theta){
  reg = rMat %*% cMat
  mu = muMarg * exp(reg* psi)
  -((X-mu)*(theta+mu))/(reg*(theta+X)*mu)
}

#--------------------------------------#
#A score function of the NB for the column scores

dNBllcol_1B1 = function(beta, X, reg, thetas, muMarg, k, p, n, colWeights, nLambda, cMatK) {
  # @param beta: a vector of k*p column scores + k within dimensional centering lambda parameters + k within dimensional normalization parameters +  k(k-1)/2 between dimension lambda parameters
  # @param y: the data vector of length n
  # @param reg: a nxk regressor matrix: outer product of rowScores and psis
  # @param thetas: a scalar,  the current dispersion parameters
  # @param logAbund: a scalar with the LOG OF THE abundance parameter
  # @param libSizes : a vector of length n with (known) library sizes
  # @param k: a scalar, the dimension of the RC solution
  # @param p: a scalar, the number of taxa
  # @param n: a scalar, the number of samples
  # @param cMatK: the lower dimensions of the colScores

  # @return A vector of length p*k+2*k+ k(k-1)/2 evaluations of the lagrangian
  
  cMat = matrix(beta[1:p], byrow=TRUE, ncol=p, nrow=1)
  # logMu[,K] = reg[,K]*beta
  mu = exp(reg %*% cMat) * muMarg
  
  lambda1 = beta[p+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[p+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(p+3):length(beta)]}
  
  score = crossprod(reg,((X-mu)/(1+t(t(mu)/thetas)))) + (colWeights[,1]*lambda1 + colWeights[,2]*lambda2*2*cMat + (lambda3 %*% cMatK)*colWeights[,3])

  center = sum(colWeights[,1]*cMat)
  unitSum = sum(colWeights[,2]*cMat^2)-1
  if(k==1) return(c(score, center, unitSum))
  orthogons = colSums(t(cMatK)*c(cMat)*colWeights[,3])

  return(c(score, center, unitSum, orthogons))
}
#--------------------------------------#
# A function to provide an analytical jacobian in the hope to speed up the calculations
NBjacobianCol_1B1 = function(beta, X, reg, thetas, muMarg, k, n ,p, colWeights, nLambda, cMatK){
  #@return a symmetric jacobian matrix of size p*k + k(k-1)/2
  cMat = matrix(beta[1:p], byrow=TRUE, nrow=1, ncol=p)

  #Calculate the mean
  mu = exp(reg %*% cMat)* muMarg
  
  Jac = matrix(0, nrow= p + nLambda, ncol=p + nLambda)
  #The symmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle
#   
  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:p,(p+1)] = colWeights[,1]
  #Jac[1:(p*k),(p*k+1):((p+1)*k)] = sapply(1:k, function(K){c(rep(0,(K-1)*p),colWeights,rep(0,(k-K)*p))})
  Jac[1:p,p+2] = colWeights[,2]*2 *cMat
     
  tmp = (1+t(t(X)/thetas))*mu/(1+t(t(mu)/thetas))^2

    #dLag²/ds_{ik}dlambda_{3kk'}
  if(k>1){
    Jac[1:p,(p+3):(p+nLambda)] = t(t(cMatK)*colWeights[,3])
  }

  #Symmetrize
   Jac = Jac + t(Jac)
#dLag²/dr_{ik}²

    diag(Jac[1:p,1:p]) = -crossprod(tmp, reg^2) + 2*beta[p+2]*colWeights[,2]
  Jac
}
#--------------------------------------#
# The influence function for the column scores
NBcolInfl = function(X, psis, cMat, rMat, thetas, colWeights, k, lambdaCol){
  reg = psis[k] *rMat[,k]
  muMarg = outer(rowSums(X), colSums(X)/sum(X)) * exp(rMat[,1:k, drop=FALSE] %*% (cMat[1:k,, drop=FALSE]*psis[1:k]))
  p=ncol(X)
  n = nrow(X)
  lambdaCol = lambdaCol[seq_k(k)]
  cMatK = cMat[1:(k-1),,drop=FALSE]
  nLambda = length(lambdaCol)
  tmp = if(k>1) lambdaCol[-(1:2)] %*% cMatK else 0
  
  score= t(t(reg*((X-muMarg)/(1+t(t(muMarg)/thetas)))) + c(colWeights*(lambdaCol[1] + lambdaCol[2]*2*cMat[k,] + tmp)))

  JacobianInv = solve(NBjacobianCol_1B1(beta = c(cMat[k,], lambdaCol), X = X, reg= reg, thetas = thetas, muMarg = muMarg, k = k, p = p, n=n ,colWeights = colWeights , nLambda = nLambda, cMatK = cMatK)) #Inverse Jacobian
  
  #After a long thought: The X's do not affect the estimation of the lambda parameters!
  #Matrix becomes too large: return score and inverse jacobian
  return(list(score=score, InvJac = JacobianInv[1:p,1:p]))
}
#-------------------------------------------#

#A function to extract the influence for a given parameter index
getInflCol = function(score, InvJac, parIndex){
  t(t(score)* InvJac[, parIndex])
}

#-------------------------------------------#
#A score function of the NB for the row scores

dNBllrow_1B1 = function(beta, X, reg, thetas, muMarg, k, n , p, rowWeights, nLambda, rMatK) {
  # @param beta: a vector of k regression parameters to optimize: the k column scores
  # @param y: the data vector of length p
  # @param reg: a kxp regressor matrix: outer product of rowScores and psis
  # @param thetas: a scalar,  the current dispersion parameters
  # @param abunds: a vector of length p with the abundances
  # @param libSizes : a vector of length n with (known) library sizes
  # @param k: a scalar, the dimension of the RC solution
  # @param p: a scalar, the number of taxa
  # @param n: a scalar, the number of samples

    # @return A vector of length p*k+2*k+ k(k-1)/2 evaluations of the lagrangian
  rMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)
  mu = exp(rMat %*% reg)* muMarg
  
  lambda1 = beta[n+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[n+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(n+3):length(beta)]}

  score = c(tcrossprod(reg, (X-mu)/(1+t(t(mu)/thetas)))) + (rowWeights[,1]*lambda1 + rowWeights[,2]*lambda2* 2*rMat + (rMatK %*% lambda3)*rowWeights[,3])
  #
  center = sum(rMat*rowWeights[,1])
  unitSum = sum(rMat^2*rowWeights[,2])-1
  if(k==1){ return(c(score,center, unitSum))}
  orthogons = apply(rMatK, 2,function(x){
    sum(x*rowWeights[,3]*rMat)
  })

  return(c(score,center, unitSum, orthogons))
}
#-------------------------------------------#
# A function to provide an analytical jacobian in the hope to speed up the calculations

NBjacobianRow_1B1 = function(beta, X, reg, thetas, muMarg, k, n ,p, nlambda, rowWeights, nLambda, rMatK){
  #@return a symmetric jacobian matrix of size (n+2) *k + k(k-1)/2
  rMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)

  mu = exp(rMat %*% reg)* muMarg

  Jac = matrix(0, nrow= n + nLambda, ncol= n + nLambda)
  #The suymmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:n, n+1] = rowWeights[,1]
  #dLag²/dr_{ik}dlambda_{2k}
  Jac[1:n, n+2] = 2 *rMat*rowWeights[,2]
  tmp = (1+t(t(X)/thetas))*mu/(1+t(t(mu)/thetas))^2

  #dLag²/dr_{ik}dlambda_{3kk'}
  if(k>1){
    Jac[1:n,(n+3):(n+nLambda)] = rMatK*rowWeights[,3]
  }
  #Symmetrize
  Jac = Jac + t(Jac)
#dLag²/dr_{ik}²

    diag(Jac[1:n,1:n]) = -tcrossprod(reg^2 ,tmp) + 2*rowWeights[,2]*beta[n+2]
    
  Jac
}
#--------------------------------------#
# The influence function for the row scores
NBrowInfl = function(X, psis, cMat, rMat, thetas, rowWeights, k, lambdaRow){
  reg = psis[k] *cMat[k,]
  muMarg = outer(rowSums(X), colSums(X)/sum(X)) * exp(rMat[,1:k, drop=FALSE] %*% (cMat[1:k,, drop=FALSE]*psis[1:k]))
  p=ncol(X)
  n = nrow(X)
  lambdaRow = lambdaRow[seq_k(k)]
  rMatK = rMat[,1:(k-1),drop=FALSE]
  nLambda = length(lambdaRow)
  tmp = if(k>1) rMatK %*% lambdaRow[-(1:2)] else 0
  
  score= reg*(X-muMarg)/(1+t(t(muMarg)/thetas)) + c(rowWeights*(lambdaRow[1] + lambdaRow[2]*2*rMat[,k] + tmp))

  JacobianInv = solve(NBjacobianRow_1B1(beta = c(rMat[,k], lambdaRow), X = X, reg= reg, thetas = thetas, muMarg = muMarg, k = k, p = p, n=n, rowWeights = rowWeights , nLambda = nLambda, rMatK = rMatK)) #Inverse Jacobian
  
  #After a long thought: The X's do not affect the estimation of the lambda parameters!
  #Matrix becomes too large: return score and inverse jacobian
  return(list(score=score, InvJac = JacobianInv[1:n,1:n]))
}
#-------------------------------------------#

#A function to extract the influence for a given parameter index
getInflRow = function(score, InvJac, parIndex){
  score* InvJac[, parIndex]
}

#-------------------------------------------#
## A wrapper function to calculate the overdispersion, abundance and importance parameters. Each dimension is estimated one by one

RCM_NB_1B1 = function(X, k, rowWeights, colWeights, weightsChar, tol = 1e-3, maxItOut = 500, Psitol = 1e-3, verbose = TRUE, NBRCM=NULL, global ="dbldog", nleqslv.control=list(), method=c("Broyden"), dispFrec=10, convNorm = 2, prior.df=10, marginEst = c( "marginSums", "MLE")){
  
  # @param X: a nxp data matrix
  # @param k: a scalar, number of dimensions in the RC(M) model
  # @param tol(optional): a scalar, the relative convergende tolerance for the row scores and column scores parameters, defaults to 1e-3  
  # @param Psitol(optional): a scalar, the relative convergence tolerance for the psi parameters, defaults to 1e-4
  # @param maxItOut(optional): an integer, the maximum number of iteration in the outer loop, defaults to 50
  # @param verbose(optional): a boolean, should information on iterations be printed? Defaults to TRUE
  # @param method(optional): Method for solving non-linear equations, see nleqslv. Defaults to Broyden. The difference with the newton method is that the Jacobian is not recalculated at every iteration
  # @param global(optional): global strategy for solving non-linear systems , see nleqslv
  # @param nleqslv.control: a list with control options, see nleqslv
  # @param NBRCM: A NBRCM object, typically a previous fit that did not converge. Estimates from this objects will be used as starting values
  # @param dispFreq: a scalar, how many iterations the algorithm should wait before reestimationg the dispersions
  # @param convNorm: a scalar, the norm to use to determine convergence

  # @return A list with elements:
  # @return psis: a vector of length k with estimates for the importance parameters psi
  # @return thetas: a vector of length p with estimates for the overdispersion
  # @return rMat: a nxk matrix with estimated row scores
  # @return cMat: a pxk matrix with estimated column scores
  # @return converged: a boolean indicating if the algorithm converged
  # @return rowRec: a n x k x maxItOut array with a record of all rMat estimates through the iterations 
  # @return colRec: a k x p x maxItOut array with a record of all cMat estimates through the iterations
  # @return psiRec.: a k x maxItOut array with a record of all psi estimates through the iterations
  # @return iter: number of iterations
  # @return X: the original fitting matrix
  
  n=NROW(X)
  p=NCOL(X)
  
  #Initialize some parameters
  abunds = colSums(X)/sum(X)
  libSizes = rowSums(X)
  
  if(marginEst == "MLE"){

  logLibSizesMLE = log(libSizes)
  logAbundsMLE = log(abunds)
  

  initIter = 1
  
   while((initIter ==1) || ((initIter <= maxItOut) && (!convergenceInit))){
     libsOld = logLibSizesMLE
     absOld = logAbundsMLE
     
         thetas = estDisp(X = X, rMat = as.matrix(rep(0,n)), cMat = t(as.matrix(rep(0,p))),  muMarg=exp(outer(logLibSizesMLE, logAbundsMLE, "+")), psis = 0, prior.df = prior.df)

 libsTmp = try(nleqslv(fn = dNBlibSizes, x = logLibSizesMLE, theta = thetas, X = X, reg=logAbundsMLE, global=global, control = nleqslv.control, jac=NBjacobianLibSizes, method=method)$x, silent=TRUE)
 if(class(libsTmp)!="try-error"){ logLibSizesMLE = libsTmp}
  absTmp = try(nleqslv(fn = dNBabunds, x = logAbundsMLE, theta = thetas, X = X, reg=logLibSizesMLE, global=global, control = nleqslv.control, jac=NBjacobianAbunds, method=method)$x, silent=TRUE)
 if(class(absTmp)!="try-error"){ logAbundsMLE = absTmp}
  initIter = initIter + 1
  
  convergenceInit = ((initIter <= maxItOut) && 
                    ((sum(abs(1-logLibSizesMLE/libsOld)^convNorm)/n)^(1/convNorm) < tol) && 
                   ((sum(abs(1-logAbundsMLE/absOld)^convNorm)/p)^(1/convNorm) < tol) )
  
   }
  #Converges very fast, even when dispersions are re-estimated. For the library sizes there is a big difference, for the abundances less so
 
    muMarg = exp(outer(logLibSizesMLE, logAbundsMLE, "+")) #The marginals to be used as expectation. These are augmented with the previously estimated dimensions every time
  } else if(marginEst=="marginSums"){
    muMarg = outer(libSizes, abunds)
  } else{
    stop("No valid margin estimation paradigm provided! \n")
  }

  nLambda = 2*k+k*(k-1)/2
  # Pre-allocate arrays to track iterations
  rowRec = array(0,dim=c(n,k, maxItOut))
  colRec = thetaRec = array(0,dim=c(k,p, maxItOut))
  psiRec = matrix(0, nrow=k,ncol=maxItOut)
  convergence = rep(FALSE, k)
  iterOut = rep(1,k)
  
  #Triple weights if needed (allows for different weighting schemes)
  if(!is.matrix(colWeights)){
    colWeights = matrix(colWeights, ncol = 3, nrow= length(colWeights))
  }
  if(!is.matrix(rowWeights)){
    rowWeights = matrix(rowWeights, ncol = 3, nrow= length(rowWeights))
  }
  
  #If previous fit provided with higher or equal dimension, stop here
  if((!is.null(NBRCM)) ){
    if(NBRCM$fit != "RCM_NB_1B1"){
      stop("Fit provided is not of same type as the one requested! \n")
    } else if((k <= NBRCM$k)) {
      stop("Fit provided is already of the required dimension or higher! \n")
    } else{}
  #Otherwise try to use intelligent starting values
  }
  
 ## 1) Initialization
  svdX = svd(diag(1/libSizes) %*% (X-muMarg) %*% diag(1/colSums(X)))
  rMat = svdX$u[,1:k, drop=FALSE]
  cMat = t(svdX$v[,1:k, drop=FALSE])
  psis = svdX$d[1:k]

#Center
  cMat = t(apply(cMat, 1, function(colS){
      colS-sum(colS*colWeights[,1])/sum(colWeights[,1])
  }))
rMat = apply(rMat, 2, function(rowS){
      rowS-sum(rowS*rowWeights[,1])/sum(rowWeights[,1])
  })
   
#Redistribute some weight to fit the constraints 
  psis = c(psis *t(apply(cMat, 1, function(colS){
      sqrt(sum(colWeights[,2] * colS^2))
  })) * apply(rMat, 2, function(rowS){
      sqrt(sum(rowWeights[,2] * rowS^2))
  }))

#Normalize  
cMat = t(apply(cMat, 1, function(colS){
      colS/sqrt(sum(colWeights[,2] * colS^2))
  }))
rMat = apply(rMat, 2, function(rowS){
      rowS/sqrt(sum(rowWeights[,2] * rowS^2))
  })
  lambdaRow =  rep.int(0,nLambda)
  lambdaCol =  rep.int(0,nLambda)
  
 if(!is.null(NBRCM)){ #If fit provided, replace lower dimension starting values
   Kprev = NBRCM$k
   rMat[,1:Kprev] = NBRCM$rMat
   rowRec[,1:Kprev,] = NBRCM$rowRec
   cMat[1:Kprev,] = NBRCM$cMat
   colRec[1:Kprev,,] = NBRCM$colRec
   psis[1:Kprev] = NBRCM$psis
   psiRec[1:Kprev,] = NBRCM$psiRec
   thetas = NBRCM$thetas #Useless now but could prevent code to break when estimation order is changed
   lambdaCol[1:(Kprev*(2+(Kprev-1)/2))] = NBRCM$lambdaCol
   lambdaRow[1:(Kprev*(2+(Kprev-1)/2))] = NBRCM$lambdaRow
   convergence[1:Kprev] = NBRCM$converged
   iterOut[1:Kprev] = NBRCM$iter
 }

  minK = ifelse(is.null(NBRCM),1,Kprev+1)
  for (KK in minK:k){

    cat("Dimension" ,KK, "is being esimated \n")
    
  #Modify offset if needed
  if(KK>1){muMarg = muMarg * exp(rMat[,(KK-1), drop=FALSE] %*% (cMat[(KK-1),, drop=FALSE]*psis[(KK-1)]))}

    idK = seq_k(KK)
  ## 2) Propagation
  
  while((iterOut[KK] ==1) || ((iterOut[KK] <= maxItOut) && (!convergence[KK])))
    {
    
  if(verbose && iterOut[KK]%%1 == 0){
  cat("\n","Outer Iteration", iterOut[KK], "\n","\n")
    if(iterOut[KK]!=1){
  cat("Old psi-estimates: ", psisOld, "\n")
  cat("New psi-estimates: ", psis[KK], "\n")
    }
  }
  ## 2)a. Store old parameters
  psisOld = psis[KK]
  rMatOld = rMat[,KK]
  cMatOld = cMat[KK,]
 
#Overdispersions (not at every iterations to speed things up, doesn't change a lot anyway)
    if((iterOut[KK] %% dispFrec) ==0  || iterOut[KK]==1){
  if (verbose) cat("\n Estimating overdispersions \n")
 thetas = estDisp(X = X, rMat = rMat[,KK,drop=FALSE], cMat = cMat[KK,,drop=FALSE],  muMarg=muMarg, psis = psis[KK], prior.df = prior.df)
  }
    
#Psis
    if (verbose) cat("\n Estimating psis (k =",k,") \n")
regPsis = outer(rMat[,KK] ,cMat[KK,])

psisTmp = try(abs(nleqslv(fn = dNBpsis_1B1, x = psis[KK], theta = thetas, X = X, reg=regPsis, muMarg=muMarg, global=global, control = nleqslv.control, jac=NBjacobianPsi_1B1, method=method)$x), silent=TRUE)
if(!inherits(psisTmp, "try-error")) psis[KK] = psisTmp

#Column scores
  if (verbose) cat("\n Estimating column scores \n")
regCol = t(t(rMat[,KK, drop=FALSE])*psis[KK])
tmpCol = try(nleqslv(fn = dNBllcol_1B1, x = c(t(cMat[KK,]), lambdaCol[idK]), thetas=thetas, X = X, reg = regCol, muMarg=muMarg, k=KK,  global = global, control = nleqslv.control, n=n, p=p, jac = NBjacobianCol_1B1, method=method, colWeights=colWeights, nLambda=(KK+1), cMatK = cMat[1:(KK-1),,drop=FALSE]), silent=TRUE)

if(class(tmpCol)=="list"){
  cat(ifelse(tmpCol$termcd==1, "Column scores converged \n", "Column scores DID NOT converge \n"))
  cMat[KK,] = tmpCol$x[1:p]
  lambdaCol[idK] = tmpCol$x[p + seq_along(idK)]
}
#Normalize (speeds up algorithm if previous step had not converged)
cMat[KK,] = cMat[KK,] - sum(cMat[KK,] * colWeights[,1])/sum(colWeights[,1]) 
cMat[KK,] = cMat[KK,]/sqrt(sum(colWeights[,2] * cMat[KK,]^2))

#Row scores
  if (verbose) cat("\n Estimating row scores \n")
regRow = cMat[KK,,drop=FALSE]*psis[KK]
tmpRow = try(nleqslv(fn = dNBllrow_1B1, x = c(rMat[,KK], lambdaRow[idK]), thetas=thetas, X = X, reg = regRow, muMarg=muMarg, k=KK,  global = global, control = nleqslv.control, n=n, p=p, jac = NBjacobianRow_1B1, method=method, rowWeights=rowWeights, nLambda=(KK+1), rMatK = rMat[,1:(KK-1), drop=FALSE]), silent=TRUE)

if(class(tmpRow)=="list"){
  cat(ifelse(tmpRow$termcd==1, "Row scores converged \n", "Row scores DID NOT converge \n"))
  rMat[,KK] = tmpRow$x[1:n]
  lambdaRow[idK] = tmpRow$x[n + seq_along(idK)]
}

#Normalize (speeds up algorithm if previous step had not converged)
rMat[,KK] = rMat[,KK] - sum(rMat[,KK] * rowWeights[,1])/sum(rowWeights[,1]) 
rMat[,KK] = rMat[,KK]/sqrt(sum(rowWeights[,2] * rMat[,KK]^2))

#Store intermediate estimates
  rowRec[,KK, iterOut[KK]] = rMat[,KK]
  colRec[KK,, iterOut[KK]] = cMat[KK,]
  thetaRec [KK,, iterOut[KK]] = thetas
  psiRec[KK, iterOut[KK]] = psis[KK]

    ## Change iterator
    iterOut[KK] = iterOut[KK] + 1
    
  ##Check convergence  (any numbered norm for row and column scores)
  convergence[KK] = ((iterOut[KK] <= maxItOut) && 
                   (all(abs(1-psis[KK]/psisOld) < Psitol)) && #Infinity norm for the psis
                   ((sum(abs(1-rMat[,KK]/rMatOld)^convNorm)/n)^(1/convNorm) < tol) && 
                   ((sum(abs(1-cMat[KK,]/cMatOld)^convNorm)/p)^(1/convNorm) < tol) )
} # END while-loop until convergence
  
  }# END for-loop over dimensions
  
  ## 3) Termination
  
  rownames(rMat) = rownames(X)
  colnames(cMat) = colnames(X)
  rownames(cMat) = colnames(rMat) = paste0("Dim",1:k)
  
  if(!all(convergence) ){
    warning("Algorithm did not converge for all dimensions! Check for errors or consider changing tolerances or number of iterations")
  }
    return(list(converged = convergence,rMat=rMat, cMat=cMat, thetas = thetas, psis = psis, X=X,
                 rowRec= rowRec, colRec = colRec, psiRec = psiRec, thetaRec = thetaRec, lambdaRow=lambdaRow, lambdaCol=lambdaCol, rowWeights=rowWeights, colWeights=colWeights, iter=iterOut-1, fit="RCM_NB_1B1", libSizesMLE = switch(marginEst, "MLE"=exp(logLibSizesMLE), "marginSums" = libSizes), abundsMLE = switch(marginEst, "MLE"=exp(logAbundsMLE), "marginSums"=abunds)))
}
#-------------------------------------------#
```

## Zero-inflated Poisson

```{r Zero-inflated poisson, purl=TRUE}
#----------------------------#
#expit
expit=function(x){
  tmp = exp(x)/(1+exp(x))
  tmp[is.na(tmp)]=1 #Adjust for overflow
  tmp}

#-----------------------------------#
## A function to perform the E-step

EstepZIP  = function(X, rMat, cMat, tMat, vMat, k, muMarg,  Zcol, psis, chis){
  
  # @return: The values of Z
  expMu = exp(rMat %*% (cMat*psis)) * muMarg
  regZero =  t(Zcol + t(tMat %*% (vMat * chis)))
  
  Z = X
  Z[X>0] = 0
  Z[X==0] = (1+exp(-regZero-expMu))[X==0]^(-1)
  Z
}
#-----------------------------------#

## A function to perform the M step: maximize the likelihoods. This will again be an iterative process, estimating the parameters step by step. estimation of poisson and zero-inflated part can occur independently, which opens up opportunities for parallelization.

MstepZIP = function(Z, X, rMat, cMat, tMat, vMat,  k, muMarg,  Zcol, psis, chis, lambdaCol, lambdaRow, lambdaColZero, lambdaRowZero, twoCores=TRUE, tol=1e-3, psiTol = 1e-4, chiTol = psiTol, convNorm = 2 , maxItMean=5 , maxItZeroes= 20,n=n, p=p, global=global, nleqslv.control= nleqslv.control, nLambda, rowWeights, colWeights){
  
#Optimization of the mena and zero-inflated components are independent (see Lambert 1992), so fork here
if(twoCores){
resList = mclapply(mc.cores=2, c(meanEstZIP, ZIestZIP), function(fun){
  fun(X=X, rMat=rMat, cMat=cMat, tMat=tMat, chis=chis, vMat=vMat, Zcol=Zcol, lambdaCol=lambdaCol, lambdaRow=lambdaRow, lambdaRowZero=lambdaRowZero, lambdaColZero=lambdaColZero, psiTol=psiTol, chiTol=chiTol, tol=tol, convNorm = convNorm, nleqslv.control = nleqslv.control, global=global, nLambda=nLambda, k=k, Z=Z, muMarg=muMarg,n=n, p=p, psis=psis, maxItMean = maxItMean, maxItZeroes = maxItZeroes, rowWeights=rowWeights, colWeights=colWeights)
})
} else{
  resList = lapply(c(meanEstZIP, ZIestZIP), function(fun){ fun(X=X, rMat=rMat, cMat=cMat, tMat=tMat, chis=chis, vMat=vMat, Zcol=Zcol, lambdaCol=lambdaCol, lambdaRow=lambdaRow, lambdaRowZero=lambdaRowZero, lambdaColZero=lambdaColZero, psiTol=psiTol, chiTol=chiTol, tol=tol, convNorm = convNorm, nleqslv.control = nleqslv.control, global=global, nLambda=nLambda, k=k, Z=Z, muMarg=muMarg,n=n, p=p, psis=psis, maxItMean = maxItMean, maxItZeroes = maxItZeroes, rowWeights=rowWeights, colWeights=colWeights)})
  }

return(unlist(resList, recursive=FALSE))
}
#--------------------------------------#

# A function to estimate the mean component of the ZIP model
meanEstZIP = function(X, rMat, cMat, Z, muMarg, k, global, nleqslv.control, tol, psiTol,lambdaCol, lambdaRow, convNorm,  nLambda, n, p, psis, rowWeights, colWeights, maxItMean = 10, maxItZeroes = 10,...){
 #Mean component
  
    iter = 1
  while((iter==1 || !converged) && iter<maxItMean){
    
  cat("Inner iteration(mean)", iter, "\n")
    
  psiOld = psis
  rMatOld = rMat
  cMatOld = cMat

    ## Row scores
  cat("Estimating row scores mean \n")
  regRow = cMat*psis
  rMatSol = try(nleqslv(fn = dZipMeanRmat, x = c(rMat, lambdaRow),X=X, reg =regRow, muMarg=muMarg, k=k, n=n, p=p, global=global, control = nleqslv.control, jac=ZipJacobianRmat, Z=Z, nLambda=nLambda, rowWeights=rowWeights)$x, silent=TRUE)
  
  
  if(class(rMatSol)!="try-error"){
  rMat = matrix(rMatSol[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)
  lambdaRow = rMatSol[(k*n+1):length(rMatSol)]
  }
  
  #Normalize (speeds up algorithm if previous step had not converged)
rMat = apply(rMat,2, function(rowS){
    rowS - sum(rowS * rowWeights)/sum(rowWeights) 
  })
rMat = apply(rMat, 2, function(rowS){
      rowS/sqrt(sum(rowWeights * rowS^2))
  })
  
  ## Column scores
  cat("Estimating column scores mean \n")
  regCol = t(t(rMat)*psis)
  cMatSol = try(nleqslv(fn = dZipMeanCmat, x = c(t(cMat), lambdaCol), X=X, reg=regCol, muMarg=muMarg, k=k, n=n, p=p, global=global, control = nleqslv.control, jac=ZipJacobianCmat, Z=Z, nLambda=nLambda, colWeights=colWeights)$x, silent=TRUE)
    if(class(cMatSol)!="try-error"){
  cMat = matrix(cMatSol[1:(k*p)], byrow=TRUE, nrow=k, ncol=p)
  lambdaCol = cMatSol[(k*p+1):length(cMatSol)]
    }
  
cMat = t(apply(cMat,1, function(colS){
    colS - sum(colS * colWeights)/sum(colWeights) 
  }))
cMat = t(apply(cMat, 1, function(colS){
      colS/sqrt(sum(colWeights * colS^2))
  }))
  
    cat("\n Estimating psis (k=",k,") \n")
    
  regPsi = sapply(1:k, simplify="array", function(K){
    outer(rMat[,K], cMat[K,])
  })

  psisSol = try(sort(abs(nleqslv(fn = dZipMeanPsi, x = psis, X=X, reg=regPsi, Z=Z, muMarg=muMarg, k=k, global=global, control = nleqslv.control, jac=ZipJacobianPsi)$x), decreasing=TRUE), silent=TRUE)
    if(class(psisSol)!="try-error") psis=psisSol
  
    converged = all(abs(psiOld-psis) < psiTol) &&  (sum(abs(1-rMat/rMatOld)^convNorm))^(1/convNorm) < tol &&  (sum(abs(1-cMat/cMatOld)^convNorm))^(1/convNorm) < tol 
  iter = iter +1
  }
  
  return(list(cMat=cMat, rMat=rMat, iterMean = iter, psis=psis, convergedMean=converged, lambdaCol = lambdaCol, lambdaRow=lambdaRow))
}
#--------------------------------------#

# A function to estimate the zero inflated component of the ZIP model

ZIestZIP = function(X, Z, muMarg, k, global, nleqslv.control, tol, chiTol, tMat, vMat, chis, Zcol,lambdaColZero, lambdaRowZero,convNorm,n, p, nLambda,psis,rowWeightsZeroNum, colWeightsZeroNum, maxItMean = 10, maxItZeroes = 10,...){
  
      iter = 1
  while((iter==1 || !converged) && iter<maxItZeroes){
   chiOld = chis
  tMatOld = tMat
  vMatOld = vMat
  
    cat("Inner iteration(zeroes)", iter, "\n")

  # nleqslv.control$trace=FALSE
  # Zero component
    ## Chis
      cat("Estimating chis (zeroes) \n")
  regChis =  sapply(1:k, simplify="array", function(K){
    outer(tMat[,K], vMat[K,])
  })
  chisSol = try(sort(abs(nleqslv(fn = dZipMeanChi, x = chis, reg=regChis, Z=Z,  k=k, global=global, control = nleqslv.control, Zcol=Zcol, jac=ZipJacobianChi)$x), decreasing=TRUE), silent=TRUE)
  if(!inherits(chisSol,"try-error")){
    chis=chisSol
  }
  
    ## Row scoers
    cat("Estimating row scores zeroes \n")
    regRowZero = vMat*chis
    tMatSol = try(nleqslv(fn = dZipMeanTmat, x = c(tMat, lambdaRowZero),  k=k, n=n, reg=regRowZero,p=p, global=global, control = nleqslv.control, Zcol=Zcol, jac=ZipJacobianTmat, Z=Z, nLambda=nLambda, rowWeights=rowWeightsZeroNum)$x, silent=TRUE)
  if(!inherits(tMatSol,"try-error")){
    tMat = matrix(tMatSol[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)
    lambdaRowZero = tMatSol[(k*n+1):length(tMatSol)]
      }
    
    ## Column scores
    cat("Estimating column scores zeroes \n")
    regColZero = t(t(tMat)*chis)
    vMatSol = try(nleqslv(fn = dZipMeanVmat, x = c(t(vMat), lambdaColZero), reg=regColZero, k=k, n=n, p=p, global=global, control = nleqslv.control, Zcol=Zcol, jac=ZipJacobianVmat, Z=Z, nLambda=nLambda, colWeights=colWeightsZeroNum)$x, silent=TRUE)
  if(!inherits(vMatSol,"try-error")){
    vMat = matrix(vMatSol[1:(k*p)], byrow=TRUE, nrow=k, ncol=p)
    lambdaColZero = vMatSol[(k*p+1):length(vMatSol)]
          }

  converged = all ((chiOld-chis) < chiTol) &&  (sum(abs(1-tMat/tMatOld)^convNorm))^(1/convNorm) < tol &&  (sum(abs(1-vMat/vMatOld)^convNorm))^(1/convNorm) < tol 
  iter = iter +1
}
  return(list(vMat=vMat, tMat=tMat, iterZI = iter, chis=chis, convergedZI=converged, lambdaColZero=lambdaColZero, lambdaRowZero=lambdaRowZero))
}

#All matrices X are considered to be nxp, i.e. samples are rows and taxa are columns

#--------------------------------------#
dZipMeanPsi = function(beta, X, muMarg, k, Z, reg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  mu = exp(arrayprod(reg, beta)) * muMarg
  
  vapply(1:k,FUN.VALUE=0, function(u){
  sum((1 - Z)*(X - mu)*reg[,,u])
  })
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianPsi = function(beta, X, reg, muMarg, k, Z){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  mu = exp(arrayprod(reg, beta)) * muMarg
  
  sapply(1:k, function(K){
    vapply(1:k,FUN.VALUE=0, function(Kinner){
    sum(mu*reg[,,Kinner]*reg[,,K]*(Z-1))
  })})
}

#--------------------------------------#
dZipMeanRmat = function(beta, X, reg, muMarg, k,p,n,  Z, nLambda, rowWeights){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  rMat = matrix(beta[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)
  mu = exp(rMat %*% reg) * muMarg
  
   lambda1 = beta[(k*n+1):(k*n+k)] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[(k*(n+1)+1):(k*(n+2))] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  if(k==1){lambda3Mat=0}else{
  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(n+2)+1):length(beta)]
  }
  
  score = c(t(
    tcrossprod(reg ,(1-Z)*(X-mu))) + 
                        rowWeights*(t(lambda1 + lambda2*2*t(rMat)) + (rMat %*% lambda3Mat))
    )
  centers = colSums(rMat*rowWeights)
  unitSums = colSums(rMat^2*rowWeights)-1
  if(k==1){return(c(score,centers, unitSums))}
  orthogons = unlist(sapply(1:(k-1), function(K){
    vapply((K+1):k,FUN.VALUE=0, function(Kinner){
      sum(rMat[,K]*rMat[,Kinner]*rowWeights)
    })
  }))
    return(c(score,centers, unitSums, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianRmat = function(beta, X, reg, muMarg, k, p, n, Z, nLambda, rowWeights){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  rMat = matrix(beta[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)
  mu = exp(rMat %*% reg) * muMarg

    if(k==1){lambda3Mat=0}else{
  lambda3Mat = matrix(0,ncol=k, nrow=k) #Reorder to lower triangular matrix
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(n+2)+1):length(beta)]
    }
    Jac = matrix(0, nrow= n*k + nLambda, ncol=n*k + nLambda)
  #The suXmmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:(n*k),(n*k+1):((n+1)*k)] = c(rep.int(c(rowWeights, rep.int(0,n*k)),k-1), rowWeights) 
  #dLag²/dr_{ik}dlambda_{2k}
  Jac[1:(n*k),((n+1)*k+1):((n+2)*k)] = vapply(1:k,FUN.VALUE = vector("numeric",n*k), function(K){c(rep.int(0,(K-1)*n),2*rMat[,K]*rowWeights,rep.int(0,(k-K)*n))})
  
  if(k>1){

  for (K in 1:(k-1)){
      for(Kinner in (K+1):k){
        #dLag²/dr_{ik}dr_{ik'}
      diag(Jac[(n*(K-1)+1):(n*K),(n*(Kinner-1)+1):(n*Kinner)]) =  ((1-Z)*(-t(t(mu)* reg[Kinner,]))) %*% reg[K,] +
         lambda3Mat[Kinner, K]*rowWeights
      }
  }
  #dLag²/dr_{ik}dlambda_{3kk'}
    Jac[1:(n*k),((n+2)*k+1):(n*k+nLambda)] = unlist(sapply(1:(k-1), function(K){
        vapply((K+1):k,FUN.VALUE = vector("numeric",n*k), function(Kinner){
          c(rep.int(0, n*(K-1)), rMat[,Kinner]*rowWeights, rep.int(0, n*(Kinner-K-1)), rMat[,K]*rowWeights,rep.int(0, n*(k-Kinner)))
        })
      }))
  }
  #Symmetrize
  Jac = Jac + t(Jac)
    diag(Jac[1:(n*k),1:(n*k)]) = c(tcrossprod(-mu*(1-Z), reg^2) + 
                                     2*rep(beta[((n+1)*k+1):((n+2)*k)], each = n)*rowWeights)
  Jac

}
#library(Matrix )
# JacSpa= Matrix(Jac, sparse=TRUE)
# image(JacSpa)
#--------------------------------------#
dZipMeanCmat = function(beta, X, muMarg, k,p,n, Z, nLambda, colWeights, reg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  cMat = matrix(beta[1:(k*p)], byrow=TRUE, ncol=p, nrow=k)
  mu = exp(reg %*% cMat) * muMarg
  
  lambda1 = beta[(k*p+1):(k*p+k)] #Centering restrictions sum(colWeights*c_{ik}) = 0
  lambda2 = beta[(k*p+k+1):(k*p+2*k)] #normalization restrictions sum(colWeights*c^2_{ik}) = 1
  if(k==1){lambda3Mat=0}else{
  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(p+2)+1):length(beta)]
}
  score = c(t(
    crossprod(reg,(1-Z)*(X-mu)) + 
                        t(colWeights*t(lambda1 + lambda2*2*cMat + (lambda3Mat %*% cMat)))
    ))
  
  centers = colSums(colWeights*t(cMat))
  unitSums = colSums(colWeights*t(cMat^2))-1
  if(k==1){return(c(score,centers, unitSums))}
  orthogons = unlist(sapply(1:(k-1), function(K){
    vapply((K+1):k,FUN.VALUE=0, function(Kinner){
      sum(cMat[K,]*cMat[Kinner,]*colWeights)
    })
  }))
    return(c(score,centers, unitSums, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianCmat = function(beta, X, psis, rMat, colWeights, k, p, n, muMarg, Z, nLambda, reg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  cMat = matrix(beta[1:(k*p)], byrow=TRUE, ncol=p, nrow=k)
  mu = exp(reg %*% cMat) * muMarg

  lambda1 = beta[(k*p+1):(k*p+k)] #Centering restrictions sum(colWeights*r_{ik}) = 0
  lambda2 = beta[(k*p+k+1):(k*p+2*k)] #normalization restrictions sum(colWeights*r^2_{ik}) = 1
  if(k==1){lambda3Mat=0}else{
  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(p+2)+1):length(beta)]
  }
    Jac = matrix(0, nrow= p*k + nLambda, ncol=p*k + nLambda)
  #The suXmmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:(p*k),(p*k+1):((p+1)*k)] = c(rep(c(colWeights, rep.int(0,p*k)),k-1), colWeights)
  #Jac[1:(p*k),(p*k+1):((p+1)*k)] = sapply(1:k, function(K){c(rep(0,(K-1)*p),colWeights,rep(0,(k-K)*p))})
  Jac[1:(p*k),((p+1)*k+1):((p+2)*k)] = vapply(1:k,FUN.VALUE = vector("numeric", p*k), function(K){c(rep.int(0,(K-1)*p),colWeights*2 *cMat[K,],rep.int(0,(k-K)*p))})
if(k>1){
  for (K in 1:(k-1)){
      for(Kinner in (K+1):k){
        #dLag²/dr_{ik}dr_{ik'}
      diag(Jac[(p*(K-1)+1):(p*K),(p*(Kinner-1)+1):(p*Kinner)]) =  crossprod(reg[,K],(1-Z)*(-(mu* reg[,Kinner])))  +
         lambda3Mat[Kinner, K]*colWeights
      }
  }
    #dLag²/ds_{ik}dlambda_{3kk'}
    Jac[1:(p*k),((p+2)*k+1):(p*k+nLambda)] = unlist(sapply(1:(k-1), function(K){
        vapply((K+1):k,FUN.VALUE = vector("numeric",p*k), function(Kinner){
          c(rep.int(0, p*(K-1)), colWeights*cMat[Kinner,], rep.int(0, p*(Kinner-K-1)), colWeights*cMat[K,],rep.int(0, p*(k-Kinner)))
        })
      }))
}
  #Symmetrize
  Jac = Jac + t(Jac)

         diag(Jac[1:(p*k),1:(p*k)]) = c(crossprod(-mu*(1-Z), reg^2) + 
                                     2*rep(beta[((p+1)*k+1):((p+2)*k)], each = p)*colWeights)
  Jac
}

#--------------------------------------#

dZipMeanChi = function(beta, k, Z, reg, Zcol, nLambda){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  GZero = expit(t(t(arrayprod(reg, beta))+Zcol))
  
  vapply(1:k,FUN.VALUE=0, function(u){
  sum((Z-GZero)*reg[,,u])
  })
  
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianChi = function(beta, k, Z, reg, Zcol, nLambda){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  expZero = exp(t(t(arrayprod(reg, beta))+Zcol))
  tmp=expZero/(1+expZero)^2
  sapply(1:k, function(K){
    vapply(1:k,FUN.VALUE=0, function(Kinner){
    -sum(reg[,,Kinner]*reg[,,K]*tmp)
  })})
}

#--------------------------------------#
dZipMeanTmat = function(beta, reg, k,p,n, Z, Zcol, nLambda, rowWeights){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  tMat = matrix(beta[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)
  GZero = expit(t(t(tMat %*% reg)+Zcol))

  lambda1 = beta[(k*n+1):(k*n+k)] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[(k*n+k+1):(k*n+2*k)] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  if(k==1){lambda3Mat=0}else{
  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(n+2)+1):length(beta)]
}
  score = c(t(t(
    tcrossprod((Z-GZero), reg) + 
                         rowWeights*(t(lambda1 + lambda2*2*t(tMat)) + (tMat %*% lambda3Mat)
    ) )))
  
  centers = colSums(tMat*rowWeights)
  unitSums = colSums(tMat^2*rowWeights)-1
  if(k==1){return(c(score,centers, unitSums))}
  orthogons = unlist(sapply(1:(k-1), function(K){
    vapply((K+1):k,FUN.VALUE=0, function(Kinner){
      sum(tMat[,K]*tMat[,Kinner]*rowWeights)
    })
  }))
    return(c(score,centers, unitSums, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianTmat = function(beta, reg, k,p,n, Z, Zcol, nLambda, rowWeights){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  tMat = matrix(beta[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)

  muZero = exp(t(t(tMat %*% reg)+Zcol))

  lambda1 = beta[(k*n+1):(k*n+k)] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[(k*n+k+1):(k*n+2*k)] #normalization restrictions sum(abunds*r^2_{ik}) = 1

    if(k==1){lambda3Mat=0}else{
  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(n+2)+1):length(beta)]
    }
    Jac = matrix(0, nrow= n*k + nLambda, ncol=n*k + nLambda)
  #The symmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:(n*k),(n*k+1):((n+1)*k)] = c(rep.int(c(rowWeights, rep.int(0,n*k)),k-1), rowWeights) 
  #dLag²/dr_{ik}dlambda_{2k}
  Jac[1:(n*k),((n+1)*k+1):((n+2)*k)] = vapply(1:k,FUN.VALUE = vector("numeric",n*k), function(K){c(rep.int(0,(K-1)*n),2 *tMat[,K]*rowWeights,rep.int(0,(k-K)*n))})
  tmp=muZero/(1+muZero)^2
if(k>1){
  for (K in 1:(k-1)){
      for(Kinner in (K+1):k){
        #dLag²/dr_{ik}dr_{ik'}
      diag(Jac[(n*(K-1)+1):(n*K),(n*(Kinner-1)+1):(n*Kinner)]) = -crossprod(tmp,reg[Kinner,]*reg[K,]) + lambda3Mat[Kinner, K]*rowWeights
      }
  }

  #dLag²/dr_{ik}dlambda_{3kk'}
   Jac[1:(n*k),((n+2)*k+1):(n*k+nLambda)] = unlist(sapply(1:(k-1), function(K){
        vapply((K+1):k,FUN.VALUE = vector("numeric",n*k), function(Kinner){
          c(rep.int(0, n*(K-1)), tMat[,Kinner]*rowWeights, rep.int(0, n*(Kinner-K-1)), tMat[,K]*rowWeights,rep.int(0, n*(k-Kinner)))
        })
      }))
}
  #SXmmetrize
  Jac = Jac + t(Jac)
    diag(Jac[1:(n*k),1:(n*k)]) = c(-tcrossprod(tmp, reg^2) + 2*rep(beta[((n+1)*k+1):((n+2)*k)], each = n)*rowWeights)
    
  Jac
}
#--------------------------------------#
dZipMeanVmat = function(beta, reg, k,p,n, Z, Zcol, nLambda, colWeights){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  vMat = matrix(beta[1:(k*p)], byrow=TRUE, ncol=p, nrow=k)
  muZero = exp(t(t(reg %*% vMat)+Zcol))

  lambda1 = beta[(k*p+1):(k*p+k)] #Centering restrictions sum(colWeights*r_{ik}) = 0
  lambda2 = beta[(k*p+k+1):(k*p+2*k)] #normalization restrictions sum(colWeights*r^2_{ik}) = 1
  
    if(k==1){lambda3Mat=0}else{
  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(p+2)+1):length(beta)]
}
  score = c(t(
    crossprod(reg,(Z-muZero/(1+muZero))) + 
                        t(colWeights*t(lambda1 + lambda2*2*vMat + (lambda3Mat %*% vMat)))
    ))
  
  centers = colSums(colWeights*t(vMat))
  unitSums = colSums(colWeights*t(vMat^2))-1
  if(k==1){return(c(score,centers, unitSums))}
  orthogons = unlist(sapply(1:(k-1), function(K){
    vapply((K+1):k,FUN.VALUE=0, function(Kinner){
      sum(vMat[K,]*vMat[Kinner,]*colWeights)
    })
  }))
    return(c(score,centers, unitSums, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianVmat = function(beta, reg, k,p,n, Z, Zcol, nLambda, colWeights){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  vMat = matrix(beta[1:(k*p)], byrow=TRUE, ncol=p, nrow=k)
  muZero = exp(t(t(reg %*% vMat)+Zcol))

    if(k==1){lambda3Mat=0}else{
  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(p+2)+1):length(beta)]
    }
    Jac = matrix(0, nrow= p*k + nLambda, ncol=p*k + nLambda)
  #The suXmmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:(p*k),(p*k+1):((p+1)*k)] = c(rep(c(colWeights, rep.int(0,p*k)),k-1), colWeights)
  #Jac[1:(p*k),(p*k+1):((p+1)*k)] = sapply(1:k, function(K){c(rep(0,(K-1)*p),colWeights,rep(0,(k-K)*p))})
  Jac[1:(p*k),((p+1)*k+1):((p+2)*k)] = vapply(1:k,FUN.VALUE = vector("numeric", p*k), function(K){c(rep.int(0,(K-1)*p),colWeights*2 *vMat[K,],rep.int(0,(k-K)*p))})
tmp=muZero/(1+muZero)^2
if (k>1){
  for (K in 1:(k-1)){
      for(Kinner in (K+1):k){
        #dLag²/dr_{ik}dr_{ik'}
      diag(Jac[(p*(K-1)+1):(p*K),(p*(Kinner-1)+1):(p*Kinner)]) = -reg[,Kinner]%*%(reg[,K]*tmp) + lambda3Mat[Kinner, K]*colWeights
      }
  }
    #dLag²/ds_{ik}dlambda_{3kk'}
    Jac[1:(p*k),((p+2)*k+1):(p*k+nLambda)] = unlist(sapply(1:(k-1), function(K){
        vapply((K+1):k,FUN.VALUE = vector("numeric",p*k), function(Kinner){
          c(rep.int(0, p*(K-1)), colWeights*vMat[Kinner,], rep.int(0, p*(Kinner-K-1)), colWeights*vMat[K,],rep.int(0, p*(k-Kinner)))
        })
      }))
}
  #Symmetrize
  Jac = Jac + t(Jac)
    diag(Jac[1:(p*k),1:(p*k)]) = c(t( -crossprod(reg^2,tmp)) + colWeights *2*rep(beta[((p+1)*k+1):((p+2)*k)], each = p))
    
  Jac
}
#Weighing by abs or relabunds really doesn't matter, only chis and psis get more inflated and deflated
logit=function(x){log(x/(1-x))}

RCM_ZIP = function(X, k, rowWeights , colWeights, weightsChar, tol = 1e-3, maxItOut = 500, psiTol = 1e-4, chiTol=psiTol, verbose = TRUE, global ="dbldog", nleqslv.control=list(),method=c("Broyden"), twoCores=TRUE, convNorm = 2,  maxItMean = 20, maxItZeroes=20, ZIPRCM=NULL){
  
  # @param X: a nxp data matrix
  # @param k: a scalar, number of dimensions in the RC(M) model
  # @param tol(optional): a scalar, the relative convergende tolerance for the row scores and column scores parameters, defaults to 1e-3  
  # @param Psitol(optional): a scalar, the relative convergence tolerance for the psi parameters, defaults to 1e-4
  # @param maxItOut(optional): an integer, the maximum number of iteration in the outer loop, defaults to 50
  # @param libSizes(optional) : a vector of length n with (known) library sizes. If not provided, rowSums of x are used
  # @param verbose(optional): a boolean, should information on iterations be printed? Defaults to TRUE
  # @param method(optional): Method for jacobian estimation , see nleqslv. Defaults to Broyden. The difference with the newton method is that the Jacobian is not recalculated at every iteration
  # @param global(optional): global strategy for solving non-linear systems , see nleqslv
  # @param nleqslv.control: a list with control options, see nleqslv
  # @param lambdaRow: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers for the row scores
  # @param lambdaCol: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers for the column scores
  # @param rMatInit(optional): a nxk matrix with initial row scores. If not provided values from the singular value decomposition will be used as starting values
  # @param cMatInit(optional): a pxk matrix with initial column scores. If not provided values from the singular value decomposition will be used as starting values
  # @param psisInit(optional): a vector of length k with inital values for the importance parameters psi. If not provided values from the singular value decomposition will be used as starting values
  # @param dispFreq: a scalar, how many iterations the algorithm should wait before reestimationg the dispersions
  # @param convNorm: a scalar, the norm to use to determine convergence

  # @return A list with elements:
  # @return psis: a vector of length k with estimates for the importance parameters psi
  # @return thetas: a vector of length p with estimates for the overdispersion
  # @return rMat: a nxk matrix with estimated row scores
  # @return cMat: a pxk matrix with estimated column scores
  # @return converged: a boolean indicating if the algorithm converged
  # @return rowRec: a n x k x maxItOut array with a record of all rMat estimates through the iterations 
  # @return colRec: a k x p x maxItOut array with a record of all cMat estimates through the iterations
  # @return psiRec.: a k x maxItOut array with a record of all psi estimates through the iterations
  
  abunds = colSums(X)/sum(X)
  libSizes = rowSums(X)
  muMarg = outer(libSizes, abunds) #The marginals to be used as expectation
  n=NROW(X)
  p=NCOL(X)
  nLambda = 2*k+k*(k-1)/2
  
  if(!is.null(ZIPRCM)){
    for(i in c("rMat","cMat","psis","lambdaCol","lambdaRow", "lambdaRowZero","lambdaColZero","tMat","vMat","chis","Z","Zcol")){
      assign(i, ZIPRCM[[i]])
    }} else{
      #Depending on the weighting schemes, use other starting values
  svdX = switch(weightsChar,
                "marginalmarginal" = svd(diag(1/libSizes) %*% (X-muMarg) %*% diag(1/colSums(X))),
                "marginaluniform" = svd(diag(1/libSizes) %*% (X-muMarg)),
                "uniformmarginal" = svd((X-muMarg) %*% diag(1/colSums(X))),
                "uniformuniform" = svd(X-muMarg))
  rMat = svdX$u[,1:k,drop=FALSE]
  cMat = t( svdX$v[,1:k,drop=FALSE] )
  psis = svdX$d[1:k]
 
  #Redistribute some weight to fit the constraints 
  psis = c(psis *t(apply(cMat, 1, function(colS){
      sqrt(sum(colWeights * colS^2))
  })) * apply(rMat, 2, function(rowS){
      sqrt(sum(rowWeights * rowS^2))
  }))
  
#Normalize  
cMat = t(apply(cMat, 1, function(colS){
      colS/sqrt(sum(colWeights * colS^2))
  }))
rMat = apply(rMat, 2, function(rowS){
      rowS/sqrt(sum(rowWeights * rowS^2))
  })

Zcol = sapply(1:ncol(X), function(i){
 tmp= try(zeroinfl(X[,i]~log(libSizes)|1)$coef$zero, silent=TRUE)
 if(class(tmp)=="try-error"){
   tmp= try(zeroinfl(X[,i]~log(libSizes)|1, EM=TRUE)$coef$zero, silent=TRUE)
    if(class(tmp)=="try-error"){
   tmp = -Inf #If no zeroes => No chance on a structural zero
    }
 }
 tmp
})
  #Initial estimates for zeroes is also based on an svd
  Ezeroes = matrix(expit(Zcol), nrow=n, ncol=p, byrow=TRUE)#dpois(0, lambda = outer(libSizes, abunds)*exp(rMat %*%  (psis*cMat)))
  Xzeroes = X==0
  
  svdZero = svd(diag(sqrt(1/libSizes)) %*%(Xzeroes-Ezeroes)%*% diag(1/sqrt(colSums(X))))

  tMat = svdZero$u[,1:k, drop=FALSE]
  vMat = t(svdZero$v[,1:k, drop=FALSE])
  chis = svdZero$d[1:k]

#Redistribute some weight to fit the constraints 
chis = c(chis *t(apply(vMat, 1, function(colS){
      sqrt(sum(colWeights * colS^2))
  })) * apply(tMat, 2, function(rowS){
      sqrt(sum(rowWeights * rowS^2))
  }))
  
#Normalize  
vMat = t(apply(vMat, 1, function(colS){
      colS/sqrt(sum(colWeights * colS^2))
  }))
tMat = apply(tMat, 2, function(rowS){
      rowS/sqrt(sum(rowWeights * rowS^2))
  })
  
  lambdaRow = lambdaCol = lambdaColZero=lambdaRowZero = rep.int(0,nLambda)
  }
  
  iterOut = 1
  rowRec = rowRecZero = array(0,dim=c(NROW(X),k, maxItOut))
  colRec = colRecZero = array(0,dim=c(k,NCOL(X), maxItOut))
  psiRec = chiRec = matrix(0,ncol=maxItOut, nrow=k)

  ## 2) Propagation
  
  while((iterOut ==1) || ((iterOut <= maxItOut) && (!convergence)))
    {
    
  if(verbose && iterOut%%1 == 0){
  cat("\n","Outer Iteration", iterOut, "\n","\n")
    if(iterOut!=1){
  cat("Old psi-estimates: ", psiOld, "\n")
  cat("New psi-estimates: ", psis, "\n")
    }
  }
  ## 2)a. Store old parameters
  psiOld = psis
  rMatOld = rMat
  cMatOld = cMat
  
  chiOld = chis
  tMatOld = tMat
  vMatOld = vMat 

  #Expectation
  Z = EstepZIP (X, rMat, cMat, tMat, vMat, muMarg, k, Zcol, psis, chis)

  #Maximization
  Mlist = MstepZIP(Z = Z, X = X, rMat = rMat, cMat = cMat, tMat = tMat, vMat = vMat, k = k, n=n, p=p, Zcol=Zcol, psis = psis,chis = chis, twoCores=twoCores, tol = tol, psiTol = psiTol, chiTol = chiTol, convNorm = convNorm, global=global, nLambda = nLambda, nleqslv.control = nleqslv.control, lambdaCol = lambdaCol, lambdaRow=lambdaRow, lambdaColZero = lambdaColZero, lambdaRowZero = lambdaRowZero, maxItMean = maxItMean, maxItZeroes = maxItZeroes, muMarg=muMarg, colWeights = colWeights, rowWeights = rowWeights)

  for (x in names(Mlist))
    {assign(x,Mlist[[x]])}
  #cMatSE = cMatList$cMatSE
  rowRec[,, iterOut] = rMat
  colRec[,, iterOut] = cMat
  rowRecZero[,, iterOut] = tMat
  colRecZero[,, iterOut] = vMat
  psiRec[, iterOut] = psis
  chiRec[, iterOut] = chis
  
  ## 2)f. Change iterator
    iterOut = iterOut + 1
    
  ##Check convergence  (any numbered norm for row and column scores)
  convergence = ((iterOut <= maxItOut) && 
                   (all(abs(1-psis/psiOld) < psiTol)) &&
                   ((sum((1-rMatOld/rMat)^convNorm)/n)^(1/convNorm) < tol) && 
                   ((sum((1-cMatOld/cMat)^convNorm)/p)^(1/convNorm) < tol) ) && (all(abs(1-chis/chiOld) < chiTol)) &&  (sum(abs(1-tMat/tMatOld)^convNorm)/n)^(1/convNorm) < tol &&  (sum(abs(1-vMat/vMatOld)^convNorm)/p)^(1/convNorm) < tol 
} # END while-loop
  
  ## 3) Termination
  rownames(rMat) = rownames(X)
  colnames(cMat) = colnames(X)
  rownames(cMat) = colnames(rMat) = paste0("Dim",1:k)
  
  if(!convergence ){
    warning("Algorithm did not converge! Check for errors or consider changing tolerances or number of iterations")
  }
    return(list(converged = convergence,rMat=rMat, cMat=cMat, psis = psis, X=X,
                rowRec = rowRec, colRec = colRec, psiRec = psiRec, lambdaRow = lambdaRow, lambdaCol = lambdaCol, lambdaRowZero = lambdaRowZero, lambdaColZero = lambdaColZero, chis = chis, tMat = tMat, vMat = vMat, chiRec = chiRec, rowRecZero = rowRecZero, colRecZero = colRecZero, iter=iterOut-1, Z=Z, Zcol=Zcol))
}
```

## Zero-inflated Poisson: One by one

```{r Zero-inflated poisson 1B1, purl=TRUE}
#----------------------------#
#expit
expit=function(x){
  tmp = exp(x)/(1+exp(x))
  tmp[is.na(tmp)]=1 #Adjust for overflow
  tmp}

#-----------------------------------#
## A function to perform the E-step

EstepZIP_1B1  = function(X, rMat, cMat, tMat, vMat, muMarg,  Zcol, psis, chis){
  
  # @return: The values of Z
  expMu = exp(rMat %*% (cMat*psis)) * muMarg
  regZero =  Zcol + tMat %*% (vMat * chis)
  
  Z = X
  Z[X>0] = 0
  Z[X==0] = (1+exp(-regZero-expMu))[X==0]^(-1)
  Z
}
#-----------------------------------#

## A function to perform the M step: maximize the likelihoods. This will again be an iterative process, estimating the parameters step by step. estimation of poisson and zero-inflated part can occur independently, which opens up opportunities for parallelization.

MstepZIP_1B1 = function(Z, X, rMat, cMat, tMat, vMat,  k, muMarg,  Zcol, psis, chis, lambdaCol, lambdaRow, lambdaColZero, lambdaRowZero, nLambda, rowWeights, colWeights, rMatK, cMatK, vMatK, tMatK, twoCores=TRUE, tol=1e-3, psiTol = 1e-4, chiTol = psiTol, convNorm = 2, maxItMean=5, maxItZeroes= 20, n=n, p=p, global=global, nleqslv.control= nleqslv.control){
  
#Optimization of the mena and zero-inflated components are independent (see Lambert 1992), so fork here
if(twoCores){
resList = mclapply(mc.cores=2, c(meanEstZIP_1B1, ZIestZIP_1B1), function(fun){
  fun(X=X, rMat=rMat, cMat=cMat, tMat=tMat, chis=chis, vMat=vMat, Zcol=Zcol, lambdaCol=lambdaCol, lambdaRow=lambdaRow, lambdaRowZero=lambdaRowZero, lambdaColZero=lambdaColZero, psiTol=psiTol, chiTol=chiTol, tol=tol, convNorm = convNorm, nleqslv.control = nleqslv.control, global=global, nLambda=nLambda, k=k, Z=Z, muMarg=muMarg,n=n, p=p, psis=psis, maxItMean = maxItMean, maxItZeroes = maxItZeroes, rowWeights=rowWeights, colWeights=colWeights, rMatK = rMatK, cMatK = cMatK, tMatK = tMatK, vMatK = vMatK)
})
} else{
  resList = lapply(c(ZIestZIP_1B1), function(fun){ fun(X=X, rMat=rMat, cMat=cMat, tMat=tMat, chis=chis, vMat=vMat, Zcol=Zcol, lambdaCol=lambdaCol, lambdaRow=lambdaRow, lambdaRowZero=lambdaRowZero, lambdaColZero=lambdaColZero, psiTol=psiTol, chiTol=chiTol, tol=tol, convNorm = convNorm, nleqslv.control = nleqslv.control, global=global, nLambda=nLambda, k=k, Z=Z, muMarg=muMarg,n=n, p=p, psis=psis, maxItMean = maxItMean, maxItZeroes = maxItZeroes, rowWeights=rowWeights, colWeights=colWeights, rMatK = rMatK, cMatK = cMatK, tMatK = tMatK, vMatK = vMatK)})
  }#, meanEstZIP_1B1

return(unlist(resList, recursive=FALSE))
}
#--------------------------------------#

# A function to estimate the mean component of the ZIP model by 1B1
meanEstZIP_1B1 = function(X, rMat, cMat, Z, muMarg, k, global, nleqslv.control, tol, psiTol,lambdaCol, lambdaRow, convNorm,  nLambda, n, p, psis, rowWeights, colWeights, maxItMean = 10, maxItZeroes = 10, rMatK, cMatK,...){
 #Mean component
  
    iter = 1
  while((iter==1 || !converged) && iter<maxItMean){
    
  cat("Inner iteration(mean)", iter, "\n")
    
  psiOld = psis
  rMatOld = rMat
  cMatOld = cMat

  ## Psis
 cat("\n Estimating psis (k=",k,") \n")
    
  regPsi =  rMat %*% cMat

  psisSol = try(sort(abs(nleqslv(fn = dZipMeanPsi_1B1, x = psis, X=X, reg=regPsi, Z=Z, muMarg=muMarg, global=global, control = nleqslv.control, jac=ZipJacobianPsi_1B1)$x), decreasing=TRUE), silent=TRUE)
    if(class(psisSol)!="try-error") psis=psisSol
  
    ## Row scores
  cat("Estimating row scores mean \n")
  regRow = cMat*psis
  rMatSol = try(nleqslv(fn = dZipMeanRmat_1B1, x = c(rMat, lambdaRow),X=X, reg =regRow, muMarg=muMarg, n=n, k=k, global=global, control = nleqslv.control, jac=ZipJacobianRmat_1B1, Z=Z, nLambda=nLambda, rowWeights=rowWeights, rMatK = rMatK)$x, silent=TRUE)
  
  if(class(rMatSol)!="try-error"){
  rMat = matrix(rMatSol[1:n], byrow=FALSE, ncol=1, nrow=n)
  lambdaRow = rMatSol[(n+1):length(rMatSol)]
  }
  
  #Normalize (speeds up algorithm if previous step had not converged)
rMat =  rMat - sum(rMat * rowWeights)/sum(rowWeights) 

rMat = rMat/sqrt(sum(rowWeights * rMat^2))
  
  ## Column scores
  cat("Estimating column scores mean \n")
  regCol = t(t(rMat)*psis)
  cMatSol = try(nleqslv(fn = dZipMeanCmat_1B1, x = c(t(cMat), lambdaCol), X=X, reg=regCol, muMarg=muMarg, p=p, k=k, global=global, control = nleqslv.control, jac=ZipJacobianCmat_1B1, Z=Z, nLambda=nLambda, colWeights=colWeights, cMatK = cMatK)$x, silent=TRUE)
    if(class(cMatSol)!="try-error"){
  cMat = matrix(cMatSol[1:p], byrow=TRUE, nrow=1, ncol=p)
  lambdaCol = cMatSol[(p+1):length(cMatSol)]
    }
  
cMat = cMat - sum(cMat * colWeights)/sum(colWeights) 
cMat = cMat/sqrt(sum(colWeights * cMat^2))

    converged = all(abs(psiOld-psis) < psiTol) &&  (sum(abs(1-rMat/rMatOld)^convNorm))^(1/convNorm) < tol &&  (sum(abs(1-cMat/cMatOld)^convNorm))^(1/convNorm) < tol 
  iter = iter +1
  }
  
  return(list(cMat=cMat, rMat=rMat, iterMean = iter, psis=psis, convergedMean=converged, lambdaCol = lambdaCol, lambdaRow=lambdaRow))
}
#--------------------------------------#

# A function to estimate the zero inflated component of the ZIP model

ZIestZIP_1B1 = function(X, Z, muMarg, k, global, nleqslv.control, tol, chiTol, tMat, vMat, chis, Zcol,lambdaColZero, lambdaRowZero, convNorm,n, p, nLambda, psis, rowWeightsZeroNum, colWeightsZeroNum, rMatK, cMatK, tMatK, vMatK, maxItMean = 10, maxItZeroes = 10,...){
  
      iter = 1
  while((iter==1 || !converged) && iter < maxItZeroes){
   chiOld = chis
  tMatOld = tMat
  vMatOld = vMat
  
    cat("Inner iteration(zeroes)", iter, "\n")

# Chis
      cat("Estimating chis (zeroes) \n")
  regChis =  tMat %*% vMat

  chisSol = try(sort(abs(nleqslv(fn = dZipMeanChi_1B1, x = chis, reg=regChis, Z=Z, global=global, control = nleqslv.control, Zcol=Zcol, jac=ZipJacobianChi_1B1)$x), decreasing=TRUE), silent=TRUE)
  if(!inherits(chisSol,"try-error")){
    chis=chisSol
  }
  
    ## Row scoers
    cat("Estimating row scores zeroes \n")
    regRowZero = vMat*chis
    tMatSol = try(nleqslv(fn = dZipMeanTmat_1B1, x = c(tMat, lambdaRowZero),  n=n,k=k, reg=regRowZero, global=global, control = nleqslv.control, Zcol=Zcol, jac=ZipJacobianTmat_1B1, Z=Z, nLambda=nLambda, rowWeights=rowWeightsZeroNum, tMatK = tMatK)$x, silent=TRUE)
  if(!inherits(tMatSol,"try-error")){
    tMat = matrix(tMatSol[1:n], byrow=FALSE, ncol=1, nrow=n)
    lambdaRowZero = tMatSol[(n+1):(n+nLambda)]
      }
    
    ## Column scores
    cat("Estimating column scores zeroes \n")
    regColZero = tMat*chis
    vMatSol = try(nleqslv(fn = dZipMeanVmat_1B1, x = c(t(vMat), lambdaColZero), reg=regColZero, p=p,k=k, global=global, control = nleqslv.control, Zcol=Zcol, jac=ZipJacobianVmat_1B1, Z=Z, nLambda=nLambda, colWeights=colWeightsZeroNum, vMatK = vMatK)$x, silent=TRUE)
  if(!inherits(vMatSol,"try-error")){
    vMat = matrix(vMatSol[1:p], byrow=TRUE, nrow=1, ncol=p)
    lambdaColZero = vMatSol[(p+1):(p+nLambda)]
          }

  converged = all ((chiOld-chis) < chiTol) &&  (sum(abs(1-tMat/tMatOld)^convNorm))^(1/convNorm) < tol &&  (sum(abs(1-vMat/vMatOld)^convNorm))^(1/convNorm) < tol 
  iter = iter +1
}
  return(list(vMat=vMat, tMat=tMat, iterZI = iter, chis=chis, convergedZI=converged, lambdaColZero=lambdaColZero, lambdaRowZero=lambdaRowZero))
}

#All matrices X are considered to be nxp, i.e. samples are rows and taxa are columns

#--------------------------------------#
dZipMeanPsi_1B1 = function(beta, X, muMarg, Z, reg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  mu = exp(reg* beta) * muMarg
  sum((1 - Z)*(X - mu)*reg)
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianPsi_1B1 = function(beta, X, reg, muMarg,Z){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  mu = exp(reg* beta) * muMarg
  sum(mu*reg^2*(Z-1))
}

#--------------------------------------#
dZipMeanRmat_1B1 = function(beta, X, reg, muMarg, n, k, Z, nLambda, rowWeights, rMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  rMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)
  mu = exp(rMat %*% reg) * muMarg
  
  lambda1 = beta[n+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[n+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(n+3):length(beta)]}

  score = tcrossprod(reg ,(1-Z)*(X-mu)) + c(rowWeights*(lambda1 + lambda2*2*rMat + rMatK %*% lambda3))

  center = sum(rMat*rowWeights)
  unitSum = sum(rMat^2*rowWeights)-1
  if(k==1){return(c(score,center, unitSum))}
  orthogons = apply(rMatK, 2, function(x){
    sum(rMat*x*rowWeights)
  })
      return(c(score,center, unitSum, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianRmat_1B1 = function(beta, X, reg, muMarg, n,k, nLambda, Z, rowWeights, rMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  rMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)
  mu = exp(rMat %*% reg) * muMarg

  Jac = matrix(0, nrow= n + nLambda, ncol=n + nLambda)
  #The symmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:n, n+1] = rowWeights
  #dLag²/dr_{ik}dlambda_{2k}
  Jac[1:n, n+2] = 2 *rMat*rowWeights
  
  if(k>1){
    Jac[1:n,(n+3):(n+nLambda)] = apply(rMatK, 2, function(x){rowWeights*x})
  }
  #Symmetrize
  Jac = Jac + t(Jac)
    diag(Jac[1:n,1:n]) = c(tcrossprod(-mu*(1-Z), reg^2) + 2*rowWeights*beta[n+2])
  Jac

}
#--------------------------------------#
dZipMeanCmat_1B1 = function(beta, X, muMarg, p,k, Z, nLambda, colWeights, reg, cMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  cMat = matrix(beta[1:p], byrow=TRUE, ncol=p, nrow=1)
  mu = exp(reg %*% cMat) * muMarg
  
  lambda1 = beta[p+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[p+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(p+3):length(beta)]}
  
  score = crossprod(reg,(1-Z)*(X-mu)) + colWeights*(lambda1 + lambda2*2*cMat + lambda3 %*% cMatK)
  
  center = sum(colWeights*cMat)
  unitSum = sum(colWeights*cMat^2)-1
  if(k==1){return(c(score,center, unitSum))}
  orthogons = apply(cMatK,1,function(x){
      sum(cMat*x*colWeights)
  })
    return(c(score,center, unitSum, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianCmat_1B1 = function(beta, X, psis, rMat, colWeights, k, p, muMarg, Z, nLambda, reg, cMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  cMat = matrix(beta[1:p], byrow=TRUE, ncol=p, nrow=1)
  mu = exp(reg %*% cMat) * muMarg

Jac = matrix(0, nrow= p + nLambda, ncol = p + nLambda)
  #The suXmmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:p,(p+1)] = colWeights
  #Jac[1:(p*k),(p*k+1):((p+1)*k)] = sapply(1:k, function(K){c(rep(0,(K-1)*p),colWeights,rep(0,(k-K)*p))})
  Jac[1:p,p+2] = colWeights*2 *cMat
  
    #dLag²/ds_{ik}dlambda_{3kk'}
  if(k>1){
    Jac[1:p,(p+3):(p+nLambda)] = apply(cMatK, 1,function(x){
      colWeights*x
    })
  }
  #Symmetrize
  Jac = Jac + t(Jac)

  diag(Jac[1:p,1:p]) = c(crossprod(-mu*(1-Z), reg^2)) + 2*beta[p+2]*colWeights
  Jac
}

#--------------------------------------#

dZipMeanChi_1B1 = function(beta, Z, reg, Zcol, nLambda){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  GZero = expit(reg* beta+Zcol)
  sum((Z-GZero)*reg)
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianChi_1B1 = function(beta, Z, reg, Zcol, nLambda){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  expZero = exp(reg* beta+Zcol)
  tmp=expZero/(1+expZero)^2
  -sum(reg^2*tmp)
}

#--------------------------------------#
dZipMeanTmat_1B1 = function(beta, reg, k,n, Z, Zcol, nLambda, rowWeights, tMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  tMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)
  muZero = expit(tMat %*% reg+Zcol)
  
  lambda1 = beta[n+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[n+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(n+3):length(beta)]}
  score = 
    tcrossprod((Z-muZero), reg) + c(rowWeights*(lambda1 + lambda2*2*tMat + tMatK %*% lambda3))

  
  center = sum(tMat*rowWeights)
  unitSum = sum(tMat^2*rowWeights)-1
  if(k==1){return(c(score,center, unitSum))}
  orthogons = apply(tMatK, 2, function(x){
    sum(tMat*x*rowWeights)
  })
    return(c(score,center, unitSum, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianTmat_1B1 = function(beta, reg, k, n, Z, Zcol, nLambda, rowWeights, tMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param k: a scalar, dimension of the RC solution

  tMat = matrix(beta[1:n], byrow=FALSE, ncol=1, nrow=n)
  muZero = expit(tMat %*% reg+Zcol)

  Jac = matrix(0, nrow= n + nLambda, ncol= n + nLambda)
  #The suymmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:n, n+1] = rowWeights
  #dLag²/dr_{ik}dlambda_{2k}
  Jac[1:n, n+2] = 2*tMat*rowWeights
  tmp=muZero/(1+muZero)^2
  #dLag²/dr_{ik}dlambda_{3kk'}
  if(k>1){
    Jac[1:n,(n+3):(n+nLambda)] = apply(tMatK, 2, function(x){rowWeights*x})
  }
  #Symmetrize
  Jac = Jac + t(Jac)
    diag(Jac[1:n,1:n]) = c(-tcrossprod(tmp, reg^2) + 2*beta[n+2]*rowWeights)
    
  Jac
}
#--------------------------------------#
dZipMeanVmat_1B1 = function(beta, reg, k,p, Z, Zcol, nLambda, colWeights, vMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  vMat = matrix(beta[1:p], byrow=TRUE, ncol=p, nrow=1)
  muZero = exp(reg %*% vMat +Zcol)

  lambda1 = beta[p+1] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[p+2] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3 = if(k==1){0} else {beta[(p+3):length(beta)]}
  
  score = 
    crossprod(reg,(Z-muZero/(1+muZero))) + colWeights*(lambda1 + lambda2*2*vMat + (lambda3 %*% vMatK))
  
  center = sum(colWeights*vMat)
  unitSum = sum(colWeights*vMat^2)-1
  if(k==1){return(c(score,center, unitSum))}
  orthogons = apply(vMatK, 1,function(x){
    sum(x*vMat*colWeights)})
  
    return(c(score, center, unitSum, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZipJacobianVmat_1B1 = function(beta, reg, k, p, Z, Zcol, nLambda, colWeights, vMatK){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  vMat = matrix(beta[1:p], byrow=TRUE, ncol=p, nrow=1)
  muZero = exp(reg %*% vMat +Zcol)

    Jac = matrix(0, nrow= p + nLambda, ncol=p + nLambda)
  #The suXmmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:p,(p+1)] = colWeights
  #dLag²/dr_{ik}dlambda_{2k}
  Jac[1:p,p+2] = colWeights*2 *vMat
tmp=muZero/(1+muZero)^2
  if(k>1){
    Jac[1:p,(p+3):(p+nLambda)] = apply(vMatK, 1,function(x){
      colWeights*x
    })
  }
  #Symmetrize
  Jac = Jac + t(Jac)
    diag(Jac[1:p,1:p]) = c(t( -crossprod(reg^2,tmp)) + colWeights *2*beta[p+2])
    
  Jac
}
#Weighing by abs or relabunds really doesn't matter, only chis and psis get more inflated and deflated
logit=function(x){log(x/(1-x))}

RCM_ZIP_1B1 = function(X, k, rowWeights, colWeights, weightsChar, tol = 1e-3, maxItOut = 500, psiTol = 1e-4, chiTol=psiTol, verbose = TRUE, global ="dbldog", nleqslv.control=list(), method=c("Broyden"), twoCores=FALSE, convNorm = 2,  maxItMean = 20, maxItZeroes=30, ZIPRCM=NULL){
  #load("/home/stijn/PhD/Biplots/toyDataSig.RData"); X=syntNBSigunifmarg_1B1_3$X
  #tol = 1e-3; maxItOut = 500; psiTol = 1e-4; chiTol=psiTol; verbose = TRUE; global ="dbldog"; nleqslv.control=list();method=c("Broyden"); twoCores=TRUE; convNorm = 2;  maxItMean = 20; maxItZeroes=20; ZIPRCM=NULL;n=nrow(X);n=nrow(X);rowWeights = rep(1/n,n);colWeights = colSums(X); weightsChar = "uniformmarginal"

  # @param X: a nxp data matrix
  # @param k: a scalar, number of dimensions in the RC(M) model
  # @param tol(optional): a scalar, the relative convergende tolerance for the row scores and column scores parameters, defaults to 1e-3  
  # @param Psitol(optional): a scalar, the relative convergence tolerance for the psi parameters, defaults to 1e-4
  # @param maxItOut(optional): an integer, the maximum number of iteration in the outer loop, defaults to 50
  # @param libSizes(optional) : a vector of length n with (known) library sizes. If not provided, rowSums of x are used
  # @param verbose(optional): a boolean, should information on iterations be printed? Defaults to TRUE
  # @param method(optional): Method for jacobian estimation , see nleqslv. Defaults to Broyden. The difference with the newton method is that the Jacobian is not recalculated at every iteration
  # @param global(optional): global strategy for solving non-linear systems , see nleqslv
  # @param nleqslv.control: a list with control options, see nleqslv
  # @param lambdaRow: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers for the row scores
  # @param lambdaCol: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers for the column scores
  # @param rMatInit(optional): a nxk matrix with initial row scores. If not provided values from the singular value decomposition will be used as starting values
  # @param cMatInit(optional): a pxk matrix with initial column scores. If not provided values from the singular value decomposition will be used as starting values
  # @param psisInit(optional): a vector of length k with inital values for the importance parameters psi. If not provided values from the singular value decomposition will be used as starting values
  # @param dispFreq: a scalar, how many iterations the algorithm should wait before reestimationg the dispersions
  # @param convNorm: a scalar, the norm to use to determine convergence

  # @return A list with elements:
  # @return psis: a vector of length k with estimates for the importance parameters psi
  # @return thetas: a vector of length p with estimates for the overdispersion
  # @return rMat: a nxk matrix with estimated row scores
  # @return cMat: a pxk matrix with estimated column scores
  # @return converged: a boolean indicating if the algorithm converged
  # @return rowRec: a n x k x maxItOut array with a record of all rMat estimates through the iterations 
  # @return colRec: a k x p x maxItOut array with a record of all cMat estimates through the iterations
  # @return psiRec.: a k x maxItOut array with a record of all psi estimates through the iterations
  
  abunds = colSums(X)/sum(X)
  libSizes = rowSums(X)
  muMarg = outer(libSizes, abunds) #The marginals to be used as expectation
  n=NROW(X)
  p=NCOL(X)
  
  rowRec = rowRecZeroes = array(0,dim=c(n,k, maxItOut))
  colRec = colRecZeroes = thetaRec = array(0,dim=c(k,p, maxItOut))
  psiRec = matrix(0, nrow=k,ncol=maxItOut)
  convergence = rep(FALSE, k)
  iterOut = rep(1,k)
  
  #If previous fit provided with higher or equal dimension, stop here
  if((!is.null(ZIPRCM)) ){
    if(ZIPRCM$fit != "RCM_ZIP_1B1"){
      stop("Fit provided is not of same type as the one requested! \n")
    } else if((k <= ZIPRCM$k)) {
      # stop("Fit provided is already of the required dimension or higher! \n")
    } else{}
  #Otherwise try to use intelligent starting values
  }
  
  if(!is.null(ZIPRCM)){
    for(i in c("rMat","cMat","psis","lambdaCol","lambdaRow", "lambdaRowZero","lambdaColZero","tMat","vMat","chis","Z","Zcol")){
      assign(i, ZIPRCM[[i]])
    }} else{
      #Depending on the weighting schemes, use other starting values
   svdX = svd(diag(1/libSizes) %*% (X-muMarg) %*% diag(1/colSums(X)))#switch(weightsChar,
#                 "marginalmarginal" = svd(diag(1/libSizes) %*% (X-muMarg) %*% diag(1/colSums(X))),
#                 "marginaluniform" = svd(diag(1/libSizes) %*% (X-muMarg)),
#                 "uniformmarginal" = svd((X-muMarg) %*% diag(1/colSums(X))),
#                 "uniformuniform" = svd(X-muMarg))
  rMat = svdX$u[,1:k,drop=FALSE]
  cMat = t(svdX$v[,1:k,drop=FALSE])
  psis = svdX$d[1:k]
 
  #Redistribute some weight to fit the constraints 
  psis = c(psis *t(apply(cMat, 1, function(colS){
      sqrt(sum(colWeights * colS^2))
  })) * apply(rMat, 2, function(rowS){
      sqrt(sum(rowWeights * rowS^2))
  }))
  psis = rep(0,k) #DELETE ME!
  
#Normalize  
cMat = t(apply(cMat, 1, function(colS){
      colS/sqrt(sum(colWeights * colS^2))
  }))
rMat = apply(rMat, 2, function(rowS){
      rowS/sqrt(sum(rowWeights * rowS^2))
  })

Zcol = sapply(1:p, function(i){
 tmp= try(zeroinfl(X[,i]~log(libSizes)|1)$coef$zero, silent=TRUE)
 if(class(tmp)=="try-error"){
   tmp= try(zeroinfl(X[,i]~log(libSizes)|1, EM=TRUE)$coef$zero, silent=TRUE)
    if(class(tmp)=="try-error"){
   tmp = -Inf #If no zeroes => No chance on a structural zero
    }
 }
 rep(tmp,n)
})
dimnames(Zcol) = dimnames(X)
  #Initial estimates for zeroes is also based on an svd
  Ezeroes = matrix(expit(Zcol), nrow=n, ncol=p, byrow=TRUE)#dpois(0, lambda = outer(libSizes, abunds)*exp(rMat %*%  (psis*cMat)))
  Xzeroes = X==0
  
  svdZero = svd(diag(sqrt(1/libSizes)) %*%(Xzeroes-Ezeroes)%*% diag(1/sqrt(colSums(X))))

  tMat = svdZero$u[,1:k, drop=FALSE]
  vMat = t(svdZero$v[,1:k, drop=FALSE])
  chis = svdZero$d[1:k]

#Redistribute some weight to fit the constraints 
chis = c(chis *t(apply(vMat, 1, function(colS){
      sqrt(sum(colWeights * colS^2))
  })) * apply(tMat, 2, function(rowS){
      sqrt(sum(rowWeights * rowS^2))
  }))
  
#Normalize  
vMat = t(apply(vMat, 1, function(colS){
      colS/sqrt(sum(colWeights * colS^2))
  }))
tMat = apply(tMat, 2, function(rowS){
      rowS/sqrt(sum(rowWeights * rowS^2))
  })
  
  lambdaRow = lambdaCol = lambdaColZero=lambdaRowZero = rep.int(0,2*k+k*(k-1)/2)
    }
  
  rowRec = rowRecZero = array(0,dim=c(NROW(X),k, maxItOut))
  colRec = colRecZero = array(0,dim=c(k,NCOL(X), maxItOut))
  psiRec = chiRec = matrix(0,ncol=maxItOut, nrow=k)
  
   if(!is.null(ZIPRCM)){ #If fit provided, replace lower dimension starting values
   Kprev = ZIPRCM$k
   rMat[,1:Kprev] = ZIPRCM$rMat
   rowRec[,1:Kprev,] = ZIPRCM$rowRec
   cMat[1:Kprev,] = ZIPRCM$cMat
   colRec[1:Kprev,,] = ZIPRCM$colRec
   psis[1:Kprev] = ZIPRCM$psis
   psiRec[1:Kprev,] = ZIPRCM$psiRec
   lambdaCol[1:(Kprev*(2+(Kprev-1)/2))] = ZIPRCM$lambdaCol
   lambdaRow[1:(Kprev*(2+(Kprev-1)/2))] = ZIPRCM$lambdaRow
   tMat[,1:Kprev] = ZIPRCM$tMat
   rowRecZero[,1:Kprev,] = ZIPRCM$rowRecZero
   vMat[1:Kprev,] = ZIPRCM$vMat
   colRecZero[1:Kprev,,] = ZIPRCM$colRecZero
   chis[1:Kprev] = ZIPRCM$chis
   chiRec[1:Kprev,] = ZIPRCM$chiRec
   lambdaColZero[1:(Kprev*(2+(Kprev-1)/2))] = ZIPRCM$lambdaColZero
   lambdaRowZero[1:(Kprev*(2+(Kprev-1)/2))] = ZIPRCM$lambdaRowZero
   convergence[1:Kprev] = ZIPRCM$converged
   iterOut[1:Kprev] = ZIPRCM$iter
   Zcol = ZIPRCM$Zcol
 }

  minK = ifelse(is.null(ZIPRCM),1,Kprev+1)
  for (KK in minK:k){
  
  cat("Dimension" ,KK, "is being esimated \n")
    
  #Modify offsets if needed
  if(KK>1){
    muMarg = muMarg * exp(rMat[,(KK-1), drop=FALSE] %*% (cMat[(KK-1),, drop=FALSE]*psis[(KK-1)]))
    Zcol = Zcol + tMat[,(KK-1), drop=FALSE] %*% (vMat[(KK-1),, drop=FALSE]*chis[(KK-1)])
  }
    #A lambda parameter
    nLambda = KK + 1
    
    #The location of the lambda parameters
    idK = seq_k(KK)

    ## 2) Propagation
  
    while((iterOut[KK] ==1) || ((iterOut[KK] <= maxItOut) && (!convergence[KK])))
    {
    
  if(verbose && iterOut[KK]%%1 == 0){
  cat("\n","Outer Iteration", iterOut[KK], "\n","\n")
    if(iterOut[KK]!=1){
  cat("Old psi-estimates: ", psiOld, "\n")
  cat("New psi-estimates: ", psis[KK], "\n")
    }
  }
  ## 2)a. Store old parameters
  psiOld = psis[KK]
  rMatOld = rMat[,KK]
  cMatOld = cMat[KK,]
  
  chiOld = chis[KK]
  tMatOld = tMat[,KK]
  vMatOld = vMat[KK,]

  #Expectation
  Z = EstepZIP_1B1 (X, rMat = rMat[,KK, drop=FALSE], cMat = cMat[KK,, drop=FALSE], tMat = tMat[,KK, drop=FALSE], vMat = vMat[KK,, drop=FALSE], muMarg = muMarg, Zcol=Zcol, psis = psis[KK], chis =chis[KK]) #FIX ME Zcol dimensions do not match
  
   #Maximization
  Mlist = MstepZIP_1B1(Z = Z, X = X, rMat = rMat[,KK, drop=FALSE], cMat = cMat[KK,, drop=FALSE], tMat = tMat[,KK, drop=FALSE], vMat = vMat[KK,, drop=FALSE], k = KK, n=n, p=p, Zcol=Zcol, psis = psis[KK],chis = chis[KK], twoCores=twoCores, tol = tol, psiTol = psiTol, chiTol = chiTol, convNorm = convNorm, global=global, nLambda = nLambda, nleqslv.control = nleqslv.control, lambdaCol = lambdaCol[idK], lambdaRow=lambdaRow[idK], lambdaColZero = lambdaColZero[idK], lambdaRowZero = lambdaRowZero[idK], maxItMean = maxItMean, maxItZeroes = maxItZeroes, muMarg=muMarg, colWeights = colWeights, rowWeights = rowWeights, rMatK = rMat[,1:(KK-1), drop=FALSE], cMatK = cMat[1:(KK-1),, drop=FALSE], tMatK = tMat[,1:(KK-1), drop=FALSE], vMatK = vMat[1:(KK-1),, drop=FALSE])

  #Assign outcomes to tracking vectors and to this environment
#    lambdaCol[idK] = Mlist$lambdaCol
#    lambdaRow[idK] = Mlist$lambdaRow
   lambdaColZero[idK] = Mlist$lambdaColZero
   lambdaRowZero[idK] = Mlist$lambdaRowZero

#   rowRec[,KK, iterOut[KK]] = rMat[,KK] = Mlist$rMat
#   colRec[KK,, iterOut[KK]] = cMat[KK,] = Mlist$cMat
  rowRecZero[,KK, iterOut[KK]] = tMat[,KK] = Mlist$tMat
  colRecZero[KK,, iterOut[KK]] = vMat[KK,] = Mlist$vMat
  # psiRec[KK, iterOut[KK]] = psis[KK] = Mlist$psis
  chiRec[KK, iterOut[KK]] = chis[KK] = Mlist$chis
  
  ## 2)f. Change iterator
    iterOut[KK] = iterOut[KK] + 1
    
  ##Check convergence  (any numbered norm for row and column scores)
  convergence[KK] = (iterOut[KK] <= maxItOut) && 
#                     (all(abs(1-psis/psiOld) < psiTol)) &&
#                     ((sum((1-rMatOld/rMat)^convNorm)/n)^(1/convNorm) < tol) && 
#                     ((sum((1-cMatOld/cMat)^convNorm)/p)^(1/convNorm) < tol) ) && 
                    (all(abs(1-chis/chiOld) < chiTol)) &&  
                    (sum(abs(1-tMat/tMatOld)^convNorm)/n)^(1/convNorm) < tol &&
                    (sum(abs(1-vMat/vMatOld)^convNorm)/p)^(1/convNorm) < tol 
} # END while-loop until convergence
  } # END for-loop over dimensions
  
  ## 3) Termination
  rownames(rMat) = rownames(X)
  colnames(cMat) = colnames(X)
  rownames(cMat) = colnames(rMat) = paste0("Dim",1:k)
  
  if(!convergence ){
    warning("Algorithm did not converge! Check for errors or consider changing tolerances or number of iterations")
  }
    return(list(converged = convergence,rMat=rMat, cMat=cMat, psis = psis, X=X,
                rowRec = rowRec, colRec = colRec, psiRec = psiRec, lambdaRow = lambdaRow, lambdaCol = lambdaCol, lambdaRowZero = lambdaRowZero, lambdaColZero = lambdaColZero, chis = chis, tMat = tMat, vMat = vMat, chiRec = chiRec, rowRecZero = rowRecZero, colRecZero = colRecZero, iter=iterOut-1, Z=Z, Zcol=Zcol, fit="RCM_ZIP_1B1"))
}
```

## Zero-inflated negative binomial

```{r ZINB, purl=TRUE}
#-----------------------------------#
## A function to perform the E-step

EstepNB  = function(X, rMat, cMat, tMat, vMat, muMarg, k,  Zcol, psis, chis, thetas){
  
  # @return: The values of Z
  expMu = exp(rMat %*% (cMat*psis)) * muMarg
  pZero =  expit(t(Zcol + t(tMat %*% (vMat * chis))))

  Z = X
  Z[X>0] = 0
  thetaMat=matrix(thetas, ncol=ncol(X), nrow=nrow(X), byrow=TRUE)
  d0=dnbinom(0,mu=expMu, size=thetaMat)
  Z[X==0] = (pZero/((1-pZero)*d0 + pZero))[X==0]
  Z
}
#-----------------------------------#

## A function to perform the M step: maximize the likelihoods. This will again be an iterative process, estimating the parameters step by step. estimation of poisson and zero-inflated part can occur independently, which opens up opportunities for parallelization.

MstepNB = function(Z, X, rMat, cMat, tMat, vMat, muMarg, k,  Zcol, psis, chis, lambdaCol, lambdaRow, lambdaColZero, lambdaRowZero, twoCores=TRUE, tol=1e-3, psiTol = 1e-4, chiTol = psiTol, convNorm = 2 , maxItMean=20 , maxItZeroes= 20,n, p, global=global, nleqslv.control= nleqslv.control, nLambda, thetas, dispFreq,rowWeights, colWeights){
  
#Optimization of the mena and zero-inflated components are independent (see Lambert 1992), so fork here
resList = mclapply(mc.cores=1+twoCores, c(meanEstZINB, ZIestNB), function(fun){
  fun(X=X, rMat=rMat, cMat=cMat, tMat=tMat, chis=chis, vMat=vMat, Zcol=Zcol, lambdaCol=lambdaCol, lambdaRow=lambdaRow, lambdaRowZero=lambdaRowZero, lambdaColZero=lambdaColZero, psiTol=psiTol, chiTol=chiTol, tol=tol, convNorm = convNorm, nleqslv.control = nleqslv.control, global=global, nLambda=nLambda, k=k, Z=Z, muMarg=muMarg,n=n, p=p, psis=psis, maxItMean = maxItMean, maxItZeroes = maxItZeroes, thetas=thetas, dispFreq=dispFreq, rowWeights = rowWeights, colWeights=colWeights)
})

return(unlist(resList, recursive=FALSE))
}
#--------------------------------------#

# A function to estimate the mean component of the ZIP model
meanEstZINB = function(X, rMat, cMat, Z, muMarg,  k, global, nleqslv.control, tol, psiTol, thetas, lambdaCol, lambdaRow, convNorm, dispFreq, nLambda,n,p,psis, maxItMean = 10, maxItZeroes = 10,rowWeights, colWeights,...){
 #Mean component
  
    iter = 1
  while((iter==1 || !converged) && iter<maxItMean){
    
  cat("Inner iteration(mean)", iter, "\n")
    
  psiOld = psis
  rMatOld = rMat
  cMatOld = cMat

  cat("estimating overdispersions \n")
  if(iter==1 | iter %% dispFreq ==0){ #Again too slow and unnecessary to reestimate overdispersions every time
  thetasTry = try(estDisp(X=X, cMat=cMat, rMat=rMat, muMarg=muMarg, psis=psis,k=k, dispWeights=t(1-Z)), silent=TRUE)
  if(class(thetasTry)!="try-error") thetas=thetasTry
  }

  cat("Estimating row scores mean \n")
  regRows = cMat*psis
  rMatSol = try(nleqslv(fn = dZinbMeanRmat, x = c(rMat, lambdaRow), X=X, reg =regRows,muMarg=muMarg, k=k, n=n, p=p, global=global, control = nleqslv.control, jac=ZinbJacobianRmat, Z=Z, nLambda=nLambda, thetas=thetas, rowWeights=rowWeights)$x, silent=TRUE)
  if(class(rMatSol)!="try-error"){
  rMat = matrix(rMatSol[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)
  lambdaRow = rMatSol[(k*n+1):length(rMatSol)]
  }
  
  ## Column scores
  cat("Estimating column scores mean \n")
  regCols = t(t(rMat)*psis)
  cMatSol = try(nleqslv(fn = dZinbMeanCmat, x = c(t(cMat), lambdaCol),reg=regCols, X=X, muMarg=muMarg, k=k, n=n, p=p, global=global, control = nleqslv.control, jac=ZinbJacobianCmat, Z=Z, nLambda=nLambda, thetas=thetas, colWeights=colWeights)$x, silent=TRUE)
    if(class(cMatSol)!="try-error"){
  cMat = matrix(cMatSol[1:(k*p)], byrow=TRUE, nrow=k, ncol=p)
  lambdaCol = cMatSol[(k*p+1):length(cMatSol)]
    }

    cat("\n Estimating psis (k=",k,") \n")
regPsis = sapply(1:k, simplify="array", function(K){outer(rMat[,K], cMat[K,]) })
  psisSol = try(sort(abs(nleqslv(fn = dZinbMeanPsi, reg=regPsis,x = psis, X=X, Z=Z, muMarg=muMarg, k=k, global=global, control = nleqslv.control, jac=ZinbJacobianPsi, thetas=thetas)$x), decreasing=TRUE), silent=TRUE)
    if(class(psisSol)!="try-error") psis=psisSol

    converged = all(abs(psiOld-psis) < psiTol) &&  (sum(abs(1-rMat/rMatOld)^convNorm))^(1/convNorm) < tol &&  (sum(abs(1-cMat/cMatOld)^convNorm))^(1/convNorm) < tol 
  iter = iter +1
  }
  
  return(list(cMat=cMat, rMat=rMat, iterMean = iter, psis=psis, convergedMean=converged, lambdaCol = lambdaCol, lambdaRow=lambdaRow))
}
#--------------------------------------#

# A function to estimate the zero inflated component of the ZIP model

ZIestNB = function(X, Z, k, global, nleqslv.control, tol,  chiTol, tMat, vMat, chis, Zcol, lambdaColZero, lambdaRowZero,convNorm,n, p,  nLambda,rowWeights, colWeights, maxItMean = 10, maxItZeroes = 10, thetas,...){
  
      iter = 1
  while((iter==1 || !converged) && iter<maxItZeroes){
   chiOld = chis
  tMatOld = tMat
  vMatOld = vMat
  
    cat("Inner iteration(zeroes)", iter, "\n")

  # nleqslv.control$trace=FALSE
  # Zero component
    ## Chis
      cat("Estimating chis (zeroes) \n")
      regChis = sapply(1:k, simplify="array", function(K){outer(tMat[,K], vMat[K,])})
  chisSol = try(sort(abs(nleqslv(fn = dZinbMeanChi, reg=regChis, x = chis, Z=Z, k=k, global=global, control = nleqslv.control, Zcol=Zcol, jac=ZinbJacobianChi)$x), decreasing=TRUE), silent=TRUE)
  if(!inherits(chisSol,"try-error")){
    chis=chisSol
  }
  
    ## Row scoers
    cat("Estimating row scores zeroes \n")
    regRows = vMat*chis
    tMatSol = try(nleqslv(fn = dZinbMeanTmat, x = c(tMat, lambdaRowZero), k=k, n=n, p=p, global=global, control = nleqslv.control, Zcol=Zcol,reg=regRows, jac=ZinbJacobianTmat, Z=Z, nLambda=nLambda, rowWeights=rowWeights)$x, silent=TRUE)
  if(!inherits(tMatSol,"try-error")){
    tMat = matrix(tMatSol[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)
    lambdaRowZero = tMatSol[(k*n+1):length(tMatSol)]
      }
    
    ## Column scores
        cat("Estimating column scores zeroes \n")
        regCols = t(t(tMat)*chis)
    vMatSol = try(nleqslv(fn = dZinbMeanVmat, x = c(t(vMat), lambdaColZero),reg=regCols,  k=k, n=n, p=p, global=global, control = nleqslv.control, Zcol=Zcol, jac=ZinbJacobianVmat, Z=Z, nLambda=nLambda, colWeights=colWeights)$x, silent=TRUE)
  if(!inherits(vMatSol,"try-error")){
    vMat = matrix(vMatSol[1:(k*p)], byrow=TRUE, nrow=k, ncol=p)
    lambdaColZero = vMatSol[(k*p+1):length(vMatSol)]
          }

  converged = all ((chiOld-chis) < chiTol) &&  (sum(abs(1-tMat/tMatOld)^convNorm))^(1/convNorm) < tol &&  (sum(abs(1-vMat/vMatOld)^convNorm))^(1/convNorm) < tol 
  iter = iter +1
}
  return(list(vMat=vMat, tMat=tMat, iterZI = iter, chis=chis, convergedZI=converged, lambdaColZero=lambdaColZero, lambdaRowZero=lambdaRowZero))
}

#--------------------------------------#

dZinbMeanPsi = function(beta, X, muMarg, k, Z, reg, thetas){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  mu = exp(arrayprod(reg, beta)) * muMarg
  
  vapply(1:k,FUN.VALUE=0, function(z){
  sum(reg[,,z]*(1-Z)*((X-mu)/(1+t(t(mu)/thetas))))
  })
}

#--------------------------------------#
#A jacobian for the psi parameters
ZinbJacobianPsi = function(beta, X,reg, muMarg, k, Z, thetas){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  mu = exp(arrayprod(reg, beta)) * muMarg

  sapply(1:k, function(K){
    vapply(1:k,FUN.VALUE=0, function(Kinner){
    -sum(reg[,,K]*reg[,,Kinner]*(1-Z)*((1+t(t(X)/thetas))*mu/(1+t(t(mu)/thetas))^2))
  })})
}

#--------------------------------------#
dZinbMeanRmat = function(beta, X, reg, muMarg, k,p,n, Z, nLambda, thetas, rowWeights){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

   rMat = matrix(beta[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)
  mu = exp(rMat %*% reg) * muMarg

  lambda1 = beta[(k*n+1):(k*n+k)] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[(k*(n+1)+1):(k*(n+2))] #normalization restrictions sum(abunds*r^2_{ik}) = 1
  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(n+2)+1):length(beta)]

  score = c(tcrossprod((1-Z)*((X-mu)/(1+t(t(mu)/thetas))),reg) + rowWeights*t(lambda1 + lambda2* 2*t(rMat) + t(rMat %*% lambda3Mat)))
  
  centers = colSums(rMat*rowWeights)
  unitSums = colSums(rMat^2*rowWeights)-1
  orthogons = unlist(sapply(1:(k-1), function(K){
    vapply((K+1):k,FUN.VALUE=0, function(Kinner){
      sum(rMat[,K]*rMat[,Kinner]*rowWeights)
    })
  }))
    return(c(score,centers, unitSums, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZinbJacobianRmat = function(beta, X, reg, muMarg, k, p, n, Z, nLambda, thetas, rowWeights){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  rMat = matrix(beta[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)
  mu = exp(rMat %*% reg) * muMarg

  lambda3Mat = matrix(0,ncol=k, nrow=k) #Reorder to lower triangular matrix
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(n+2)+1):length(beta)]
      tmp= ((1+t(t(X)/thetas))*mu/(1+t(t(mu)/thetas))^2)*(1-Z)
    Jac = matrix(0, nrow= n*k + nLambda, ncol=n*k + nLambda)
  #The suymmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:(n*k),(n*k+1):((n+1)*k)] = c(rep.int(c(rowWeights, rep.int(0,n*k)),k-1),rowWeights) 
  #dLag²/dr_{ik}dlambda_{2k}
  Jac[1:(n*k),((n+1)*k+1):((n+2)*k)] = vapply(1:k,FUN.VALUE = vector("numeric",n*k), function(K){c(rep.int(0,(K-1)*n),2*rMat[,K]*rowWeights,rep.int(0,(k-K)*n))})

  for (K in 1:(k-1)){
      for(Kinner in (K+1):k){
        #dLag²/dr_{ik}dr_{ik'}
      diag(Jac[(n*(K-1)+1):(n*K),(n*(Kinner-1)+1):(n*Kinner)]) = -tcrossprod(reg[K,]*reg[Kinner,],tmp) + lambda3Mat[Kinner, K] * rowWeights
      }
  }
  #dLag²/dr_{ik}dlambda_{3kk'}
    Jac[1:(n*k),((n+2)*k+1):(n*k+nLambda)] = unlist(sapply(1:(k-1), function(K){
        vapply((K+1):k,FUN.VALUE = vector("numeric",n*k), function(Kinner){
          c(rep.int(0, n*(K-1)), rMat[,Kinner]*rowWeights, rep.int(0, n*(Kinner-K-1)), rMat[,K]*rowWeights,rep.int(0, n*(k-Kinner)))
        })
      }))
  
  #Symmetrize
  Jac = Jac + t(Jac)
diag(Jac[1:(n*k),1:(n*k)]) = c(t(-tcrossprod(reg^2 ,tmp) + 2*rep(beta[((n+1)*k+1):((n+2)*k)], each = n)* rowWeights))
  Jac
}
#library(Matrix )
# JacSpa= Matrix(Jac, sparse=TRUE)
# image(JacSpa)
#--------------------------------------#
dZinbMeanCmat = function(beta, X, reg, muMarg, k,p,n, Z, nLambda, thetas,colWeights){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  cMat = matrix(beta[1:(k*p)], byrow=TRUE, ncol=p, nrow=k)
  mu = exp(reg %*% cMat) * muMarg

  lambda1 = beta[(k*p+1):(k*p+k)] #Centering restrictions sum(colWeights*r_{ik}) = 0
  lambda2 = beta[(k*p+k+1):(k*p+2*k)] #normalization restrictions sum(colWeights*r^2_{ik}) = 1

  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(p+2)+1):length(beta)]

  score = c(t(
    crossprod(reg,((1-Z)*((X-mu)/(1+t(t(mu)/thetas))))) + 
                        t(colWeights*t(lambda1 + lambda2*2*cMat + (lambda3Mat %*% cMat)))
    ))
  
  centers = colSums(colWeights*t(cMat))
  unitSums = colSums(colWeights*t(cMat^2))-1
  orthogons = unlist(sapply(1:(k-1), function(K){
    vapply((K+1):k,FUN.VALUE=0, function(Kinner){
      sum(cMat[K,]*cMat[Kinner,]*colWeights)
    })
  }))
    return(c(score,centers, unitSums, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZinbJacobianCmat = function(beta, X,reg, colWeights, k, p, n,  Z, nLambda, thetas, muMarg){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param X: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  cMat = matrix(beta[1:(k*p)], byrow=TRUE, ncol=p, nrow=k)
  mu = exp(reg %*% cMat) * muMarg


  lambda1 = beta[(k*p+1):(k*p+k)] #Centering restrictions sum(colWeights*r_{ik}) = 0
  lambda2 = beta[(k*p+k+1):(k*p+2*k)] #normalization restrictions sum(colWeights*r^2_{ik}) = 1

  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(p+2)+1):length(beta)]
  
    tmp= ((1+t(t(X)/thetas))*mu/(1+t(t(mu)/thetas))^2)*(1-Z)
    Jac = matrix(0, nrow= p*k + nLambda, ncol=p*k + nLambda)
  #The suymmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:(p*k),(p*k+1):((p+1)*k)] = c(rep(c(colWeights, rep.int(0,p*k)),k-1), colWeights)
  #Jac[1:(p*k),(p*k+1):((p+1)*k)] = sapply(1:k, function(K){c(rep(0,(K-1)*p),colWeights,rep(0,(k-K)*p))})
  Jac[1:(p*k),((p+1)*k+1):((p+2)*k)] = vapply(1:k,FUN.VALUE = vector("numeric", p*k), function(K){c(rep.int(0,(K-1)*p),colWeights*2 *cMat[K,],rep.int(0,(k-K)*p))})

  for (K in 1:(k-1)){
      for(Kinner in (K+1):k){
  #dLag²/dr_{ik}dr_{ik'}
  diag(Jac[(p*(K-1)+1):(p*K),(p*(Kinner-1)+1):(p*Kinner)]) =  -crossprod(  tmp, (reg[,K]*reg[,Kinner])) + lambda3Mat[Kinner, K]*colWeights
    }
  }
    #dLag²/ds_{ik}dlambda_{3kk'}
    Jac[1:(p*k),((p+2)*k+1):(p*k+nLambda)] = unlist(sapply(1:(k-1), function(K){
        vapply((K+1):k,FUN.VALUE = vector("numeric",p*k), function(Kinner){
          c(rep.int(0, p*(K-1)), colWeights*cMat[Kinner,], rep.int(0, p*(Kinner-K-1)), colWeights*cMat[K,],rep.int(0, p*(k-Kinner)))
        })
      }))
  
  #Symmetrize
  Jac = Jac + t(Jac)

    diag(Jac[1:(p*k),1:(p*k)]) = c(-crossprod(tmp, reg^2)) + 2*rep(beta[((p+1)*k+1):((p+2)*k)], each = p)*colWeights
    
  Jac
}

#--------------------------------------#

dZinbMeanChi = function(beta, k,  Z, reg, Zcol, nLambda){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates
  GZero = expit(t(t(arrayprod(reg, beta))+Zcol))
  
  vapply(1:k,FUN.VALUE=0, function(u){
  sum((Z-GZero)*reg[,,u])
  })
  
}

#--------------------------------------#
#A jacobian for the psi parameters
ZinbJacobianChi = function(beta, reg,  k, Z, Zcol){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  expZero = exp(t(t(arrayprod(reg, beta))+Zcol))
  tmp = (expZero/(1+expZero)^2)
  tmp[is.infinite(expZero)]=0
  
  sapply(1:k, function(K){
    vapply(1:k,FUN.VALUE=0, function(Kinner){
    tmp2=-sum(reg[,,Kinner]*reg[,,K]*tmp)
#     tmp2[tmp2==Inf]=1e16
#     tmp2[tmp2==-Inf]=-1e16
  })})
}

#--------------------------------------#
dZinbMeanTmat = function(beta, reg,  k,p,n, Z, Zcol, nLambda, rowWeights){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  tMat = matrix(beta[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)

  GZero = expit(t(t(tMat %*% reg)+Zcol))

  lambda1 = beta[(k*n+1):(k*n+k)] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[(k*n+k+1):(k*n+2*k)] #normalization restrictions sum(abunds*r^2_{ik}) = 1

  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(n+2)+1):length(beta)]

  score = c(t(t(
    tcrossprod((Z-GZero), reg) + 
                         (t(lambda1+ lambda2*2*t(tMat)) + (tMat %*% lambda3Mat))*rowWeights))) 
  
  centers = colSums(tMat*rowWeights)
  unitSums = colSums(tMat^2*rowWeights)-1
  orthogons = unlist(sapply(1:(k-1), function(K){
    vapply((K+1):k,FUN.VALUE=0, function(Kinner){
      sum(tMat[,K]*tMat[,Kinner]*rowWeights)
    })
  }))
    return(c(score,centers, unitSums, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZinbJacobianTmat = function(beta, reg, k, p, n, Z, Zcol, nLambda, rowWeights){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param abunds: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  tMat = matrix(beta[1:(k*n)], byrow=FALSE, ncol=k, nrow=n)

  expZero = exp(t(t(tMat %*% reg)+Zcol))
  
  tmp = (expZero/(1+expZero)^2)
  tmp[is.infinite(expZero)]=0

  lambda1 = beta[(k*n+1):(k*n+k)] #Centering restrictions sum(abunds*r_{ik}) = 0
  lambda2 = beta[(k*n+k+1):(k*n+2*k)] #normalization restrictions sum(abunds*r^2_{ik}) = 1

  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(n+2)+1):length(beta)]
  
    Jac = matrix(0, nrow= n*k + nLambda, ncol=n*k + nLambda)
  #The suymmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:(n*k),(n*k+1):((n+1)*k)] = c(rep.int(c(rowWeights, rep.int(0,n*k)),k-1), rowWeights) 
  #dLag²/dr_{ik}dlambda_{2k}
  Jac[1:(n*k),((n+1)*k+1):((n+2)*k)] = vapply(1:k,FUN.VALUE = vector("numeric",n*k), function(K){c(rep.int(0,(K-1)*n),2 *tMat[,K]*rowWeights,rep.int(0,(k-K)*n))})

  for (K in 1:(k-1)){
      for(Kinner in (K+1):k){
        #dLag²/dr_{ik}dr_{ik'}
      diag(Jac[(n*(K-1)+1):(n*K),(n*(Kinner-1)+1):(n*Kinner)]) = -tcrossprod(reg[Kinner,]*reg[K,],tmp) + lambda3Mat[Kinner, K]*rowWeights
      }
  }
  #dLag²/dr_{ik}dlambda_{3kk'}
   Jac[1:(n*k),((n+2)*k+1):(n*k+nLambda)] = unlist(sapply(1:(k-1), function(K){
        vapply((K+1):k,FUN.VALUE = vector("numeric",n*k), function(Kinner){
          c(rep.int(0, n*(K-1)), tMat[,Kinner]*rowWeights, rep.int(0, n*(Kinner-K-1)), tMat[,K]*rowWeights,rep.int(0, n*(k-Kinner)))
        })
      }))
  
  #Symmetrize
  Jac = Jac + t(Jac)
    diag(Jac[1:(n*k),1:(n*k)]) = c(-tcrossprod(tmp, reg^2) + 2*rep(beta[((n+1)*k+1):((n+2)*k)], each = n)*rowWeights)
    
  Jac
}
#--------------------------------------#
dZinbMeanVmat = function(beta, reg, colWeights, k,p,n, Z, Zcol, nLambda){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes

  # @return A vector of length r with the new psi estimates

  vMat = matrix(beta[1:(k*p)], byrow=TRUE, ncol=p, nrow=k)
  reg = t(t(tMat)*chis)
  
  GZero = expit(t(t(tMat %*% (vMat*chis))+Zcol))

  lambda1 = beta[(k*p+1):(k*p+k)] #Centering restrictions sum(colWeights*r_{ik}) = 0
  lambda2 = beta[(k*p+k+1):(k*p+2*k)] #normalization restrictions sum(colWeights*r^2_{ik}) = 1
  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(p+2)+1):length(beta)]

  score = c(t(
    crossprod(reg,(Z-GZero)) + 
                        t(colWeights*t(lambda1 + lambda2*2*vMat + (lambda3Mat %*% vMat)))
    ))
  
  centers = colSums(colWeights*t(vMat))
  unitSums = colSums(colWeights*t(vMat^2))-1
  orthogons = unlist(sapply(1:(k-1), function(K){
    vapply((K+1):k,FUN.VALUE=0, function(Kinner){
      sum(vMat[K,]*vMat[Kinner,]*colWeights)
    })
  }))
    return(c(score,centers, unitSums, orthogons))
}

#--------------------------------------#
#A jacobian for the psi parameters
ZinbJacobianVmat = function(beta, reg, colWeights, k, p, n, Z, Zcol, nLambda){
  # @param beta: a vector of r regression parameters to optimize: the r psi parameters
  # @param y: the nxp data matrix
  # @param reg: a nxpxr regressor array with r the number of regressors
  # @param theta: a vector of length p with the dispersion parameters
  # @param k: a scalar, dimension of the RC solution
  # @param colWeights: a vector of length p with the abundance parameters
  # @param libSizes (optional): a vector of length n with (known) library sizes
  
  vMat = matrix(beta[1:(k*p)], byrow=TRUE, ncol=p, nrow=k)
  reg = t(t(tMat)*chis)
  expZero = exp(t(t(tMat %*% (vMat*chis))+Zcol))
  tmp = (expZero/(1+expZero)^2)
  tmp[is.infinite(expZero)]=0

  lambda3Mat = matrix(0,ncol=k, nrow=k)
  lambda3Mat[lower.tri(lambda3Mat)] = lambda3Mat[upper.tri(lambda3Mat)]= beta[(k*(p+2)+1):length(beta)]
  
    Jac = matrix(0, nrow= p*k + nLambda, ncol=p*k + nLambda)
  #The suymmetric jacobian matrix, equal to the fisher information. The upper part is filled first, then mirror image is taken for lower triangle

  #dLag²/dr_{ik}dlambda_{1k}
  Jac[1:(p*k),(p*k+1):((p+1)*k)] = c(rep(c(colWeights, rep.int(0,p*k)),k-1), colWeights)
  #Jac[1:(p*k),(p*k+1):((p+1)*k)] = sapply(1:k, function(K){c(rep(0,(K-1)*p),colWeights,rep(0,(k-K)*p))})
  Jac[1:(p*k),((p+1)*k+1):((p+2)*k)] = vapply(1:k,FUN.VALUE = vector("numeric", p*k), function(K){c(rep.int(0,(K-1)*p),colWeights*2 *vMat[K,],rep.int(0,(k-K)*p))})

  for (K in 1:(k-1)){
      for(Kinner in (K+1):k){
        #dLag²/dr_{ik}dr_{ik'}
      diag(Jac[(p*(K-1)+1):(p*K),(p*(Kinner-1)+1):(p*Kinner)]) = -reg[,Kinner]%*%(reg[,K]*(tmp)) + lambda3Mat[Kinner, K]*colWeights
      }
  }
    #dLag²/ds_{ik}dlambda_{3kk'}
    Jac[1:(p*k),((p+2)*k+1):(p*k+nLambda)] = unlist(sapply(1:(k-1), function(K){
        vapply((K+1):k,FUN.VALUE = vector("numeric",p*k), function(Kinner){
          c(rep.int(0, p*(K-1)), colWeights*vMat[Kinner,], rep.int(0, p*(Kinner-K-1)), colWeights*vMat[K,],rep.int(0, p*(k-Kinner)))
        })
      }))
  
  #Symmetrize
  Jac = Jac + t(Jac)
    diag(Jac[1:(p*k),1:(p*k)]) = c(t( -crossprod(reg^2,tmp)) + colWeights *2*rep(beta[((p+1)*k+1):((p+2)*k)], each = p))
    
  Jac
}

RCM_ZINB = function(X, k, rowWeights , colWeights, weightsChar, tol = 1e-3, maxItOut = 500, psiTol = 1e-4, chiTol=psiTol, verbose = TRUE, ZINBRCM = NULL, global ="dbldog", nleqslv.control=list(),method=c("Broyden"), twoCores=TRUE, convNorm = 2, maxItMean = 20, maxItZeroes=20, dispFreq = 5){
  
  # @param X: a nxp data matrix
  # @param k: a scalar, number of dimensions in the RC(M) model
  # @param tol(optional): a scalar, the relative convergende tolerance for the row scores and column scores parameters, defaults to 1e-3  
  # @param Psitol(optional): a scalar, the relative convergence tolerance for the psi parameters, defaults to 1e-4
  # @param maxItOut(optional): an integer, the maximum number of iteration in the outer loop, defaults to 50
  # @param libSizes(optional) : a vector of length n with (known) library sizes. If not provided, rowSums of x are used
  # @param verbose(optional): a boolean, should information on iterations be printed? Defaults to TRUE
  # @param method(optional): Method for jacobian estimation , see nleqslv. Defaults to Broyden. The difference with the newton method is that the Jacobian is not recalculated at every iteration
  # @param global(optional): global strategy for solving non-linear systems , see nleqslv
  # @param nleqslv.control: a list with control options, see nleqslv
  # @param lambdaRow: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers for the row scores
  # @param lambdaCol: a vector of length 2*k+k*(k-1)/2 with inital estimates or the lagrange multipliers for the column scores
  # @param rMatInit(optional): a nxk matrix with initial row scores. If not provided values from the singular value decomposition will be used as starting values
  # @param cMatInit(optional): a pxk matrix with initial column scores. If not provided values from the singular value decomposition will be used as starting values
  # @param psisInit(optional): a vector of length k with inital values for the importance parameters psi. If not provided values from the singular value decomposition will be used as starting values
  # @param dispFreq: a scalar, how many iterations the algorithm should wait before reestimationg the dispersions
  # @param convNorm: a scalar, the norm to use to determine convergence

  # @return A list with elements:
  # @return psis: a vector of length k with estimates for the importance parameters psi
  # @return thetas: a vector of length p with estimates for the overdispersion
  # @return rMat: a nxk matrix with estimated row scores
  # @return cMat: a pxk matrix with estimated column scores
  # @return converged: a boolean indicating if the algorithm converged
  # @return rowRec: a n x k x maxItOut array with a record of all rMat estimates through the iterations 
  # @return colRec: a k x p x maxItOut array with a record of all cMat estimates through the iterations
  # @return psiRec.: a k x maxItOut array with a record of all psi estimates through the iterations
  if(dispFreq>maxItMean){
    stop("Dispersion estimation frequency lower than loop length. Overdispersions must be estimated!")
  }
  libSizes = rowSums(X)
  abunds = (colSums(X)/sum(X))
  n=NROW(X)
  p=NCOL(X)
  muMarg=outer(libSizes, abunds)

  nLambda = k*(k-1)/2+2*k #Number of lambda parameters
  
    if(!is.null(ZINBRCM)){
for (i in c("cMat", "rMat","tMat","vMat", "lambdaCol","lambdaRow","lambdaRowZero","lambdaColZero", "psis","chis", "Z","Zcol", "thetas")){
  assign(i, ZINBRCM[[i]])
}}else{
  
 ## 1) Initialization
Zcol = unlist(mclapply(1:ncol(X), mc.cores=3,function(i){
 tmp= try(zeroinfl(X[,i]~log(libSizes)|1, dist="negbin")$coef$zero, silent=TRUE)
 if(class(tmp)=="try-error"){
   tmp= try(zeroinfl(X[,i]~log(libSizes)|1, dist="negbin", EM=TRUE)$coef$zero, silent=TRUE)
    if(class(tmp)=="try-error"){
   tmp = -1e16 #If no zeroes => No chance on a structural zero
    }
 }
 tmp
}))
   
  svdX = svd((X-muMarg)%*%diag((1-expit(Zcol))/colSums(X)))
  rMat = svdX$u[,1:k]
  cMat = t( svdX$v[,1:k] )
  psis = svdX$d[1:k]
  
psis = c(psis *t(apply(cMat, 1, function(colS){
      sqrt(sum(colWeights * colS^2))
  })) * apply(rMat, 2, function(rowS){
      sqrt(sum(rowWeights * rowS^2))
  }))
  
#Normalize  
cMat = t(apply(cMat, 1, function(colS){
      colS/sqrt(sum(colWeights * colS^2))
  }))
rMat = apply(rMat, 2, function(rowS){
      rowS/sqrt(sum(rowWeights * rowS^2))
  })

   #Initial estimates for zeroes is also based on an svd
  Ezeroes = matrix(expit(Zcol), nrow=n, ncol=p, byrow=TRUE)#dpois(0, lambda = outer(libSizes, abunds)*exp(rMat %*%  (psis*cMat)))
  Xzeroes = X==0
  
  svdZero = svd(diag(sqrt(1/libSizes)) %*%(Xzeroes-Ezeroes)%*% diag(1/sqrt(colSums(X))))

  tMat = svdZero$u[,1:k]
  vMat = t(svdZero$v[,1:k])
  chis = svdZero$d[1:k]

#Redistribute some weight to fit the constraints 
chis = c(chis *t(apply(vMat, 1, function(colS){
      sqrt(sum(colWeights * colS^2))
  })) * apply(tMat, 2, function(rowS){
      sqrt(sum(rowWeights * rowS^2))
  }))
  
#Normalize  
vMat = t(apply(vMat, 1, function(colS){
      colS/sqrt(sum(colWeights * colS^2))
  }))
tMat = apply(tMat, 2, function(rowS){
      rowS/sqrt(sum(rowWeights * rowS^2))
  })
  
  lambdaRow = lambdaCol = lambdaColZero=lambdaRowZero = rep.int(0,nLambda)
  thetas=estDisp(X = X, cMat = cMat, rMat=rMat, muMarg=muMarg, psis = psis, k = k ) #Regular dispersions as starting values
  }
  
  iterOut = 1
  rowRec = rowRecZero = array(0,dim=c(NROW(X),k, maxItOut))
  colRec = colRecZero = array(0,dim=c(k,NCOL(X), maxItOut))
  psiRec = chiRec = matrix(0,ncol=maxItOut, nrow=k)

  ## 2) Propagation
  
  while((iterOut ==1) || ((iterOut <= maxItOut) && (!convergence)))
    {
    
  if(verbose && iterOut%%1 == 0){
  cat("\n","Outer Iteration", iterOut, "\n","\n")
    if(iterOut!=1){
  cat("Old psi-estimates: ", psiOld, "\n")
  cat("New psi-estimates: ", psis, "\n")
    }
  }
  ## 2)a. Store old parameters
  psiOld = psis
  rMatOld = rMat
  cMatOld = cMat
  
  chiOld = chis
  tMatOld = tMat
  vMatOld = vMat 

  #Expectation
  Z = EstepNB (X, rMat, cMat, tMat, vMat, muMarg, k, Zcol, psis, chis, thetas)

  #Maximization
  Mlist = MstepNB(Z = Z, X = X, rMat = rMat, cMat = cMat, tMat = tMat, vMat = vMat, k = k, n=n, p=p,muMarg = muMarg, Zcol=Zcol, psis = psis,chis = chis, twoCores=twoCores, tol = tol, psiTol = psiTol, chiTol = chiTol, convNorm = convNorm, global=global, nLambda = nLambda, nleqslv.control = nleqslv.control, lambdaCol = lambdaCol, lambdaRow=lambdaRow, lambdaColZero = lambdaColZero, lambdaRowZero = lambdaRowZero, maxItMean = maxItMean, maxItZeroes = maxItZeroes, dispFreq=dispFreq, thetas = thetas, rowWeights = rowWeights, colWeights = colWeights)

  for (x in names(Mlist))
    {assign(x,Mlist[[x]])}
  #cMatSE = cMatList$cMatSE
  rowRec[,, iterOut] = rMat
  colRec[,, iterOut] = cMat
  rowRecZero[,, iterOut] = tMat
  colRecZero[,, iterOut] = vMat
  psiRec[, iterOut] = psis
  chiRec[, iterOut] = chis
  
  ## 2)f. Change iterator
    iterOut = iterOut + 1
    
  ##Check convergence  (any numbered norm for row and column scores)
  convergence = ((iterOut <= maxItOut) && 
                   (all(abs(1-psis/psiOld) < psiTol)) &&
                   ((sum((1-rMatOld/rMat)^convNorm)/n)^(1/convNorm) < tol) && 
                   ((sum((1-cMatOld/cMat)^convNorm)/p)^(1/convNorm) < tol) ) && (all(abs(1-chis/chiOld) < chiTol)) &&  (sum(abs(1-tMat/tMatOld)^convNorm)/n)^(1/convNorm) < tol &&  (sum(abs(1-vMat/vMatOld)^convNorm)/p)^(1/convNorm) < tol 
} # END while-loop
  
  ## 3) Termination
  
  rownames(rMat) = rownames(tMat) = rownames(X)
  colnames(cMat) = colnames(vMat) = colnames(X)
  rownames(cMat) = rownames(vMat) = colnames(tMat) = colnames(rMat) = paste0("Dim",1:k)
  
  if(!convergence ){
    warning("Algorithm did not converge! Check for errors or consider changing tolerances or number of iterations")
  }
    return(list(converged = convergence, rMat=rMat, cMat=cMat, psis = psis, X=X,
                 rowRec = rowRec, colRec = colRec, psiRec = psiRec, lambdaRow = lambdaRow, lambdaCol = lambdaCol, lambdaRowZero = lambdaRowZero, lambdaColZero = lambdaColZero, chis = chis, tMat = tMat, vMat = vMat, chiRec = chiRec, rowRecZero = rowRecZero, colRecZero = colRecZero, iter=iterOut-1, Z=Z, Zcol=Zcol, thetas=thetas))
}
```

## Wrapper for all distirbutions

```{r Wrapper all dists, purl = TRUE}
#A simple wrapper function for phyloseq objects and matrices, for all posible distrbutions passes all argument sonto outerLoop()
#Trim on prevalence to avoid instability of the algorithm
RCM = function(dat, k, round=FALSE, method= c("NB","NB_1B1", "ZIP","ZIP_1B1", "ZINB"), prevCutOff=0.01, rowWeights = "uniform", colWeights = "marginal",...){
  if (class(dat)=="matrix"){
  }else  if(class(dat)=="phyloseq"){
  dat = if (taxa_are_rows(physeq)) t(otu_table(physeq)@.Data) else otu_table(physeq)@.Data
  } else {stop("Please provide a matrix or a phyloseq object! \n")}
  p=ncol(dat); n=nrow(dat)
  
  #Find the weights, if not provided
  if(rowWeights %in% c("marginal","uniform")){
  rowWeightsNum = switch(rowWeights, uniform = rep(1/n,n), marginal=rowSums(dat)/sum(dat)) #Keep weights high to avoid overflow
  } else {if(!nrow(dat) %in% c(length(rowWeights), nrow(rowWeights))){
    stop("Length of row weights incorrect! \n")}
    rowWeightsNum=rowWeights}
  if(colWeights %in% c("marginal","uniform")){
  colWeightsNum = switch(colWeights, uniform = rep(1/p,p), marginal=colSums(dat)/sum(dat), colWeights)
  } else {
    if(!ncol(dat) %in% c(length(colWeights), nrow(colWeights))){
    stop("Length of col weights incorrect! \n")}
    colWeightsNum=colWeights}
  
  if(is.null(colnames(dat))){colnames(dat)=1:ncol(dat)}
  if(is.null(rownames(dat))){rownames(dat)=1:nrow(dat)}
  colNames = colnames(dat); rowNames =rownames(dat)
  names(rowWeightsNum) =rowNames; names(colWeightsNum) = colNames
  if(round) {dat=round(dat, 0) }#Round to integer
  
  #Check data type
  if(!all(sapply(dat, function(x){(x%%1)==0}))){stop("Please provide integer count matrix! \n")
    } else{dat=matrix(as.integer(dat), ncol=ncol(dat), nrow=nrow(dat))}

  colnames(dat)=colNames; rownames(dat)=rowNames
  dat=dat[rowSums(dat)>0, colSums(dat)>0]
  dat=dat[, colMeans(dat==0)<(1-prevCutOff)]
  if (method %in% c("ZIP","ZINB")) dat=dat[rowSums(dat==0)>0, colSums(dat==0)>0] #For a zero-inflated model, make sure every row and column has zeroes
  
rowWeightsNum = if (is.matrix(rowWeightsNum)) rowWeightsNum[rownames(dat), ] else rowWeightsNum[rownames(dat)]
colWeightsNum = if (is.matrix(colWeightsNum)) colWeightsNum[colnames(dat), ] else colWeightsNum[colnames(dat)]
weightsChar = paste0(rowWeights, colWeights)

tic = proc.time() #Time the calculation
  tmp = switch(method, 
               NB=RCM_NB(dat, rowWeights=rowWeightsNum, colWeights=colWeightsNum, weightsChar = weightsChar, k=k,...),
               NB_1B1=RCM_NB_1B1(dat, rowWeights=rowWeightsNum, colWeights=colWeightsNum, weightsChar = weightsChar, k=k,...), 
               ZIP=RCM_ZIP(dat, rowWeights=rowWeightsNum, colWeights=colWeightsNum,weightsChar = weightsChar, k=k,...),
               ZIP_1B1 = RCM_ZIP_1B1(dat, rowWeights=rowWeightsNum, colWeights=colWeightsNum,weightsChar = weightsChar, k=k,...),
               ZINB=RCM_ZINB(dat,  rowWeights=rowWeightsNum, colWeights=colWeightsNum,weightsChar = weightsChar, k=k,...))
  if(class(dat)=="phyloseq"){tmp$physeq = physeq} 
  within(tmp, {
    runtimeInMins = (proc.time()-tic)[1]/60
    k = k #Store number of dimensions
  })
}

#-------------------------------------
# A function to calculate the likelihoods of
#-The independence model
#-The saturated model
#-The fitted model
#-All models with dimension k 0<k<K
#Which overdispersions to use is a non-trivial problem.  One option would be to use the estimated dispersions of the full model for all calculations. Another is to estimate the overdispersions of the independence and lower dimensional models separately and use them. The problem is that if we use the edgeR machinery again, we get stable estimates but not MLE's, so that the likelihood of the independence model can sometimes be larger than that of a RC model. We provide three options, specified through the Disp parameter:
# - "MLE" Use the MLE's of the separate models where possible,
# - "edgeR" Use the edgeR robust estimate separately for every model
liks = function(rcm, Disp=c("edgeR","MLE")){
  require(MASS)
  #@param rcm: a list, the output of the outerLoop function
  
  #@return a list with components
    #-indLL: likelihood of the indepence model
    #-LL1,..., LL[K-1]: likelihood of intermediate models
    #-LLK: The likelihood of the fitted model
  #Independence model
  C = colSums(rcm$X)
  R = rowSums(rcm$X)
  onesn =rep.int(1, nrow(rcm$X))
  onesp = rep.int(1, ncol(rcm$X))
  E = diag(R) %*% outer(onesn ,onesp) %*% diag(C)/sum(C)
  
  if(Disp=="MLE"){
    
  #Estimate dispersions for the independence model
  # thetasInd = estDisp(rcm$X, cMat=matrix(0,ncol=ncol(rcm$X)), rMat=matrix(0,nrow=nrow(rcm$X)), libSizes=rowSums(rcm$X), abunds=colSums(rcm$X)/sum(rcm$X), psis=0)
  thetasInd =sapply(1:ncol(rcm$X), function(i){theta.ml(y=rcm$X[,i],mu=E[,i])})
  #The overdispersions of the independence model are larger: This is logical since less variation has been explained in this model
  
    #Estimate the overdispersions for the intermediate models and the Full RC model
    LLintDisp = vapply(1:(ncol(rcm$rMat)),FUN.VALUE=rep(0, ncol(rcm$X)), function(k){
      mu=E * exp(rcm$rMat[, 1:k, drop=FALSE] %*% (rcm$cMat[1:k,,drop=FALSE]*rcm$psis[1:k]))
    sapply(1:ncol(rcm$X), function(i){theta.ml(y=rcm$X[,i],mu=mu[,i])})
  })

  } else if(Disp=="edgeR"){
  #Independence model
  trended.dispersion.ind  <- estimateGLMTrendedDisp(y = t(rcm$X), design = NULL, method = "bin.loess",offset=t(log(E)))
  thetasInd = 1/estimateGLMTagwiseDisp(y = t(rcm$X), design = NULL,  offset=t(log(E)), dispersion = trended.dispersion.ind)
  
  #RCM models
  thetasInt = vapply(1:(ncol(rcm$rMat)),FUN.VALUE=rep(0, ncol(rcm$X)), function(k){
  mu=E * exp(rcm$rMat[, 1:k, drop=FALSE] %*% (rcm$cMat[1:k,,drop=FALSE]*rcm$psis[1:k]))
  trended.dispersion  <- estimateGLMTrendedDisp(y = t(rcm$X), design = NULL, method = "bin.loess",offset=t(log(mu)))
  1/estimateGLMTagwiseDisp(y = t(rcm$X), design = NULL,  offset=t(log(mu)), dispersion = trended.dispersion)
  })
  LLintDisp=cbind(thetasInd, thetasInt)
  }  else{stop("No valid dispersion estimation paradigm provided! Choose either MLE or edgeR")}
    names(LLintDisp) = paste0("dispLL",1:(ncol(rcm$rMat)-1))
#Now we have the overdispersions, estimate the likelihoods
  
  #Estimate the likelihoods
  LLintList = mapply(1:(ncol(rcm$rMat)-1),1:(ncol(LLintDisp)-2), FUN=function(k, ThetasI){
    sum(dnbinom(rcm$X, mu=E * exp(rcm$rMat[, 1:k, drop=FALSE] %*% (rcm$cMat[1:k,,drop=FALSE]*rcm$psis[1:k])), size=LLintDisp[,ThetasI], log=TRUE))
  })
  names(LLintList) = paste0("LL",1:(ncol(rcm$rMat)-1))
  indLL = sum(dnbinom(rcm$X, mu=E, size=thetasInd, log=TRUE))
  
#Full RC model
  LLK = sum(dnbinom(rcm$X, mu=E * exp(rcm$rMat %*% (rcm$cMat*rcm$psis)), size=LLintDisp[, ncol(LLintDisp)], log=TRUE))
    
  c(indLL=indLL, LLintList,  LLK=LLK)
}
```

## A plotting function

```{r plot}
plotRCM = function(RCM, X = NULL, thetas = NULL, abunds = NULL, arrowFrac = 0.04, biplot = TRUE,
                   libLoc ="topright", libInset = c(-0.7,-0.1), libCex = 0.85, libInterSp = 0.4, samColour=NULL,
                   dispInset = c(0,-0.4), abInset = c(0,-0.4), abundLeg=FALSE, stressSpecies=NULL, taxColour = NULL, taxLegPos = "topleft",
                   asp=1, xpd=TRUE, mar=c(4,5,5,5), Dim=c(1,2), ...){
  
  # @param psis: vector of length k with psi estimates
  # @param rMat: a nxk matrix with final row scores
  # @param cMat: a pxk with matrix with final column scores
  # @param Dim (optional): a vector of length 2, indicating which dimensions to fit, defaults to the first two
  # @param X (optional): the nxp data matrix
  # @param thetas (optional): a vector of length p with estimates for the overdispersion
  # @param abunds(optional): a vector of length p with abundance estimates (otherwise it is estimated from the data)
  # @param arrowFrac(optional): Fraction of largest species to plot. defaults to 0.1
  # @param biplot(optional): A boolean indicating if species should be added to the plot, defaults to TRUE
  # @param libLoc(optional): a string, location of the library size legend. Defaults to "topleft"
  # @param libLegend(optional): a boolean, should library size legend be displayed? defaults to TRUE
  # @param libInset, dispInset, abInset(optional): numeric vectors of length 2, insets for library size, dispersion and abundance legends
  # param stressSpecies(optional): names of species to be highlighted
  # @param ... additional arguments, passed on to the plot() function
  
  cMat = RCM$cMat
  rMat = RCM$rMat
  psis = RCM$psis
  
  # @return: NONE,  plots the result in the plotting window
  # tmp = par(no.readonly = TRUE)
  par(mar=mar, pty="s")
  if(!(length(psis)== NCOL(rMat) && length(psis) == NROW(cMat))){
    stop("Dimensions don't match")
  }
  #Reorder dimensions according to importance

  a = Dim[1]
  b = Dim[2]
  
  #Add colours for the library sizes
  if(!is.null(X) & is.null(samColour)){
  samColour = ifelse(rowSums(X) < median(rowSums(X)), "blue","red")
  } else if(!is.null(samColour)){
  }else {
    samColour = 1}
  
  ## Add linetypes for the dispersions
  if(!is.null(thetas)){
    LineType = rowSums(sapply(quantile(1/thetas, c(0.25,0.5,0.75,1)), function(x){
      1/thetas > x
    })) 
  } else {LineType=rep(1, ncol(cMat))}
  
  ##Add colours for the abundances
  if(!is.null(abunds) & is.null(taxColour)){
    taxColour = rowSums(sapply(quantile(abunds, c(0.25,0.5,0.75,1)), function(x){
      abunds < x
    })) 
  } else if(is.null(taxColour)) {
    taxColour = rep(1, ncol(cMat))
    }else{}
  
  plot(x = rMat[,a] * psis[a],
  y = rMat[,b] *psis[b],
  xlab=paste0("Dim",a),
  ylab=paste0("Dim",b),
  xlim = max(abs(rMat[,a] * psis[a]))*c(-1,1),
  ylim = max(abs(rMat[,b] * psis[b]))*c(-1,1),
  col = samColour,
  asp=asp,
  ...)
legend(pch = 1, col = seq_along(levels(samColour)), legend = levels(samColour), x=libLoc, inset=libInset, xpd=xpd, cex = libCex, y.intersp = libInterSp, x.intersp = 0.7)

# {legend(libLoc,legend=c("Small library size","Large library size"),
#       pch=c(1,1), col=c("blue","red"), inset=libInset, xpd=xpd)
#   }
  
    if (biplot){
      #Arrows
        arrowLengths = apply(cMat[Dim,],2,function(x){sqrt(sum(x^2))})
    id = arrowLengths >= quantile(arrowLengths,1-arrowFrac)
        scalingFactor = max(abs(apply(t(t(rMat[,Dim])*psis[Dim]),2, range)))/
        max(abs(cMat[Dim,id]))*0.99
    arrows(x0=0,y0=0,x1=cMat[a,id]*scalingFactor,y1=cMat[b, id]*scalingFactor, 
           lty=LineType[id], col = taxColour[id])
    # legend(x = taxLegPos, legend = levels(taxColour),pch = 1, col = seq_along(levels(taxColour)) )
    if(!is.null(thetas)){
      legend("top", legend=paste0(">",seq(0,75,25), "th quantile"), 
             lty=1:4, title="Dispersion", xpd=xpd, inset = dispInset, cex= 0.75)
    }
    if(!is.null(abunds) & abundLeg){
      legend("topright", legend=paste0(">",seq(0,75,25), "th quantile"), 
             col = 1:4,lty=1, title="Abundance", xpd=TRUE, inset = abInset,cex=0.75)
    }
    #if(!is.null(stressSpecies))
    }
return(invisible())
  # par(tmp)
}
```

# Demonstration

## Toy data

### Negative binomial

We generate some data as before with the NB distribution but differing library sizes, apply our algorithm and plot the results.

#### NB without signal

##### Create the data

```{r NB no signal, trace=TRUE}
#Negative binomial, no signal. Set parameters
NsamplesNBNS= 300
NtaxaNBNS = 900
thetasNBNS=sample(thetas,NtaxaNBNS)
thetasNBNS = thetasNBNS[1/thetasNBNS<60]
rhosNBNS= rhos[names(thetasNBNS)]
rhosNBNS=rhosNBNS/sum(rhosNBNS)
NtaxaNBNS=length(rhosNBNS)
libSizesNBNS =c(rep(1e4, floor(NsamplesNBNS/2)), rep(1e5, floor(NsamplesNBNS/2)))

#Create means and overdispersion matrices
meanNBNS = outer(libSizesNBNS, rhosNBNS)
thetaMatNBNS =  matrix(thetasNBNS, nrow=NsamplesNBNS, ncol=NtaxaNBNS, byrow=TRUE)

#Define a function to make NB data
makeNBdata=function(meanMat, thetaMat){apply(array(data= c(meanMat, thetaMat), dim=c(nrow(meanMat), ncol(meanMat), 2)), c(1,2), function(x){rnbinom(1,mu=x[1], size=x[2])})}

#Generate the data
dataMatNBNS = makeNBdata(meanNBNS, thetaMatNBNS)
```

##### Fit the RC(M) model

```{r fit RCM}
#Set control parameters
nleqslv.control = list(trace=FALSE, maxit = 500, cndtol=.Machine$double.eps)
#Fit the RC(M) model
if(!file.exists("toyDataNS.RData")){

      syntNBNSmargmarg_1B1_3Job = mcparallel(RCM(dataMatNBNS, method="NB_1B1", k=3, nleqslv.control= nleqslv.control, maxItOut=1e3, colWeights = "marginal",rowWeights ="marginal", prevCutOff=0.01))
  syntNBNSmargmarg_1B1_3 = mccollect(syntNBNSmargmarg_1B1_3Job, FALSE)[[1]]
  
      syntNBNSunifmarg_1B1_3Job = mcparallel(RCM(dataMatNBNS, method="NB_1B1", k=3, nleqslv.control= nleqslv.control, maxItOut=1e3, colWeights = "marginal",rowWeights ="uniform",prevCutOff=0.01))
  syntNBNSunifmarg_1B1_3 = mccollect(syntNBNSunifmarg_1B1_3Job, FALSE)[[1]]
  
      syntNBNSunifunif_1B1_3Job = mcparallel(RCM(dataMatNBNS, method="NB_1B1", k=3, nleqslv.control= nleqslv.control, maxItOut=1e3, colWeights = "uniform",rowWeights ="uniform", prevCutOff=0.01))
  syntNBNSunifunif_1B1_3 = mccollect(syntNBNSunifunif_1B1_3Job, FALSE)[[1]]
  
      syntNBNSmargunif_1B1_3Job = mcparallel(RCM(dataMatNBNS, method="NB_1B1", k=3, nleqslv.control= nleqslv.control, maxItOut=1e3, rowWeights = "marginal",colWeights ="uniform", prevCutOff=0.01))
  syntNBNSmargunif_1B1_3 = mccollect(syntNBNSmargunif_1B1_3Job, FALSE)[[1]]
  
  #Only weight the normalization, unfiorm weights for the rows
  Dim = dim(dataMatNBNS)
  unifWeights = rep(1/Dim[2], Dim[2])
  margWeights = colSums(dataMatNBNS)/sum(dataMatNBNS)
  colWeightsTest1 = cbind(unifWeights, margWeights, unifWeights)
  colWeightsTest2 = cbind(margWeights, margWeights, unifWeights)
  colWeightsTest3 = cbind(margWeights, unifWeights, unifWeights)
  
 testJob1 = mcparallel(RCM(dataMatNBNS, method="NB_1B1", k=3, nleqslv.control= nleqslv.control, maxItOut=1e3, rowWeights = "marginal",colWeights = colWeightsTest1, prevCutOff=0.01))
  test1 = mccollect(testJob1, FALSE)[[1]]
  
 testJob2 = mcparallel(RCM(dataMatNBNS, method="NB_1B1", k=3, nleqslv.control= nleqslv.control, maxItOut=1e3, rowWeights = "marginal",colWeights = colWeightsTest2, prevCutOff=0.01))
  test2 = mccollect(testJob2, FALSE)[[1]]
  
   testJob3 = mcparallel(RCM(dataMatNBNS, method="NB_1B1", k=3, nleqslv.control= nleqslv.control, maxItOut=1e3, rowWeights = "marginal",colWeights = colWeightsTest3, prevCutOff=0.01))
  test3 = mccollect(testJob3, FALSE)[[1]]
  
   testJob1unif = mcparallel(RCM(dataMatNBNS, method="NB_1B1", k=3, nleqslv.control= nleqslv.control, maxItOut=1e3, rowWeights = "uniform",colWeights = colWeightsTest1, prevCutOff=0.01))
  test1unif = mccollect(testJob1unif, FALSE)[[1]]
  
 testJob2unif = mcparallel(RCM(dataMatNBNS, method="NB_1B1", k=3, nleqslv.control= nleqslv.control, maxItOut=1e3, rowWeights = "uniform",colWeights = colWeightsTest2, prevCutOff=0.01))
  test2unif = mccollect(testJob2unif, FALSE)[[1]]
  
   testJob3unif = mcparallel(RCM(dataMatNBNS, method="NB_1B1", k=3, nleqslv.control= nleqslv.control, maxItOut=1e3, rowWeights = "uniform",colWeights = colWeightsTest3, prevCutOff=0.01))
  test3unif = mccollect(testJob3unif, FALSE)[[1]]
  
   syntNBNSmargmarg_1B1_2JobMLE = mcparallel(RCM(dataMatNBNS, method="NB_1B1", k=2, nleqslv.control= nleqslv.control, maxItOut=1e3, colWeights = "marginal",rowWeights ="marginal", prevCutOff=0.01, marginEst = "MLE"))
  syntNBNSmargmarg_1B1_2MLE = mccollect(syntNBNSmargmarg_1B1_2JobMLE, FALSE)[[1]]
  
      syntNBNSunifmarg_1B1_2JobMLE = mcparallel(RCM(dataMatNBNS, method="NB_1B1", k=2, nleqslv.control= nleqslv.control, maxItOut=1e3, colWeights = "marginal",rowWeights ="uniform",prevCutOff=0.01, marginEst = "MLE"))
  syntNBNSunifmarg_1B1_2MLE = mccollect(syntNBNSunifmarg_1B1_2JobMLE, FALSE)[[1]]
  
      syntNBNSunifunif_1B1_2JobMLE = mcparallel(RCM(dataMatNBNS, method="NB_1B1", k=2, nleqslv.control= nleqslv.control, maxItOut=1e3, colWeights = "uniform",rowWeights ="uniform", prevCutOff=0.01, marginEst = "MLE"))
  syntNBNSunifunif_1B1_2MLE = mccollect(syntNBNSunifunif_1B1_2JobMLE, FALSE)[[1]]
  
      syntNBNSmargunif_1B1_2JobMLE = mcparallel(RCM(dataMatNBNS, method="NB_1B1", k=2, nleqslv.control= nleqslv.control, maxItOut=1e3, rowWeights = "marginal",colWeights ="uniform", prevCutOff=0.01, marginEst = "MLE"))
  syntNBNSmargunif_1B1_2MLE = mccollect(syntNBNSmargunif_1B1_2JobMLE, FALSE)[[1]]

  #Save the results
  save( syntNBNSunifmarg_1B1_3,syntNBNSmargunif_1B1_3,syntNBNSmargmarg_1B1_3,syntNBNSunifunif_1B1_3, dataMatNBNS,test1, test2, test3, test1unif, test2unif, test3unif, syntNBNSunifunif_1B1_2MLE, syntNBNSunifmarg_1B1_2MLE, syntNBNSmargunif_1B1_2MLE, syntNBNSmargmarg_1B1_2MLE ,file="toyDataNS.RData" ) #syntNBNSunif,syntNBNSunif_1B1, yntNBNSmarg,syntNBNSmarg_1B1,
} else {load("toyDataNS.RData")}
```

##### Plot the results

```{r Plot results no signal, purl=FALSE}
solListNS = list("unifunif" = syntNBNSunifunif_1B1_3, "unifmarg" = syntNBNSunifmarg_1B1_3, "margunif" = syntNBNSmargunif_1B1_3, "margmarg" = syntNBNSmargmarg_1B1_3,"unifmargunifRmarg" = test1,  "margmargunifRmarg"=test2, "margunifunifRmarg" = test3, "unifmargunifRunif" = test1unif, "margmargunifRunif"=test2unif, "margunifunifRunif" = test3unif, "MLEunifunif" = syntNBNSunifunif_1B1_2MLE, "MLEunifmarg" = syntNBNSunifmarg_1B1_2MLE, "MLEmargunif" = syntNBNSmargunif_1B1_2MLE, "MLEmargmarg" = syntNBNSmargmarg_1B1_2MLE)#
#Runtimes and convergence
sapply(solListNS,function(x){x$converged})
sapply(solListNS,function(x){x$runtime})

par(mfrow=c(3,5))
#unifmarg
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {
  rMatPsi = rMat %*% diag(psis)
  dfCol = data.frame(Dim1=rMatPsi[,1], Dim2=rMatPsi[,2], col=rowSums(X))
  ggplot(data=dfCol, aes(x=Dim1, y=Dim2, col=col)) +geom_point(size=2) +ggtitle(Y)
    })
  })
#There definitely is a trend: larger library sizes, larger dim1 scores. the library size signal has not completely disappeared yet
#Weighting scheme for colscore normalization does not affect the samples plot, but the centering weights do

lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,t(cMat))}) })
#Marginal column centering is clearly needed to avoid the outliers
#Orthogonality weighting does not seem to matter much, especially the centering weights do!

#With MLE estimation, the outliers are gone!

lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,(rMat %*% diag(psis))[,2:3])})})

lapply(names(solListNS), function(Y){with(solListNS[[Y]], {
  rMatPsi = rMat %*% diag(psis)
  dfCol = data.frame(Dim3=rMatPsi[,3], Dim2=rMatPsi[,2], col=rowSums(X))
  ggplot(data=dfCol, aes(x=Dim2, y=Dim3, col=col)) +geom_point(size=2) +ggtitle(Y)
    })
  }) #In higher dimensions library sizes seem to matter much
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,t(cMat)[,2:3])}) })
# No signal in either plot. For the uniform taxon weighting the first dimension of the plot is dominated by outliers, but not the other dimensions

#Look at the loadings in function of the library sizes and abunds
par(pty = "m")
#Libsizes
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,rMat[,1] *psis[1] ,rowSums(X) )})})
#In all weighing schemes, rowScores increase with library size (not in absolute values but they form a gradient). Is this correct(i.e. represents the noise in the data? E.g. large counts in abundant taxa increase the library size?)

#Indeed the relationship with the library sizes is gone!

#lapply(names(solListNS), function(Y){with(solListNS[[Y]], {lines(lowess(rMat[,1] *psis[1], rowSums(X)))})})
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,rMat[,2] *psis[2],rowSums(X) )})})
#lapply(names(solListNS), function(Y){with(solListNS[[Y]], {lines(lowess(rMat[,2] *psis[2], rowSums(X)))})})
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,rMat[,3] *psis[3],rowSums(X) )})})
#lapply(names(solListNS), function(Y){with(solListNS[[Y]], {lines(lowess(rMat[,3] *psis[3], rowSums(X)))})})
# The first dimension scores are related to the library sizes in all weighting schemes

#Abundances
par(mfrow=c(3,5))
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,cMat[1,], colSums(X), log="y", cex=0.5)})})
#lapply(names(solListNS), function(Y){with(solListNS[[Y]], {lines(lowess(cMat[1,], colSums(X)))})})
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,cMat[2,], colSums(X), log="y", cex=0.5)})})
#lapply(names(solListNS), function(Y){with(solListNS[[Y]], {lines(lowess(cMat[2,], colSums(X)))})})
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,cMat[3,], colSums(X), log="y", cex=0.5)})})
#lapply(names(solListNS), function(Y){with(solListNS[[Y]], {lines(lowess(cMat[3,], colSums(X)))})})
lapply(names(solListNS), function(Y){with(solListNS[[Y]], {plot(main=Y,abs(cMat[1,]), colSums(X), log="y")})})
#The least abundant species get higher scores in all weighting schemes

#And no more species outliers!!

par(pty = "s", mfrow=c(1,1))
```

##### Influence function

Take a look at the influence function values on the psis

```{r NS influence measures: psis}
infl1 = lapply(solListNS, function(x, i){
  infl=with(x, NBpsiInfl(psi = psis[i], X = X, cMat = cMat[i,,drop=FALSE], rMat = rMat[,i, drop=FALSE], muMarg = outer(rowSums(X), colSums(X)/sum(X)), theta = thetas))
  id=abs(infl) > quantile( abs(infl),0.995)
  Xid = x$X[id]
  abunds = colSums(x$X)/sum(x$X)
  libsizes = rowSums(x$X)
  list(infl = infl, id = id, Xid = Xid, abunds=abunds, libsizes=libsizes)
},1)
par(mfrow=c(2,5))
lapply(names(infl1), function(x){
  with(infl1[[x]], plot(abunds, colSums(id), log="x", main=x))
})
lapply(names(infl1), function(x){
  with(infl1[[x]], plot(rep(abunds, nrow(infl)), c(t(infl)), log="x", main=x)) #Very heavy!
})
lapply(names(infl1), function(x){
  IdTmp = sample(seq_along(infl1[[x]]$libsizes), 200)
  with(infl1[[x]], plot(rep(libsizes[IdTmp], ncol(infl)), c(infl[IdTmp,]), log="x", main=x, col=(libsizes==libsizes[IdTmp])+1)) #Very heavy!
})
#Expectations
lapply(names(infl1), function(x){
  with(infl1[[x]], {rbind(quantile(outer(libsizes, abunds)),
       quantile(outer(libsizes, abunds)[id])) })
})
lapply(names(infl1), function(x){
  cat(x, "\n")
  with(infl1[[x]], table(Xid))
})

infl2 = lapply(solListNS, function(x, i){
  infl=with(x, NBpsiInfl(psi = psis[i], X = X, cMat = cMat[i,,drop=FALSE], rMat = rMat[,i, drop=FALSE], muMarg = outer(rowSums(X), colSums(X)/sum(X)), theta = thetas))
  id=abs(infl) > quantile( abs(infl),0.995)
  Xid = x$X[id]
  abunds = colSums(x$X)/sum(x$X)
  libsizes = rowSums(x$X)
  list(infl = infl, id = id, Xid = Xid, abunds=abunds, libsizes=libsizes)
},2)
par(mfrow=c(2,2))
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(abunds, colSums(id), log="x", main=x))
})
#In the second dimension, larger abundance means larger influence on the psis too, and no more outliers
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(rep(abunds, nrow(infl)), c(t(infl)), log="x", main=x)) #Very heavy!
}) #Outlier in unifmarg weigthing scheme
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(rep(libsizes, ncol(infl)), c(infl), log="x", main=x)) #Very heavy!
})
#Expectations
lapply(names(infl2), function(x){
  with(infl2[[x]], {rbind(quantile(outer(libsizes, abunds)),
       quantile(outer(libsizes, abunds)[id])) })
})
lapply(names(infl2), function(x){
  cat(x, "\n")
  with(infl2[[x]], table(Xid))
})
```

The most influential observations are zero counts in lowly abundant species and high libsizes (i.e. high expectations)! Under the uniform weighting scheme for the taxa, one single observation will get an enormous influence in the first dimension, this is in all likelihood the outlier. Note that we cen derive this from the influence function that does not even depend on the weights!

Influence on the colScores

```{r NBNS influence colscores}
inflCol1 = lapply(solListNS, function(x){
  try(with(x, NBcolInfl(X, psis, cMat, rMat,thetas , colWeights , k=1 , lambdaCol)))
})
#Only look at most extreme colscores
#Cannot calculate influence functions for the interesting cases (with the outliers)

```

```{r NBNS influence rowscores}
inflRowList1 = lapply(solListNS, function(x){
  try(with(x, NBrowInfl(X, psis, cMat, rMat,thetas , rowWeights , k=1 , lambdaRow)))
})
#Only look at most extreme colscores
#Cannot calculate influence functions for the interesting cases (with the outliers)
inflRowList1=inflRowList1[sapply(inflRowList1, class)=="list"]
#The influence on the first row score
inflRow1 = lapply(inflRowList1, function(x){getInflRow(x$score, x$InvJac, 1)}) 
par(mfrow=c(1,2))
libSizes = rowSums(solListNS[[1]]$X)
lapply(inflRow1, function(x){
  plot(y=x[,1], libSizes, log="x")
})
```

##### Conclusions from the null plots

 - No signal in either plot. 
 - For the uniform taxon weighting the first dimension of the plot is dominated by outliers
 - The first dimension row scores are related to the library sizes in all weighting schemes
 - The least abundant species get higher column scores in all weighting schemes
 
#### NB with signal

##### Create the data

Here I use a sort of "method-based" approach, declaring the parameters as above in our RC(M) model. One might also just modify the abundances directly in some of the samples for a subset of taxa, but it is very hard to achieve orthogonality of signals in that case.

And now the "model-based" apporach

```{r NB model based data with signal, purl=TRUE, eval=FALSE}
#Define parameters
Nsamples= 200
Ntaxa = 900
thetasNB=sample(thetas, Ntaxa)
thetasNB = thetasNB[1/thetasNB<100]
rhosNB=rhos[names(thetas)]
Ntaxa=length(rhosNB)- length(rhosNB)%%2
rhosNB = rhosNB/sum(rhosNB); rhosNB=rhosNB[1:Ntaxa]; thetasNB=thetasNB[1:Ntaxa]
libSizes4 =c(rep(1e4, floor(Nsamples/2)), rep(1e5, floor(Nsamples/2)))
NsamplesChanged = 40
NtaxaChanged = 90


#Psis (relative importance of the dimensions and signal strength)
psi1 = log(7)
psi2 = log(6)

#Row scores (add three standard deviations)

rChange1 = rChange2 = integer(Nsamples)
rChange1[sample(size = NsamplesChanged, 1:Nsamples)] = 1
rChange2[sample(size = NsamplesChanged, 1:Nsamples)] = 1

#Logical indices for these
rid1 = rChange1!=0
rid2 = rChange2!=0

rowScores1 =  rChange1 # + rnorm(Nsamples-Nsamples%%2, sd=SD)
rowScores2 =  rChange2 # + rnorm(Nsamples-Nsamples%%2, sd=SD) 

#Column scores
cChange1 = cChange2 = integer(Ntaxa)
cChange1[sample(size = NtaxaChanged, 1:Ntaxa)] = 1
cChange2[sample(size = NtaxaChanged, 1:Ntaxa)] = 1

cid1 = cChange1!=0
cid2 = cChange2!=0

colScores1 = cChange1 #rnorm(Ntaxa-Ntaxa%%2, sd = SD) + cChange1 #Why this additional noise? The parameters need not be random after all. The randomness will come from the NB estimation. We have been too pessimistic about the method so far
colScores2 = cChange2  #rnorm(Ntaxa-Ntaxa%%2, sd = SD) + cChange2

# A function to renoralize the scores
normalize = function(mat, dim,weights=rep(1/dim(mat)[3-dim], dim(mat)[3-dim])){
  mat = apply(mat,dim, function(vec){
    vec - sum(vec*weights)/sum(weights)#
  })
  if(dim==1) mat= t(mat)
  mat = apply(mat, dim, function(y){
    y/sqrt(sum(weights*y^2))
  })
  if(dim==1) mat =t(mat)
  return(mat)
}

#Matrices of scores, mean and dispersions
rowScoresMat = cbind(rowScores1, rowScores2)
colScoresMat = rbind(colScores1, colScores2)
meanMat = outer(libSizes4, rhosNB)* (exp(psi1*outer(rowScoresMat[,1],colScoresMat[1,]) + psi2*outer(rowScoresMat[,2],colScoresMat[2,])))
thetaMat = matrix(thetasNB, nrow=Nsamples, ncol=Ntaxa, byrow=TRUE)

#The final data matrix
dataMatNBSig = apply(array(data= c(meanMat, thetaMat), dim=c(Nsamples, Ntaxa, 2)), c(1,2), function(x){rnbinom(1,mu=x[1], size=x[2])})
colnames(dataMatNBSig) = names(cid1) = names(cid2) = names(rhosNB)

#Trim data and scores
cid1 = cid1[ colSums(dataMatNBSig) > 0]
cid2 = cid2[colSums(dataMatNBSig) > 0 ]
colScoresMat=colScoresMat[, colSums(dataMatNBSig) > 0]
rowScoresMat =rowScoresMat[rowSums(dataMatNBSig)>0,]
dataMatNBSig = dataMatNBSig[rowSums(dataMatNBSig)>0, colSums(dataMatNBSig) > 0]
rownames(dataMatNBSig) = paste0("Sample", 1:Nsamples)
rhosNB=rhosNB[colnames(dataMatNBSig)]
thetas =thetas[colnames(dataMatNBSig)]

# The signals
taxaSigNB = factor(mapply(cid1,cid2,FUN=paste), levels=c("FALSE FALSE", "TRUE FALSE","FALSE TRUE","TRUE TRUE"),  labels=c("Reference", "Signal 1","Signal2", "Signal 1 and 2"))
sampleSigNB = factor(mapply(rid1,rid2,FUN=paste), levels=c("FALSE FALSE", "TRUE FALSE","FALSE TRUE","TRUE TRUE"), labels=c("Reference", "Signal 1","Signal2", "Signal 1 and 2"))
```

##### Fit the RC(M) model

Test runs show that uniform weights for the taxa are not a good idea: always some taxa become outliers. Therefor they are omitted in the comparisons to come.

```{r fit RC(M) NB with signal, purl=FALSE, eval=FALSE}
nleqslv.control = list(trace=TRUE, maxit = 250, cndtol=.Machine$double.eps)
if(!file.exists("toyDataSig.RData")){

    syntNBSigmargmarg_1B1_2Job = mcparallel(RCM(dataMatNBSig, method="NB_1B1", k=2, nleqslv.control= nleqslv.control, maxItOut=2e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "marginal"))
  syntNBSigmargmarg_1B1_2 = mccollect(syntNBSigmargmarg_1B1_2Job, FALSE)[[1]] 
  
      syntNBSigunifmarg_1B1_2Job = mcparallel(RCM(dataMatNBSig, method="NB_1B1", k=2, nleqslv.control= nleqslv.control, maxItOut=2e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform"))
  syntNBSigunifmarg_1B1_2 = mccollect(syntNBSigunifmarg_1B1_2Job, FALSE)[[1]] 
  
      syntNBSigmargunif_1B1_2Job = mcparallel(RCM(dataMatNBSig, method="NB_1B1", k=2, nleqslv.control= nleqslv.control, maxItOut=2e3, prevCutOff=0.01, rowWeights="marginal", colWeights = "uniform"))
  syntNBSigmargunif_1B1_2 = mccollect(syntNBSigmargunif_1B1_2Job, FALSE)[[1]] 
  
      syntNBSigunifunif_1B1_2Job = mcparallel(RCM(dataMatNBSig, method="NB_1B1", k=2, nleqslv.control= nleqslv.control, maxItOut=2e3, prevCutOff=0.01, colWeights="uniform", rowWeights = "uniform"))
  syntNBSigunifunif_1B1_2 = mccollect(syntNBSigunifunif_1B1_2Job, FALSE)[[1]] 

  save(dataMatNBSig, syntNBSigunifmarg_1B1_2, syntNBSigunifunif_1B1_2, syntNBSigmargmarg_1B1_2,syntNBSigmargunif_1B1_2, taxaSigNB, sampleSigNB, file="toyDataSig.RData") #syntNBSigunif, syntNBSigmarg, syntNBSigmarg_3,
} else {load("toyDataSig.RData")}

# A function to pour the results into a dataframe
# RCMres2DF = function(RCM,  sampleGroups, taxaGroups){
#   tmpRow = RCM$rMat %*% diag(RCM$psis)
#   colnames(tmpRow) = paste0("R",1:RCM$k)
#   tmpCol = t(RCM$cMat)
#   colnames(tmpCol) = paste0("C",1:RCM$k)
#   list(rows =data.frame( tmpRow ,  sampleGroups=sampleGroups), columns = data.frame(tmpCol, taxaGroups=taxaGroups))
# }
# 
# RCMplot = function(df, colours = c("grey50","blue","red","purple", "green","cyan","brown","magenta"), dim = c(1,2), plotType = "rows"){
#   df2 = df[[plotType]]
#   df2$x = df2[[paste0("R", dim[1])]]
#   df2$y = df2[[paste0("R", dim[2])]]
#   ggplot(data=df2, aes(x=x, y=y, col=sampleGroups)) + geom_point() + scale_colour_manual(values=colours)
# }
```

##### Sample plot

```{r 3 plots, eval=FALSE}
solListWS3 = list("unifunif" = syntNBSigunifunif_1B1_2, "unifmarg" = syntNBSigunifmarg_1B1_2, "margunif" = syntNBSigmargunif_1B1_2, "margmarg" = syntNBSigmargmarg_1B1_2)
solListWS3 = solListWS3[sapply(solListWS3, class)=="list"]
#Runtimes and convergence
sapply(solListWS3,function(x){x$converged}) #No convergence 2nd dimension margmarg despite long fit
sapply(solListWS3,function(x){x$runtime})
sapply(solListWS3,function(x){x$iter})

par(mfrow=c(1,2))
#unifmarg
# lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {plot(main=Y,rMat %*% diag(psis))})})
# lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {plot(main=Y,t(cMat))}) })
# lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {plot(main=Y,(rMat %*% diag(psis))[,2:3])})})
# lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {plot(main=Y,t(cMat)[,2:3])}) })
# Uniform taxon weighting clearly is not an option

cols = c("grey", "red","blue","purple","green","brown","cyan","black")
palette(cols)
lapply(names(solListWS3), function(Y){plotRCM(solListWS3[[Y]], samColour = sampleSigNB, main=Y, biplot=FALSE, Dim = c(1,2), libInset = c(-0.81,-0.1))})
lapply(names(solListWS3), function(Y){plotRCM(solListWS3[[Y]], samColour = sampleSigNB, main=Y, biplot=FALSE, Dim = c(1,3), libInset = c(-0.51,-0.1))})
lapply(names(solListWS3), function(Y){plotRCM(solListWS3[[Y]], samColour = sampleSigNB, main=Y, biplot=FALSE, Dim = c(2,3), libInset = c(-1.21,-0.1))})
```

We'll probably need stronger signals It is clear that we need marginal weighting for the taxa. Look a bit deeper at these plots

```{r 3 plots2 marginal taxon weighting, eval=FALSE}
subList3 = solListWS3[c("unifmarg","margmarg")]
mapply(list(c(1,2), c(1,3),c(1,2), c(1,3)),rep(names(subList3),each=2), FUN = function(x,Y){plotRCM(solListWS3[[Y]], Colour = sampleSigNB, main=Y, biplot=FALSE, Dim = x)})
```

The results are very similar, but marginal weighting of the samples seems to result in a better separation. Let's look at the biplots

```{r 3 biplots, eval=FALSE}
par(mfrow=c(2,2))
lapply(names(solListWS3), function(Y){plotRCM(solListWS3[[Y]], samColour = sampleSigNB3, main=Y, biplot=TRUE, Dim = c(1,2), taxColour=taxaSigNB3)})
```

Also here it is clear that we need marginal taxon weights. Look in the other dimensions

```{r 3 biplots margTaxWeights, eval=FALSE}
par(mfrow=c(2,2))
mapply(list(c(1,2), c(1,3),c(1,2), c(1,3)),rep(names(subList3),each=2), FUN = function(x,Y){plotRCM(subList3[[Y]], samColour = sampleSigNB3, main=Y, biplot=TRUE, Dim = x, taxColour=taxaSigNB3)})
par(mfrow=c(1,1))
Y="margmarg"
plotRCM(subList3[[Y]], samColour = sampleSigNB, main=Y, biplot=TRUE, Dim = c(1,2), taxColour=taxaSigNB)
```

With uniform sample weighting the plot also tends to be dominated by a few outliers. The taxa are correctly identified

##### Relationship to library sizes/abundances

```{r 3 libsizes, eval=FALSE}
#Look at the loadings in function of the library sizes and abunds
par(pty = "m")
logP="y"
#Libsizes
lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {plot(main=Y,rMat[,1] *psis[1],rowSums(X), log=logP )})})
#lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {lines(lowess(rMat[,1] *psis[1], rowSums(X)))})})
lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {plot(main=Y,rMat[,2] *psis[2],rowSums(X), log=logP )})})
#lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {lines(lowess(rMat[,2] *psis[2], rowSums(X)))})})
lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {plot(main=Y,rMat[,3] *psis[3],rowSums(X), log=logP )})})
#lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {lines(lowess(rMat[,3] *psis[3], rowSums(X)))})})
```

The first dimension scores are related to the library sizes in all weighting schemes

```{r 3 abunds, eval=FALSE}
logA = "y"
#Abundances
lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {plot(main=Y,cMat[1,], colSums(X), log=logA, cex=0.5)})})
#lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {lines(lowess(cMat[1,], colSums(X)))})})
lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {plot(main=Y,cMat[2,], colSums(X), log=logA)})})
#lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {lines(lowess(cMat[2,], colSums(X)))})})
lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {plot(main=Y,cMat[3,], colSums(X), log=logA)})})
#lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {lines(lowess(cMat[3,], colSums(X)))})})
lapply(names(solListWS3), function(Y){with(solListWS3[[Y]], {plot(main=Y,abs(cMat[1,]), colSums(X), log=logA)})})
par(pty = "s", mfrow=c(1,1))
```

The least abundant species get higher scores in all weighting schemes

#### Three dimensional signal

```{r NB_3D, eval=FALSE}
psi3 = log(9)

#Row scores (add three standard deviations)
rChange3 = integer(Nsamples)
rChange3[sample(1:Nsamples, NsamplesChanged)] = 1

#Logical indices for these
rid3 = rChange3!=0

rowScores3 =  rChange3

#Column scores
cChange3 = integer(Ntaxa)
cChange3[sample(1:Ntaxa, NtaxaChanged)] = 1

cid3 = cChange3!=0

colScores3 = cChange1

#Matrices of scores, mean and dispersions
rowScoresMat3 = normalize(cbind(rowScores1, rowScores2, rowScores3),dim=2,weights=rep(1, Nsamples))
colScoresMat3 = normalize(rbind(colScores1, colScores2, colScores3),dim=1,weights=rep(1, Ntaxa))
meanMat3 = outer(libSizes4, rhos)* exp(rowScoresMat3 %*% (colScoresMat3 *c(psi1,psi2,psi3)))
thetaMat3 = matrix(thetas, nrow=Nsamples, ncol=Ntaxa, byrow=TRUE)

#The final data matrix
dataMatNBSig3 = apply(array(data= c(meanMat3, thetaMat3), dim=c(Nsamples, Ntaxa, 2)), c(1,2), function(x){rnbinom(1,mu=x[1], size=x[2])})
colnames(dataMatNBSig3) = names(rhos)

#Trim data and scores
cid3 = cid3[ colSums(dataMatNBSig3) > 0]
colScoresMat3=colScoresMat3[, colSums(dataMatNBSig3) > 0]
rowScoresMat3 =rowScoresMat3[rowSums(dataMatNBSig3)>0,]
dataMatNBSig3 = dataMatNBSig3[rowSums(dataMatNBSig3)>0, colSums(dataMatNBSig3) > 0]
rownames(dataMatNBSig3) = paste0("Sample", 1:Nsamples)
rhos=rhos[colnames(dataMatNBSig3)]
thetas =thetas[colnames(dataMatNBSig3)]

# The signals
taxaSigNB3 = factor(mapply(cid1,cid2,cid3,FUN=paste), levels=c("FALSE FALSE FALSE", "TRUE FALSE FALSE","FALSE TRUE FALSE","TRUE TRUE FALSE","FALSE FALSE TRUE", "TRUE FALSE TRUE","FALSE TRUE TRUE","TRUE TRUE TRUE"),  labels=c("Reference", "Signal 1","Signal 2", "Signal 1 and 2","Signal 3", "Signal 1 and 3","Signal 2 and 3", "Signal 1 and 2 and 3"))
sampleSigNB3 = factor(mapply(rid1,rid2,rid3,FUN=paste),  levels=c("FALSE FALSE FALSE", "TRUE FALSE FALSE","FALSE TRUE FALSE","TRUE TRUE FALSE","FALSE FALSE TRUE", "TRUE FALSE TRUE","FALSE TRUE TRUE","TRUE TRUE TRUE"),  labels=c("Reference", "Signal 1","Signal 2", "Signal 1 and 2","Signal 3", "Signal 1 and 3","Signal 2 and 3", "Signal 1 and 2 and 3"))
```

#### Fit

```{r Fit3DNB, eval=FALSE}
nleqslv.control3 = list(trace=TRUE, maxit = 250, cndtol=.Machine$double.eps)
if(!file.exists("toyDataSig3D.RData")){

    syntNBSigmargmarg_1B1_3DJob = mcparallel(RCM(dataMatNBSig3, method="NB_1B1", k=3, nleqslv.control= nleqslv.control3, maxItOut=2e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "marginal"))
  syntNBSigmargmarg_1B1_3D = mccollect(syntNBSigmargmarg_1B1_3DJob, FALSE)[[1]] 
  
      syntNBSigunifmarg_1B1_3DJob = mcparallel(RCM(dataMatNBSig3, method="NB_1B1", k=3, nleqslv.control= nleqslv.control3, maxItOut=2e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform"))
  syntNBSigunifmarg_1B1_3D = mccollect(syntNBSigunifmarg_1B1_3DJob, FALSE)[[1]] 
  
      syntNBSigmargunif_1B1_3DJob = mcparallel(RCM(dataMatNBSig3, method="NB_1B1", k=3, nleqslv.control= nleqslv.control3, maxItOut=2e3, prevCutOff=0.01, rowWeights="marginal", colWeights = "uniform"))
  syntNBSigmargunif_1B1_3D = mccollect(syntNBSigmargunif_1B1_3DJob, FALSE)[[1]] 
  
      syntNBSigunifunif_1B1_3DJob = mcparallel(RCM(dataMatNBSig3, method="NB_1B1", k=3, nleqslv.control= nleqslv.control3, maxItOut=2e3, prevCutOff=0.01, colWeights="uniform", rowWeights = "uniform"))
  syntNBSigunifunif_1B1_3D = mccollect(syntNBSigunifunif_1B1_3DJob, FALSE)[[1]] 

  save(dataMatNBSig3, syntNBSigunifmarg_1B1_3D, syntNBSigunifunif_1B1_3D,syntNBSigmargmarg_1B1_3D,syntNBSigmargunif_1B1_3D,taxaSigNB3, sampleSigNB3, file="toyDataSig3D.RData") #syntNBSigunif, syntNBSigmarg, syntNBSigmarg_3,
} else {
  load("toyDataSig3D.RData")
  }
```

##### Sample plot

```{r 3D plots, eval=FALSE}
solListWS3D = list("unifunif" = syntNBSigunifunif_1B1_3D, "unifmarg" = syntNBSigunifmarg_1B1_3D, "margunif" = syntNBSigmargunif_1B1_3D, "margmarg" = syntNBSigmargmarg_1B1_3D)
solListWS3Dplot = lapply(solListWS3D,RCMres2DF, sampleGroups= sampleSigNB3, taxaGroups=taxaSigNB3[colMeans(dataMatNBSig3==0)<0.99])
#Runtimes and convergence
sapply(solListWS3D,function(x){x$converged})
sapply(solListWS3D,function(x){x$runtime})
sapply(solListWS3D,function(x){x$iter})

par(mfrow=c(2,2))
#unifmarg
# lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {plot(main=Y,rMat %*% diag(psis))})})
# lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {plot(main=Y,t(cMat))}) })
# lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {plot(main=Y,(rMat %*% diag(psis))[,2:3])})})
# lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {plot(main=Y,t(cMat)[,2:3])}) })
# Uniform taxon weighting clearly is not an option

palStore = palette()

cols = c("grey", "red","blue","purple","green","brown","cyan","black")
palette(cols)
lapply(names(solListWS3D), function(Y){plotRCM(solListWS3D[[Y]], samColour = sampleSigNB3, main=Y, biplot=FALSE, Dim = c(1,2))})
lapply(names(solListWS3D), function(Y){plotRCM(solListWS3D[[Y]], samColour = sampleSigNB3, main=Y, biplot=FALSE, Dim = c(1,3))})
lapply(names(solListWS3D), function(Y){plotRCM(solListWS3D[[Y]], samColour = sampleSigNB3, main=Y, biplot=FALSE, Dim = c(2,3))})
```

It is clear that we need marginal weighting for the taxa. Look a bit deeper at these plots

```{r 3D plots2 marginal taxon weighting, eval=FALSE}
subList3D = solListWS3D[c("unifmarg","margmarg")]
mapply(list(c(1,2), c(1,3),c(1,2), c(1,3)),rep(names(subList3D),each=2), FUN = function(x,Y){plotRCM(solListWS3D[[Y]], Colour = sampleSigNB3, main=Y, biplot=FALSE, Dim = x)})
palette(palStore)
```

The results are very similar, but marginal weighting of the samples seems to result in a better separation. Let's look at the biplots

```{r 3D biplots, eval=FALSE}
par(mfrow=c(2,2))
lapply(names(solListWS3D), function(Y){plotRCM(solListWS3D[[Y]], samColour = sampleSigNB3, main=Y, biplot=TRUE, Dim = c(1,2), taxColour=taxaSigNB3)})
```

Also here it is clear that we need marginal taxon weights. Look in the other dimensions

```{r 3D biplots margTaxWeights, eval=FALSE}
par(mfrow=c(2,2))
mapply(list(c(1,2), c(1,3),c(1,2), c(1,3)),rep(names(subList3D),each=2), FUN = function(x,Y){plotRCM(subList3D[[Y]], samColour = sampleSigNB3, main=Y, biplot=TRUE, Dim = x, taxColour=taxaSigNB3)})
par(mfrow=c(1,1))
Y="unifmarg"
plotRCM(subList3D[[Y]], samColour = sampleSigNB3, main=Y, biplot=TRUE, Dim = c(1,3), taxColour=taxaSigNB3)
```

With uniform sample weighting the plot also tends to be dominated by a few outliers. The taxa are correctly identified

##### Relationship to library sizes/abundances

```{r 3D libsizes, eval=FALSE}
#Look at the loadings in function of the library sizes and abunds
par(pty = "m")
logP="y"
#Libsizes
lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {plot(main=Y,rMat[,1] *psis[1],rowSums(X), log=logP )})})
#lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {lines(lowess(rMat[,1] *psis[1], rowSums(X)))})})
lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {plot(main=Y,rMat[,2] *psis[2],rowSums(X), log=logP )})})
#lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {lines(lowess(rMat[,2] *psis[2], rowSums(X)))})})
lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {plot(main=Y,rMat[,3] *psis[3],rowSums(X), log=logP )})})
#lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {lines(lowess(rMat[,3] *psis[3], rowSums(X)))})})
```

The first dimension scores are related to the library sizes in all weighting schemes

```{r 3D abunds, eval=FALSE}
logA = "y"
#Abundances
lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {plot(main=Y,cMat[1,], colSums(X), log=logA, cex=0.5)})})
#lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {lines(lowess(cMat[1,], colSums(X)))})})
lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {plot(main=Y,cMat[2,], colSums(X), log=logA)})})
#lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {lines(lowess(cMat[2,], colSums(X)))})})
lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {plot(main=Y,cMat[3,], colSums(X), log=logA)})})
#lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {lines(lowess(cMat[3,], colSums(X)))})})
lapply(names(solListWS3D), function(Y){with(solListWS3D[[Y]], {plot(main=Y,abs(cMat[1,]), colSums(X), log=logA)})})
par(pty = "s", mfrow=c(1,1))
```

The least abundant species get higher scores in all weighting schemes

#### The abundance based approach

With modified abundances (4 groups)

```{r NB with signal}
rhosNBSigref = rhosNBSig1 = rhosNBSig2 = rhosNBSig3 = rhosNBSig4 = rhosNBNS

NsamplesSignal1 = NsamplesSignal2 =  20
NsamplesSignal3 = NsamplesSignal4 =  15

NtaxaSignalNB = 40

Signal1NB = 10
Signal2NB = 8
Signal3NB = 7.5
Signal4NB = 7

idSig1NB = sample(1:NtaxaNBNS, NtaxaSignalNB) #Random sampling should ensure orthogonality
idSig2NB = sample(1:NtaxaNBNS, NtaxaSignalNB)
idSig3NB = sample(1:NtaxaNBNS, NtaxaSignalNB) #Random sampling should ensure orthogonality
idSig4NB = sample(1:NtaxaNBNS, NtaxaSignalNB)
#Apply the signals

rhosNBSig1[idSig1NB] = rhosNBSig1[idSig1NB]*Signal1NB
rhosNBSig2[idSig2NB] = rhosNBSig2[idSig2NB]*Signal2NB
rhosNBSig3[idSig3NB] = rhosNBSig3[idSig3NB]*Signal3NB
rhosNBSig4[idSig4NB] = rhosNBSig4[idSig4NB]*Signal4NB

#Renormalize
renorm=function(x){x/sum(x)}
rhosNBSig1=renorm(rhosNBSig1);rhosNBSig2=renorm(rhosNBSig2);
rhosNBSig3=renorm(rhosNBSig3);rhosNBSig4=renorm(rhosNBSig4);

#Generate data
Nref = (NsamplesNBNS-NsamplesSignal1-NsamplesSignal2-NsamplesSignal3-NsamplesSignal4)
meanMatRefNB = outer(libSizesNBNS[sample(size=Nref, 1:NsamplesNBNS)], rhosNBSigref)
meanMatSig1NB = outer(libSizesNBNS[sample(size=NsamplesSignal1, 1:NsamplesNBNS)], rhosNBSig1)
meanMatSig2NB = outer(libSizesNBNS[sample(size=NsamplesSignal2, 1:NsamplesNBNS)], rhosNBSig2)
meanMatSig3NB = outer(libSizesNBNS[sample(size=NsamplesSignal3, 1:NsamplesNBNS)], rhosNBSig3)
meanMatSig4NB = outer(libSizesNBNS[sample(size=NsamplesSignal4, 1:NsamplesNBNS)], rhosNBSig4)
thetaMatSigNB = thetaMatNBNS

dataMatRefNB = makeNBdata(meanMatRefNB, thetaMatSigNB)
dataMatSig1NB = makeNBdata(meanMatSig1NB, thetaMatSigNB)
dataMatSig2NB = makeNBdata(meanMatSig2NB, thetaMatSigNB)
dataMatSig3NB = makeNBdata(meanMatSig3NB, thetaMatSigNB)
dataMatSig4NB = makeNBdata(meanMatSig4NB, thetaMatSigNB)

dataMatSigNBab = rbind(dataMatRefNB, dataMatSig1NB, dataMatSig2NB, dataMatSig3NB,dataMatSig4NB)

#Save signals
sampleSigNBab = factor(c(rep("Reference", Nref), rep("Signal1", NsamplesSignal1), rep("Signal2", NsamplesSignal2), rep("Signal 3", NsamplesSignal3), rep("Signal 4", NsamplesSignal4)))
taxaSigNBtmp = rep("Reference",ncol(dataMatSigNBab))
taxaSigNBtmp[idSig1NB] = "Signal 1"
taxaSigNBtmp[idSig2NB] = "Signal 2"
taxaSigNBtmp[idSig3NB] = "Signal 3"
taxaSigNBtmp[idSig4NB] = "Signal 4"
taxaSigNBtmp[idSig1NB & idSig2NB] = "Signal 1 and 2"
taxaSigNBtmp[idSig1NB & idSig3NB] = "Signal 1 and 3"
taxaSigNBtmp[idSig1NB & idSig4NB] = "Signal 1 and 4"
taxaSigNBtmp[idSig2NB & idSig3NB] = "Signal 2 and 3"
taxaSigNBtmp[idSig2NB & idSig4NB] = "Signal 2 and 4"
taxaSigNBtmp[idSig3NB & idSig4NB] = "Signal 3 and 4"
taxaSigNBtmp[idSig1NB & idSig2NB & idSig3NB] = "Signal 1 and 2 and 3"
taxaSigNBtmp[idSig4NB & idSig2NB & idSig3NB] = "Signal 2 and 3 and 4"
taxaSigNBtmp[idSig1NB & idSig2NB & idSig4NB] = "Signal 1 and 2 and 4"
taxaSigNBtmp[idSig1NB & idSig4NB & idSig3NB] = "Signal 1 and 3 and 4"
taxaSigNBab = factor(taxaSigNBtmp) 
names(taxaSigNBab) = colnames(dataMatSigNBab) = names(rhosNBSigref)
names(sampleSigNBab) = rownames(dataMatSigNBab) = 1:NsamplesNBNS

libSizesAG = sample_sums(AGphylo)
Nref = (NsamplesNBNS-NsamplesSignal1-NsamplesSignal2-NsamplesSignal3-NsamplesSignal4)
meanMatRefNBag = outer(libSizesAG[sample(size=Nref, 1:NsamplesNBNS)], rhosNBSigref)
meanMatSig1NBag = outer(libSizesAG[sample(size=NsamplesSignal1, 1:NsamplesNBNS)], rhosNBSig1)
meanMatSig2NBag = outer(libSizesAG[sample(size=NsamplesSignal2, 1:NsamplesNBNS)], rhosNBSig2)
meanMatSig3NBag = outer(libSizesAG[sample(size=NsamplesSignal3, 1:NsamplesNBNS)], rhosNBSig3)
meanMatSig4NBag = outer(libSizesAG[sample(size=NsamplesSignal4, 1:NsamplesNBNS)], rhosNBSig4)
thetaMatSigNBag = thetaMatNBNS

dataMatRefNBag = makeNBdata(meanMatRefNBag, thetaMatSigNB)
dataMatSig1NBag = makeNBdata(meanMatSig1NBag, thetaMatSigNB)
dataMatSig2NBag = makeNBdata(meanMatSig2NBag, thetaMatSigNB)
dataMatSig3NBag = makeNBdata(meanMatSig3NBag, thetaMatSigNB)
dataMatSig4NBag = makeNBdata(meanMatSig4NBag, thetaMatSigNB)

dataMatSigNBabag = rbind(dataMatRefNBag, dataMatSig1NBag, dataMatSig2NBag, dataMatSig3NBag, dataMatSig4NBag)
```

#### The abundance based approach: fit

```{r Fit3Dab}
nleqslv.control.ab = list(trace=TRUE, maxit = 500, cndtol=.Machine$double.eps)
if(!file.exists("toyDataSigab.RData")){

    syntNBSigmargmarg_1B1_abJob = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "marginal"))
  syntNBSigmargmarg_1B1_ab = mccollect(syntNBSigmargmarg_1B1_abJob, FALSE)[[1]] 
  
      syntNBSigunifmarg_1B1_abJob = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform"))
  syntNBSigunifmarg_1B1_ab = mccollect(syntNBSigunifmarg_1B1_abJob, FALSE)[[1]] 
  
      syntNBSigmargunif_1B1_abJob = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, rowWeights="marginal", colWeights = "uniform"))
  syntNBSigmargunif_1B1_ab = mccollect(syntNBSigmargunif_1B1_abJob, FALSE)[[1]] 
  
      syntNBSigunifunif_1B1_abJob = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, colWeights="uniform", rowWeights = "uniform"))
  syntNBSigunifunif_1B1_ab = mccollect(syntNBSigunifunif_1B1_abJob, FALSE)[[1]] 
  
    Dim = dim(dataMatSigNBab)
  unifWeightsAb = rep(1/Dim[2], Dim[2])
  margWeightsAb = colSums(dataMatSigNBab)/sum(dataMatSigNBab)
  colWeightsTestAb1 = cbind(unifWeightsAb, margWeightsAb, unifWeightsAb)
  colWeightsTestAb2 = cbind(margWeightsAb, margWeightsAb, unifWeightsAb)
  colWeightsTestAb3 = cbind(margWeightsAb, unifWeightsAb, unifWeightsAb)
  
  testAbJob1 = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=1e3, rowWeights = "marginal", colWeights = colWeightsTestAb1, prevCutOff=0.01))
  testAb1 = mccollect(testAbJob1, FALSE)[[1]]
  
 testAbJob2 = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=1e3, rowWeights = "marginal", colWeights = colWeightsTestAb2, prevCutOff=0.01))
  testAb2 = mccollect(testAbJob2, FALSE)[[1]]
  
 testAbJob3 = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=1e3, rowWeights = "marginal", colWeights = colWeightsTestAb3, prevCutOff=0.01))
  testAb3 = mccollect(testAbJob3, FALSE)[[1]]
  
  testAbJob1unif = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=1e3, rowWeights = "uniform", colWeights = colWeightsTestAb1, prevCutOff=0.01))
  testAb1unif = mccollect(testAbJob1unif, FALSE)[[1]]
  
 testAbJob2unif = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=1e3, rowWeights = "uniform", colWeights = colWeightsTestAb2, prevCutOff=0.01))
  testAb2unif = mccollect(testAbJob2unif, FALSE)[[1]]
  
  testAbJob3unif = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=1e3, rowWeights = "uniform", colWeights = colWeightsTestAb3, prevCutOff=0.01))
  testAb3unif = mccollect(testAbJob3unif, FALSE)[[1]]
  
  #Try inverse weighting?
    testAbJobInv = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=1e3, rowWeights = sum(dataMatSigNBab)/rowSums(dataMatSigNBab), colWeights =1/margWeightsAb, prevCutOff=0.01))
  testAbInv = mccollect(testAbJobInv, FALSE)[[1]]
  
  #More realistic libsizes
      testAbJobLibs = mcparallel(RCM(dataMatSigNBabag, method="NB_1B1", k=2, nleqslv.control= nleqslv.control.ab, maxItOut=1e3, rowWeights = "uniform", colWeights = "marginal", prevCutOff=0.01))
  testAbLibs = mccollect(testAbJobLibs, FALSE)[[1]]
  
      syntNBSigmargmarg_1B1_abJobMLE = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "marginal", marginEst = "MLE"))
  syntNBSigmargmarg_1B1_abMLE = mccollect(syntNBSigmargmarg_1B1_abJobMLE, FALSE)[[1]] 
  
      syntNBSigunifmarg_1B1_abJobMLE = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform", marginEst = "MLE"))
  syntNBSigunifmarg_1B1_abMLE = mccollect(syntNBSigunifmarg_1B1_abJobMLE, FALSE)[[1]] 
  
      syntNBSigmargunif_1B1_abJobMLE = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, rowWeights="marginal", colWeights = "uniform", marginEst = "MLE"))
  syntNBSigmargunif_1B1_abMLE = mccollect(syntNBSigmargunif_1B1_abJobMLE, FALSE)[[1]] 
  
      syntNBSigunifunif_1B1_abJobMLE = mcparallel(RCM(dataMatSigNBab, method="NB_1B1", k=3, nleqslv.control= nleqslv.control.ab, maxItOut=2e3, prevCutOff=0.01, colWeights="uniform", rowWeights = "uniform", marginEst = "MLE"))
  syntNBSigunifunif_1B1_abMLE = mccollect(syntNBSigunifunif_1B1_abJobMLE, FALSE)[[1]] 
 
  save(dataMatSigNBab, syntNBSigunifmarg_1B1_ab, syntNBSigunifunif_1B1_ab,syntNBSigmargmarg_1B1_ab,syntNBSigmargunif_1B1_ab,taxaSigNBab, sampleSigNBab, testAb1, testAb2, testAb3, testAb1unif, testAb2unif, testAb3unif,testAbInv,testAbLibs,dataMatSigNBabag,  file="toyDataSigab.RData")#syntNBSigunif, syntNBSigmarg, syntNBSigmarg_3,
} else {load("toyDataSigab.RData")}
```

##### Sample plot

```{r ab plots}
solListWSab = list("unifunif" = syntNBSigunifunif_1B1_ab, "unifmarg" = syntNBSigunifmarg_1B1_ab, "margunif" = syntNBSigmargunif_1B1_ab, "margmarg" = syntNBSigmargmarg_1B1_ab, "unifmargunifRmarg" = testAb1,  "margmargunifRmarg"=testAb2, "margunifunifRmarg" = testAb3, "unifmargunifRunif" = testAb1unif, "margmargunifRunif"=testAb2unif, "margunifunifRunif" = testAb3unif, "inverse"= testAbInv, "testEqualLibs" =   testAbLibs)
solListWSab = solListWSab[sapply(solListWSab, class)=="list"] 
#Runtimes and convergence
sapply(solListWSab,function(x){x$converged})
sapply(solListWSab,function(x){x$runtime})
sapply(solListWSab,function(x){x$iter})

#unifmarg
# lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,rMat %*% diag(psis))})})
# lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,t(cMat))}) })
# lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,(rMat %*% diag(psis))[,2:3])})})
# lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,t(cMat)[,2:3])}) })
# Uniform taxon weighting clearly is not an option

cols = c("grey", "red","blue","purple","green","brown","cyan","black")
palette(cols)
par(mfrow=c(2,4))
lapply(names(solListWSab), function(Y){plotRCM(solListWSab[[Y]], samColour = sampleSigNBab, main=Y, biplot=FALSE, Dim = c(1,2), libInset=c(-0.6,0))})
lapply(names(solListWSab), function(Y){plotRCM(solListWSab[[Y]], samColour = sampleSigNBab, main=Y, biplot=FALSE, Dim = c(1,3), libInset=c(-0.6,0))})
lapply(names(solListWSab), function(Y){plotRCM(solListWSab[[Y]], samColour = sampleSigNBab, main=Y, biplot=FALSE, Dim = c(2,3), libInset=c(-0.6,0))})
#Maybe a function of the library sizes
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {
  rMatPsi = rMat %*% diag(psis)
  dfCol = data.frame(Dim1=rMatPsi[,1], Dim2=rMatPsi[,2], col=log(rowSums(X)))
  ggplot(data=dfCol, aes(x=Dim1, y=Dim2, col=col)) +geom_point(size=3) +ggtitle(Y)
    })
  })
```

The first dimension does not separate the groups? It seems to be a "library size dimension"

```{r ab plots2 marginal taxon weighting}
subListab = solListWSab[c("unifmarg","margmarg")]
foo = mapply(list(c(1,2), c(1,3),c(1,2), c(1,3)),rep(names(subListab),each=2), FUN = function(x,Y){plotRCM(solListWSab[[Y]], samColour = sampleSigNBab, main=Y, biplot=FALSE, Dim = x)})
palette(palStore)
```

The results are very similar, but marginal weighting of the samples seems to result in a better separation. Let's look at the biplots

```{r ab biplots}
par(mfrow=c(2,4))
lapply(names(solListWSab), function(Y){plotRCM(solListWSab[[Y]], samColour = sampleSigNBab, main=Y, biplot=TRUE, Dim = c(2,3), taxColour=taxaSigNBab)})
```

Lagrange multipliers

```{r lagrange multipliers}
par(mfrow=c(2,4))
lapply(names(solListWSab), function(Y){cat(Y, "\n");solListWSab[[Y]]$lambdaCol})
```

Also here it is clear that we need marginal taxon weights. Look in the other dimensions

```{r ab biplots margTaxWeights}
par(mfrow=c(2,2))
foo = mapply(list(c(1,2), c(1,3),c(1,2), c(1,3)),rep(names(subListab),each=2), FUN = function(x,Y){plotRCM(subListab[[Y]], samColour = sampleSigNBab, main=Y, biplot=TRUE, Dim = x, taxColour=taxaSigNBab)})
par(mfrow=c(1,1))
Y="unifmarg"
plotRCM(subListab[[Y]], samColour = sampleSigNBab, main=Y, biplot=TRUE, Dim = c(1,3), taxColour=taxaSigNBab)
```

With uniform sample weighting the plot also tends to be dominated by a few outliers. The taxa are correctly identified

##### Relationship to library sizes/abundances

```{r ab libsizes}
#Look at the loadings in function of the library sizes and abunds
par(pty = "m")
logP="x"
#Libsizes
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,rMat[,1] *psis[1],x=rowSums(X), log=logP )})})
#lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {lines(lowess(rMat[,1] *psis[1], rowSums(X)))})})
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,rMat[,2] *psis[2],x=rowSums(X), log=logP )})})
#lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {lines(lowess(rMat[,2] *psis[2], rowSums(X)))})})
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,rMat[,3] *psis[3],x=rowSums(X), log=logP )})})
#lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {lines(lowess(rMat[,3] *psis[3], rowSums(X)))})})
```

The first dimension scores are related to the library sizes in all weighting schemes. This may explain why it is not used to discriminate between the groups. Do we need another functional form for the normalization

```{r ab abunds}
logA = "y"
#Abundances
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,cMat[1,], colSums(X), log=logA, cex=0.5)})})
#lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {lines(lowess(cMat[1,], colSums(X)))})})
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,cMat[2,], colSums(X), log=logA)})})
#lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {lines(lowess(cMat[2,], colSums(X)))})})
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,cMat[3,], colSums(X), log=logA)})})
#lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {lines(lowess(cMat[3,], colSums(X)))})})
lapply(names(solListWSab), function(Y){with(solListWSab[[Y]], {plot(main=Y,abs(cMat[1,]), colSums(X), log=logA)})})
par(pty = "s", mfrow=c(1,1))
```

The least abundant species get higher scores in all weighting schemes. We still have a serious problem with the weighting!

##### Influence function

Take a look at the influence function values on the psis

```{r Ab influence measures: psis}
infl1 = lapply(solListWSab, function(x, i){ 
  infl=with(x, NBpsiInfl(psi = psis[i], X = X, cMat = cMat[i,,drop=FALSE], rMat = rMat[,i, drop=FALSE], muMarg = outer(rowSums(X), colSums(X)/sum(X)), theta = thetas))
  id=abs(infl) > quantile( abs(infl),0.995)
  Xid = x$X[id]
  abunds = colSums(x$X)/sum(x$X)
  libsizes = rowSums(x$X)
  list(infl = infl, id = id, Xid = Xid, abunds=abunds, libsizes=libsizes)
},1)
par(mfrow=c(1,2))
lapply(names(infl1), function(x){
  with(infl1[[x]], plot(abunds, colSums(id), log="x", main=x, ylab="Number of very influential observations"))
}) #High abundances, larger influence
lapply(names(infl1), function(x){
  idTmp = sample(seq_along(infl1[[x]]$abunds), 200) #For speed
  with(infl1[[x]], plot(rep(abunds[idTmp], nrow(infl)), c(t(infl[,idTmp])), log="x", main=x, xlab="Abundance",ylab = "Influence")) #Very heavy!
})
lapply(names(infl1), function(x){
  idTmp = sample(seq_along(infl1[[x]]$libsizes), 200) #For speed
  with(infl1[[x]], plot(rep(libsizes[idTmp], ncol(infl)), c(infl[idTmp,]), log="", main=x, xlab ="Library sizes",ylab = "Influence")) #Very heavy!
})
#Expectations
lapply(names(infl1), function(x){
  with(infl1[[x]], {rbind(quantile(outer(libsizes, abunds)),
       quantile(outer(libsizes, abunds)[id])) })
}) #Influential observations have high expectations
lapply(names(infl1), function(x){
  cat(x, "\n")
  with(infl1[[x]], table(Xid))
}) # ... but are very often zero!

infl2 = lapply(solListWSab, function(x, i){
  infl=with(x, NBpsiInfl(psi = psis[i], X = X, cMat = cMat[i,,drop=FALSE], rMat = rMat[,i, drop=FALSE], muMarg = outer(rowSums(X), colSums(X)/sum(X)), theta = thetas))
  id=abs(infl) > quantile( abs(infl),0.995)
  Xid = x$X[id]
  abunds = colSums(x$X)/sum(x$X)
  libsizes = rowSums(x$X)
  list(infl = infl, id = id, Xid = Xid, abunds=abunds, libsizes=libsizes)
},2)
par(mfrow=c(1,2))
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(abunds, colSums(id), log="x", main=x))
})
#In the second dimension, larger abundance means larger influence on the psis too, and no outliers
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(rep(abunds, nrow(infl)), c(t(infl)), log="x", main=x, xlab="Abundance",ylab = "Influence")) #Very heavy!
}) #Outlier in unifmarg weigthing scheme
lapply(names(infl2), function(x){
  with(infl2[[x]], plot(rep(libsizes, ncol(infl)), c(infl), log="x", main=x, xlab="Library size",ylab = "Influence")) #Very heavy! #No trend in terms of library sizes visible from here, but their distribution should be log-normal perhaps
})
#Expectations
lapply(names(infl2), function(x){
  with(infl2[[x]], {rbind(quantile(outer(libsizes, abunds)),
       quantile(outer(libsizes, abunds)[id])) })
})
lapply(names(infl2), function(x){
  cat(x, "\n")
  with(infl2[[x]], table(Xid))
})
```

The most influential observations are zero counts in highly abundant species and high libsizes (i.e. high expectations)! Note that we cen derive this from the influence function that does not even depend on the weights!

Influence on the colScores

```{r Ab influence colscores, eval=FALSE}
inflColList1 = lapply(solListWSab, function(x){
  try(with(x, NBcolInfl(X, psis, cMat, rMat,thetas , colWeights = if(is.matrix(colWeights)) colWeights else{ cbind(colWeights, colWeights, colWeights)} , k=1 , lambdaCol)))
})
#Look at the signal from the first group
Id1 = which(taxaSigNBab=="Signal 1")[1]

inflCol1 = lapply(inflColList1, function(x){getInflCol(x$score, x$InvJac, Id1)}) 
lapply(inflCol1, function(x){
  boxplot(x[,Id1]~sampleSigNBab, las=2, ylab="Influence")
})
```

As expected, the samples with the signal have the highest impact on the score

```{r Ab influence rowscores Ab}
inflRowList1 = lapply(solListWSab, function(x){
  try(with(x, NBrowInfl(X, psis, cMat, rMat,thetas , rowWeights , k=1 , lambdaRow)))
})
#Only look at most extreme colscores
#Cannot calculate influence functions for the interesting cases (with the outliers)
inflRowList1=inflRowList1[sapply(inflRowList1, class)=="list"]
#The influence on the first row score
 
par(mfrow=c(1,2))
idRow = 10
libSizes = rowSums(solListWSab[[1]]$X)
inflRow1 = lapply(inflRowList1, function(x){getInflRow(x$score, x$InvJac, idRow)})
lapply(names(inflRow1), function(x){
  plot(y=inflRow1[[x]][,idRow], libSizes, log="x", main=x, col=(libSizes ==libSizes[idRow])+1, ylab="Influence")
})
```

Evidently, the observation from the sample itself has the largest influence. In the marginal weighting scheme for the libsizes, the larger libsizes sometimes get a larger influence, although the effect may be small.

Achieved tolerances

```{r tolerances}
tolAchPsi = with(syntNBSigmargmarg_1B1_ab, lapply(1:k, function(y){sapply( 2:iter[y], function(x){abs(1-psiRec[y,x]/psiRec[y,x-1])})}))
par(mfrow=c(1,3))
lapply(tolAchPsi, function(x){plot(x, log="y");abline(h=0.01, col="red");abline(h=0.001, col="blue")})

with(syntNBSigmargmarg_1B1_ab,plot(psiRec[3,1:iter[3]])) #Weird jumps, don't stop too early!
```

#### A log-uniform approach

We sample abundances and library sizes log-uniformly to get a better view of the scores in function of them

```{r NB with signal}
NtaxaLogUnif = 1000
NsamplesLogUnif = 300
rhosRefLU = rhosSig1LU = rhosSig2LU = rhosSig3LU = rhosSig4LU = 10^(-runif(NtaxaLogUnif, 2.5,6))

NsamplesSignal1LU = NsamplesSignal2LU =  25
NsamplesSignal3LU = NsamplesSignal4LU =  20

NtaxaSignalNBLU = 40

Signal1NBLU = 10
Signal2NBLU = 8
Signal3NBLU = 7.5
Signal4NBLU = 7

idSig1NBLU = sample(1:NtaxaLogUnif, NtaxaLogUnif) #Random sampling should ensure orthogonality
idSig2NBLU = sample(1:NtaxaLogUnif, NtaxaLogUnif)
idSig3NBLU = sample(1:NtaxaLogUnif, NtaxaLogUnif) #Random sampling should ensure orthogonality
idSig4NBLU = sample(1:NtaxaLogUnif, NtaxaLogUnif)
#Apply the signals

rhosSig1LU[idSig1NBLU] = rhosSig1LU[idSig1NBLU]*Signal1NBLU
rhosSig2LU[idSig2NBLU] = rhosSig2LU[idSig2NBLU]*Signal2NBLU
rhosSig3LU[idSig3NBLU] = rhosSig3LU[idSig3NBLU]*Signal3NBLU
rhosSig4LU[idSig4NBLU] = rhosSig4LU[idSig4NBLU]*Signal4NBLU

#Renormalize
renorm=function(x){x/sum(x)}
rhosSig1LU=renorm(rhosSig1LU);rhosSig2LU=renorm(rhosSig2LU);
rhosSig3LU=renorm(rhosSig3LU);rhosSig4LU=renorm(rhosSig4LU);

libSizesNBLU = 10^runif(NsamplesLogUnif, 3.5,5.5)

#Generate data
Nref = (NsamplesLogUnif-NsamplesSignal1LU-NsamplesSignal2LU-NsamplesSignal3LU-NsamplesSignal4LU)
meanMatRefNBLU = outer(libSizesNBLU[sample(size=Nref, 1:NsamplesNBLUNS)], rhosNBLUSigref)
meanMatSig1NBLU = outer(libSizesNBLU[sample(size=NsamplesSignal1LU, 1:NsamplesLogUnif)], rhosSig1LU)
meanMatSig2NBLU = outer(libSizesNBLU[sample(size=NsamplesSignal2LU, 1:NsamplesLogUnif)], rhosSig2LU)
meanMatSig3NBLU = outer(libSizesNBLU[sample(size=NsamplesSignal3LU, 1:NsamplesLogUnif)], rhosSig3LU)
meanMatSig4NBLU = outer(libSizesNBLU[sample(size=NsamplesSignal4LU, 1:NsamplesLogUnif)], rhosSig4LU)
load("/home/stijn/PhD/American Gut/AGpars.RData")
thetaMatSigNBLU = matrix(sample(thetas, NtaxaLogUnif), byrow=TRUE, ncol=NtaxaLogUnif, nrow=NsamplesLogUnif)

dataMatRefNBLU = makeNBdata(meanMatRefNBLU, thetaMatSigNBLU)
dataMatSig1NBLU = makeNBdata(meanMatSig1NBLU, thetaMatSigNBLU)
dataMatSig2NBLU = makeNBdata(meanMatSig2NBLU, thetaMatSigNBLU)
dataMatSig3NBLU = makeNBdata(meanMatSig3NBLU, thetaMatSigNBLU)
dataMatSig4NBLU = makeNBdata(meanMatSig4NBLU, thetaMatSigNBLU)

dataMatSigNBLUab = rbind(dataMatRefNBLU, dataMatSig1NBLU, dataMatSig2NBLU, dataMatSig3NBLU,dataMatSig4NBLU)

#Save signals
sampleSigNBLUab = factor(c(rep("Reference", Nref), rep("Signal1", NsamplesSignal1), rep("Signal2", NsamplesSignal2), rep("Signal 3", NsamplesSignal3), rep("Signal 4", NsamplesSignal4)))
taxaSigNBLUtmp = rep("Reference",ncol(dataMatSigNBLUab))
taxaSigNBLUtmp[idSig1NBLU] = "Signal 1"
taxaSigNBLUtmp[idSig2NBLU] = "Signal 2"
taxaSigNBLUtmp[idSig3NBLU] = "Signal 3"
taxaSigNBLUtmp[idSig4NBLU] = "Signal 4"
taxaSigNBLUtmp[idSig1NBLU & idSig2NBLU] = "Signal 1 and 2"
taxaSigNBLUtmp[idSig1NBLU & idSig3NBLU] = "Signal 1 and 3"
taxaSigNBLUtmp[idSig1NBLU & idSig4NBLU] = "Signal 1 and 4"
taxaSigNBLUtmp[idSig2NBLU & idSig3NBLU] = "Signal 2 and 3"
taxaSigNBLUtmp[idSig2NBLU & idSig4NBLU] = "Signal 2 and 4"
taxaSigNBLUtmp[idSig3NBLU & idSig4NBLU] = "Signal 3 and 4"
taxaSigNBLUtmp[idSig1NBLU & idSig2NBLU & idSig3NBLU] = "Signal 1 and 2 and 3"
taxaSigNBLUtmp[idSig4NBLU & idSig2NBLU & idSig3NBLU] = "Signal 2 and 3 and 4"
taxaSigNBLUtmp[idSig1NBLU & idSig2NBLU & idSig4NBLU] = "Signal 1 and 2 and 4"
taxaSigNBLUtmp[idSig1NBLU & idSig4NBLU & idSig3NBLU] = "Signal 1 and 3 and 4"
taxaSigNBLUab = factor(taxaSigNBLUtmp) 
names(taxaSigNBLUab) = colnames(dataMatSigNBLUab) = names(rhosNBLUSigref)
names(sampleSigNBLUab) = rownames(dataMatSigNBLUab) = 1:NsamplesNBLUNS

libSizesAG = sample_sums(AGphylo)
Nref = (NsamplesNBLUNS-NsamplesSignal1-NsamplesSignal2-NsamplesSignal3-NsamplesSignal4)
meanMatRefNBLUag = outer(libSizesAG[sample(size=Nref, 1:NsamplesNBLUNS)], rhosNBLUSigref)
meanMatSig1NBLUag = outer(libSizesAG[sample(size=NsamplesSignal1, 1:NsamplesNBLUNS)], rhosSig1LU)
meanMatSig2NBLUag = outer(libSizesAG[sample(size=NsamplesSignal2, 1:NsamplesNBLUNS)], rhosSig2LU)
meanMatSig3NBLUag = outer(libSizesAG[sample(size=NsamplesSignal3, 1:NsamplesNBLUNS)], rhosSig3LU)
meanMatSig4NBLUag = outer(libSizesAG[sample(size=NsamplesSignal4, 1:NsamplesNBLUNS)], rhosSig4LU)
thetaMatSigNBLUag = thetaMatNBLUNS

dataMatRefNBLUag = makeNBLUdata(meanMatRefNBLUag, thetaMatSigNBLU)
dataMatSig1NBLUag = makeNBLUdata(meanMatSig1NBLUag, thetaMatSigNBLU)
dataMatSig2NBLUag = makeNBLUdata(meanMatSig2NBLUag, thetaMatSigNBLU)
dataMatSig3NBLUag = makeNBLUdata(meanMatSig3NBLUag, thetaMatSigNBLU)
dataMatSig4NBLUag = makeNBLUdata(meanMatSig4NBLUag, thetaMatSigNBLU)

dataMatSigNBLUabag = rbind(dataMatRefNBLUag, dataMatSig1NBLUag, dataMatSig2NBLUag, dataMatSig3NBLUag, dataMatSig4NBLUag)
```

### Zero-inflated poisson

#### ZIP without signal

##### Generate data

```{r ZIP without signal, eval=FALSE}
load("/home/stijn/PhD/American Gut/AGphylo.RData")
#First estimate the ZIP parameters

if(!file.exists("AGzipParams.RData")){
  otuTab = otu_table(AGphylo)@.Data
  logLibs = log(sample_sums(AGphylo))
  ZIPfits = mclapply(mc.cores=4,1:ncol(otuTab),  function(i){
    zeroinfl(otuTab[,i]~offset(logLibs)| 1)
  })
  zeroProbs = sapply(ZIPfits, function(x){expit(x$coef$zero)})
  ZIPmeans = sapply(ZIPfits, function(x){exp(x$coef$count)})
  save(zeroProbs, ZIPmeans, file="AGzipParams.RData")
} else {load(file="AGzipParams.RData")}
#ZeroProbs are very high

#Define parameters
zeroProbs = zeroProbs[zeroProbs<0.9]
NsamplesZIPnoSig= 200
NtaxaZIPnoSig = 800
idSampleZIP = sample(size=NtaxaZIPnoSig, 1:length(ZIPmeans))
lambdasZIPnoSig = ZIPmeans[idSampleZIP]
lambdasZIPnoSig= lambdasZIPnoSig/sum(lambdasZIPnoSig)
zeroesZIPnoSig = sample(zeroProbs, NtaxaZIPnoSig)
libSizesZIPnoSig =c(rep(1e5, floor(NsamplesZIPnoSig/2)), rep(1e6, floor(NsamplesZIPnoSig/2)))

#Mean and zero matrices
meanMatZIPnoSig = outer(libSizesZIPnoSig, lambdasZIPnoSig)
zeroMatZIPnoSig = matrix(zeroesZIPnoSig, nrow=NsamplesZIPnoSig, ncol=NtaxaZIPnoSig, byrow = TRUE)

#Data generation
dataMatZIPnoSig = matrix(rzipois(n=prod(dim(meanMatZIPnoSig)),lambda = meanMatZIPnoSig, pstr0 = zeroMatZIPnoSig), ncol=NtaxaZIPnoSig, nrow=NsamplesZIPnoSig)
```

##### Fit RC(M)

```{r RC(M) fit, eval=FALSE}
nleqslv.controlZIP = list(trace=TRUE, maxit=250, cndtol = .Machine$double.eps)
if(!file.exists("syntZIPnoSig.RData")){
  syntZIPnoSigUnifUnifJob = mcparallel(RCM(dataMatZIPnoSig, method = "ZIP_1B1",k=2, nleqslv.control = nleqslv.controlZIP, colWeights = "uniform", rowWeights = "uniform", maxItOut=5e2))
  syntZIPnoSigUnifUnif =  mccollect(syntZIPnoSigUnifUnifJob, FALSE)[[1]] 
  
  syntZIPnoSigUnifMargJob = mcparallel(RCM(dataMatZIPnoSig, method = "ZIP_1B1",k=1, nleqslv.control = nleqslv.controlZIP, colWeights = "marginal", rowWeights = "uniform", maxItOut=5e3))
  syntZIPnoSigUnifMarg =  mccollect(syntZIPnoSigUnifMargJob, FALSE)[[1]]
  
  syntZIPnoSigMargUnifJob = mcparallel(RCM(dataMatZIPnoSig, method = "ZIP_1B1",k=2, nleqslv.control = nleqslv.controlZIP, colWeights = "uniform", rowWeights = "marginal", maxItOut=5e2))
  syntZIPnoSigMargUnif =  mccollect(syntZIPnoSigMargUnifJob, FALSE)[[1]]
  
   syntZIPnoSigMargMargJob = mcparallel(RCM(dataMatZIPnoSig, method = "ZIP_1B1",k=1, nleqslv.control = nleqslv.controlZIP, colWeights = "marginal", rowWeights = "marginal", maxItOut=5e3))
  syntZIPnoSigMargMarg =  mccollect(syntZIPnoSigMargMargJob, FALSE)[[1]]
  
  save(syntZIPnoSigUnifMarg, syntZIPnoSigUnifUnif, syntZIPnoSigMargMarg, syntZIPnoSigMargUnif, dataMatZIPnoSig, file="syntZIPnoSig.RData")
} else {load("syntZIPnoSig.RData")}
#No convergence, mean cannot be fitted
```

##### Plot the results

```{r RC(M) zip, eval=FALSE}
solListWSZIP = list("unifunif" = syntZIPnoSigUnifUnif, "unifmarg" = syntZIPnoSigUnifMarg, "margunif" = syntZIPnoSigMargUnif, "margmarg" = syntZIPnoSigMargMarg)
solListWSZIP = solListWSZIP[sapply(solListWSZIP, class)=="list"]
#Runtimes and convergence
sapply(solListWSZIP,function(x){x$converged})
sapply(solListWSZIP,function(x){x$runtime})
sapply(solListWSZIP,function(x){x$iter})
#No convergence and overflow....

with(syntZIPnoSig, {plot(rMat)})
with(syntZIPnoSig, {plot(t(cMat))})

with(syntZIPnoSig, {plot(cMat[1,], colSums(X), log="y")})
with(syntZIPnoSig, {plot(cMat[1,], colMeans(X==0), log="y")})
with(syntZIPnoSig, {plot(abs(cMat[1,]), colSums(X), log="y")})

with(syntZIPnoSig, {plot(vMat[1,], colSums(X), log="y")})
with(syntZIPnoSig, {plot(vMat[1,], colMeans(X==0), log="y")})

with(syntZIPnoSig, {plot(cMat[2,], colSums(X), log="y")})
with(syntZIPnoSig, {plot(abs(cMat[2,]), colSums(X), log="y")})
```

#### ZIP with signal

```{r ZIP with signal, eval=FALSE}
load("/home/stijn/PhD/American Gut/AGphylo.RData")
#First estimate/load the ZIP parameters

if(!file.exists("AGzipParams.RData")){
  otuTab = otu_table(AGphylo)@.Data
  logLibs = log(sample_sums(AGphylo))
  ZIPfits = mclapply(mc.cores=4,1:ncol(otuTab),  function(i){
    zeroinfl(otuTab[,i]~offset(logLibs)| 1)
  })
  zeroProbs = sapply(ZIPfits, function(x){expit(x$coef$zero)})
  ZIPmeans = sapply(ZIPfits, function(x){exp(x$coef$count)})
  save(zeroProbs, ZIPmeans, file="AGzipParams.RData")
} else {load(file="AGzipParams.RData")}
#ZeroPorbs are very low

#Defina parameters
NsamplesZIPsig= 150
NtaxaZIPsig = 500
lambdasZIPref = lambdasZIPsig1 =lambdasZIPsig2 = lambdasZIPsig12 =  sample(ZIPmeans, NtaxaZIPsig)
zeroesZIPref = zeroesZIPsig1 = zeroesZIPsig2 = zeroesZIPsig12 =sample(zeroProbs, NtaxaZIPsig)
libSizesZIPsig =c(rep(1e4, floor(NsamplesZIPsig/2)), rep(1e5, floor(NsamplesZIPsig/2)))

#Define the signal
NtaxaSignal1ZIP = 20
NtaxaSignal2ZIP = 20

NtaxaSignal1zeroZIP = 20
NtaxaSignal2zeroZIP = 20

Signal1taxZIP = 4
Signal2taxZIP = 3

Signal1zeroTaxZIP = 5
Signal2zeroTaxZIP = 3

NsamplesSignal1ZIP = 20
NsamplesSignal2ZIP = 20
NsamplesSignal12ZIP = 20

NsamplesSignal1zeroZIP = 20
NsamplesSignal2zeroZIP = 20
NsamplesSignal12zeroZIP = 20

idSig1TaxZIP = 1:NtaxaSignal1ZIP
idSig2TaxZIP = sample(1:NtaxaZIPsig,NtaxaSignal2ZIP)

lambdasZIPsig1[idSig1TaxZIP] = lambdasZIPsig1[idSig1TaxZIP] * Signal1taxZIP
lambdasZIPsig2[idSig2TaxZIP] = lambdasZIPsig2[idSig2TaxZIP] * Signal2taxZIP
lambdasZIPsig12[c(idSig1TaxZIP, idSig2TaxZIP)] = c(lambdasZIPsig1[1:NtaxaSignal1ZIP],lambdasZIPsig2[idSig2TaxZIP])

idSig1TaxZIPzeroes = 1:NtaxaSignal1zeroZIP + NtaxaSignal1ZIP/2
idSig2TaxZIPzeroes = idSig1TaxZIPzeroes + NtaxaSignal1zeroZIP/2 + NtaxaSignal2zeroZIP/2

zeroesZIPsig1[idSig1TaxZIPzeroes] = expit(logit(zeroesZIPsig1[idSig1TaxZIPzeroes]) * Signal1zeroTaxZIP)
zeroesZIPsig2[idSig2TaxZIPzeroes] = expit(logit(zeroesZIPsig2[idSig2TaxZIPzeroes]) * Signal1zeroTaxZIP)
zeroesZIPsig12[c(idSig1TaxZIPzeroes, idSig2TaxZIPzeroes)] = c(zeroesZIPsig1[idSig1TaxZIPzeroes],zeroesZIPsig2[idSig2TaxZIPzeroes])

NsamplesZIPsigRef =  NsamplesZIPsig - NsamplesSignal1ZIP - NsamplesSignal2ZIP-  NsamplesSignal12ZIP

#Define mean and zero matrices
meanMatrefZIP = outer(sample(libSizesZIPsig, NsamplesZIPsigRef), lambdasZIPref)
meanMatSig1ZIP = outer(sample(libSizesZIPsig, NsamplesSignal1ZIP), lambdasZIPsig1)
meanMatSig2ZIP = outer(sample(libSizesZIPsig, NsamplesSignal2ZIP), lambdasZIPsig2)
meanMatSig12ZIP = outer(sample(libSizesZIPsig, NsamplesSignal12ZIP), lambdasZIPsig12)

zeroMatrefZIP = matrix(zeroesZIPref, nrow=nrow(meanMatrefZIP),ncol=ncol(meanMatrefZIP), byrow = TRUE)
zeroMatSig1ZIP = matrix(zeroesZIPsig1, nrow=nrow(meanMatSig1ZIP),ncol=ncol(meanMatSig1ZIP), byrow = TRUE)
zeroMatSig2ZIP = matrix(zeroesZIPsig2, nrow=nrow(meanMatSig2ZIP),ncol=ncol(meanMatSig2ZIP), byrow = TRUE)
zeroMatSig12ZIP = matrix(zeroesZIPsig12, nrow=nrow(meanMatSig12ZIP),ncol=ncol(meanMatSig12ZIP), byrow = TRUE)

#Generate the data
dataMatZIPref = matrix(rzipois(n=prod(dim(meanMatrefZIP)),lambda = meanMatrefZIP, pstr0 = zeroMatrefZIP), ncol=ncol(meanMatrefZIP), nrow=nrow(meanMatrefZIP))
dataMatZIPsig1 = matrix(rzipois(n=prod(dim(meanMatSig1ZIP)),lambda = meanMatSig1ZIP, pstr0 = zeroMatSig1ZIP), ncol=ncol(meanMatSig1ZIP), nrow=nrow(meanMatSig1ZIP))
dataMatZIPsig2 = matrix(rzipois(n=prod(dim(meanMatSig2ZIP)),lambda = meanMatSig2ZIP, pstr0 = zeroMatSig2ZIP), ncol=ncol(meanMatSig2ZIP), nrow=nrow(meanMatSig2ZIP))
dataMatZIPsig12 = matrix(rzipois(n=prod(dim(meanMatSig12ZIP)),lambda = meanMatSig12ZIP, pstr0 = zeroMatSig12ZIP), ncol=ncol(meanMatSig12ZIP), nrow=nrow(meanMatSig12ZIP))

#Bind the data
dataMatZIPSig = rbind(dataMatZIPref, dataMatZIPsig1, dataMatZIPsig2, dataMatZIPsig12)
#mean(dataMatZIPsig==0) #Correct zero fraction

#Save signals
sampleSigZIP = factor(c(rep("Reference",NsamplesZIPsigRef), rep("Signal1", NsamplesSignal1ZIP), rep("Signal2", NsamplesSignal2ZIP), rep("Signal 1 and 2", NsamplesSignal12ZIP)))
taxaSigZIPtmp = rep("Reference",ncol(dataMatSigNB))
taxaSigZIPtmp[idSig1TaxZIP] = "Signal 1"
taxaSigZIPtmp[idSig2TaxZIP] = "Signal 2"
taxaSigZIP =factor(taxaSigZIPtmp) 
names(taxaSigZIP) = colnames(dataMatZIPSig) = names(rhosNBSigref)
names(sampleSigZIP) = rownames(dataMatZIPSig) = 1:NsamplesZIPsig
```

#### Apply the RC(M) algorithm

```{r ZIP signal fit, eval=FALSE}
if(!file.exists("syntZIPSig.RData")){
    syntZIPSigUnifJob = mcparallel(RCM(dataMatZIPSig, method = "ZIP",k=2, nleqslv.control = list(trace=TRUE, maxit=250), colWeights = "uniform"))
  syntZIPSigUnif =  mccollect(syntZIPSigUnifJob, FALSE)[[1]] 
  
  syntZIPSigMargJob = mcparallel(RCM(dataMatZIPSig, method = "ZIP",k=2, nleqslv.control = list(trace=TRUE, maxit=250), colWeights = "marginal"))
  syntZIPSigMarg =  mccollect(syntZIPSigMargJob, FALSE)[[1]] 
  
  save(syntZIPSigUnif, syntZIPSigMarg, sampleSigZIP, taxaSigNB, file="syntZIPSig.RData")
} else {load("syntZIPSig.RData")}
```

##### Plot the results

### Zero-inflated negative binomial

#### Without signal

##### Generate data

```{r ZINB without signal, eval=FALSE}
load("/home/stijn/PhD/American Gut/AGphylo.RData")
#First estimate the ZIP parameters

expit=function(x){
  tmp = exp(x)/(1+exp(x))
  tmp[is.na(tmp)]=1 #Adjust for overflow
  tmp}

if(!file.exists("AGzinbParams.RData")){
  otuTab = otu_table(AGphylo)@.Data
 # otuTab = otuTab[, colMeans(otuTab==0)<0.95]
  logLibs = log(sample_sums(AGphylo))
  ZINBfits = lapply(1:ncol(otuTab),  function(i){
    try(zeroinfl(otuTab[,i]~offset(logLibs)| 1, dist="negbin"), silent=TRUE)
  })
  zeroProbsNB = sapply(ZINBfits, function(x){if(is.list(x)) expit(x$coef$zero) else NA})
  ZINBmeans = sapply(ZINBfits, function(x){if(is.list(x)) expit(x$coef$count) else NA})
  thetasZINB = sapply(ZINBfits, function(x){if(is.list(x)) x$theta else NA})
  naID=is.na(zeroProbsNB)
  zeroProbsNB= zeroProbsNB[!naID]
  ZINBmeans= ZINBmeans[!is.na(ZINBmeans)]
  thetasZINB= thetasZINB[!is.na(thetasZINB)]
  save(thetasZINB, zeroProbsNB, ZINBmeans, file="AGzinbParams.RData")
} else {load(file="AGzinbParams.RData")}
#ZeroProbs are lower than for the ZIP

#Define parameters
zeroProbsNB = zeroProbsNB[zeroProbsNB<0.9]
NsamplesZINBnoSig= 200
NtaxaZINBnoSig = 800
idSampleZINB = sample(size=NtaxaZINBnoSig, 1:length(ZINBmeans))
MeansZINB = ZINBmeans[idSampleZINB]
ThetasZINB = thetasZINB[idSampleZINB]
zeroesZINBnoSig = sample(zeroProbsNB, NtaxaZINBnoSig)
libSizesZINBnoSig =c(rep(1e5, floor(NsamplesZINBnoSig/2)), rep(1e6, floor(NsamplesZINBnoSig/2)))

#Mean, OD and zero matrices
meanMatZINBnoSig = outer(libSizesZINBnoSig, MeansZINB)
ODmatZINB = outer(rep(1,NsamplesZINBnoSig) ,ThetasZINB )
zeroMatZINBnoSig = matrix(zeroesZINBnoSig, nrow=NsamplesZINBnoSig, ncol=NtaxaZINBnoSig, byrow = TRUE)

#Data generation
dataMatZINBnoSig = matrix(rzinegbin(n=prod(dim(meanMatZINBnoSig)),munb = meanMatZINBnoSig, size = ODmatZINB, pstr0 = zeroMatZINBnoSig), ncol=NtaxaZINBnoSig, nrow=NsamplesZINBnoSig)
```

##### Apply the RC(M) algorithm with the ZINB distribution

```{r ZINB signal fit, eval=FALSE}
if(!file.exists("syntZINBSig.RData")){
    syntZINBSigUnifJob = mcparallel(RCM(dataMatZINBSig, method = "ZINB",k=2, nleqslv.control = list(trace=TRUE, maxit=250), colWeights = "uniform"))
  syntZINBSigUnif =  mccollect(syntZINBSigUnifJob, FALSE)[[1]] 
  
  syntZINBSigMargJob = mcparallel(RCM(dataMatZINBSig, method = "ZINB",k=2, nleqslv.control = list(trace=TRUE, maxit=250), colWeights = "marginal"))
  syntZINBSigMarg =  mccollect(syntZINBSigMargJob, FALSE)[[1]] 
  
  save(syntZINBSigUnif, syntZINBSigMarg, sampleSigZINB, taxaSigNB, file="syntZINBSig.RData")
} else {load("syntZINBSig.RData")}
```

##### Plot the ZINB results

#### Restore defaults

```{r restore defaults, eval=FALSE}
palette(palStore)
```

## Constrained analysis

Yingjie's code...

#### Some comments on analysing microbiome count data

##### Do not log-transform count data

- log-transforms come with arbitrary pseudocounts for the many zeroes
- models based on transformed data are hard to interpret in funcion of the untransformed data
- log-transformation does not stabilize the variance
- Accept non-normality and heteroscedasticity instead of trying to transform them away!
- Log-transformation is more robust though and provides better type I error control (see papers O'Hara 2010, Ives 2015, Warton 2016 and this [post](http://stats.stackexchange.com/questions/114848/negative-binomial-glm-vs-log-transforming-for-count-data-increased-type-i-erro?rq=1) )

##### Do not rely on residuals-based approaches

- Most residual based approaches rely on normally behaved residuals
- Microbiome residuals are skewed to the right 
- The expectations should be larger than or equal to 5 for this approximation to hold, which is not hte case for the majority of microbiome entries

##### Use a GLM

- Normalizing to relative or rarefied abundances throws away information on the variance and valuable counts
- Leave the data untransformed and use a proper GLM for counts! This will properly model mean and variance and allows incorporation of covariate information