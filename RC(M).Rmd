---
title: "RC(M)"
author: "Stijn"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    includes:
      in_header: packagesRCM.sty
---

```{r check-install-load-packages, warning=FALSE, message=FALSE, echo=FALSE, purl=TRUE}
WD = "/home/stijn/PhD/Biplots"
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE,  warning = FALSE, message=FALSE, echo=FALSE, eval=FALSE, tidy = TRUE, fig.width = 9, fig.height=6, root.dir=WD, purl=TRUE, fig.show = "hold", cache.lazy = FALSE)
# The required package list:
reqpkg <- c("phyloseq","MASS", "parallel","nleqslv", "edgeR", "VGAM","HMP", "ggplot2", "pscl", "vegan", "zCompositions", "DirFactor","alabama", "reshape2", "tensor", "locfit", "SimSeq","fdrtool", "cluster", "ape", "SpiecEasi", "flux")
# Load all required packages and show version
for(i in reqpkg)
{
#   print(i) 
#   print(packageVersion(i))
  library(i, quietly=TRUE, verbose=FALSE, warn.conflicts=FALSE, character.only=TRUE)
} 
if(detectCores()==4) setwd(WD)
par(pty="s", mar = c(8,2,4,2), cex.main = 0.9) #Make sure the biplots are square!
palStore = palette()
levelsMeth = c("CApearson","CAcontRat","CAchisq","DCA", "Hellinger","BC","BCrel","BCrelNMDS", "JSD","UniFrac","wUniFrac", "CoDa","RCM", "Control")
labelsMeth = c("CApearson","CAcontRat","CAchisq","DCA","Hellinger","Bray-Curtis-Abs","Bray-Curtis","Bray-Curtis NMDS", "JSD","UniFrac","weighted UniFrac","CoDa","RCM", "Control")
groupsMeth = factor(c("Independence","Independence","Independence","Independence","Independence","Distance","Distance","Distance","Distance","Distance","Distance", "CoDa", "RCM"), levels = c("Independence","Distance","CoDa","RCM"), ordered = TRUE)
names(groupsMeth) = grep(levelsMeth, value  = TRUE, pattern ="Control", invert = TRUE)
funFiles = dir("R")
funFilesPub = dir("pubFun")
for (i in funFiles) {source(file.path("R",i))}
for (i in funFilesPub) {source(file.path("pubFun",i))};rm(i)
```

\setcounter{tocdepth}{2}
\tableofcontents

# Introduction

The aim of this notebook is to wrap the codes to fit the RC(M) model with the negative binomial, zero-inflated poisson and zero-inflated negative binomial models. Also an overview of the theory is given here. Functions for fitting the RC(M) model have been moved to separate files in the "R"-folder, functions related the simulation study but not belonging to the core RC(M) methodology are shown in the "pubFun" folder.

The underlying motivation is to develop a visualization technique for the microbiome that does not suffer from the defects of currently applied techniques such as PCA, PCoA with Bray-Curtis distance etc. These methods are in particular sensitive to differences in library size.

# Theory

## Unconstrained analysis

### Correspondence analysis

Suppose we have a $nxp$ count data matrix $\mathbf{X}$ with $n$ samples (sites) and $p$ taxa (species), with $i$ and $j$ row respectively column indices. The count matrix has been trimmed to exclude all-zero rows and columns.

#### Independence model

Under independence between rows and columns we model the counts in a contingency table as 

$$E(X_{ij}) = a_i b_j$$

whereby usually $a_i=x_{i.}=\sum_{j=1}^px_{ij}$ (the library sizes) and $b_j=\frac{x_{.j}}{x_{..}}=\frac{\sum_{i=1}^nx_{ij}}{\sum_{j=1}^p\sum_{i=1}^nx_{ij}}$ (the average relative abundances). The matrix of expectations under the independence model can then be written as:

$$E = \frac{R\mathbf{11}^TC}{x_{..}}$$

with $x_{..}=\sum_{i=1}^n\sum_{j=1}^px_{ij}$ the total sum of the count table, and __1__ a row vector of ones of suitable length.

There exist many variation so of correspondence analysis, but all are concerned with the difference between the observed count __X__ and the expected counts based on the margins __E__: $X-E$. This means that the signal can come from observations that are either smaller or larger than expected. This two-way mode of deviations will give rise to the unimodality of correspondence analysis (see later).

The aim is to represent these departures from independence in a few important dimensions.

#### Reconsitution formula of Correspondence Analysis (CA)

A more extended model than the independence model is

$$E(X_{ij}) = a_i  b_j + c_i d_j \sum_{k=1}^K \omega_k v_{ki} w_{jk}$$.

with $a_i = c_i = x_{i.} = \sum_{j=1}^px_{ij}$ and $b_j = d_j = \frac{x_{.j}}{x_{..}}$ with $x_{.j} = \sum_{i=1}^nx_{ij}$. K = min(n,p) and the terms are ordered such that $\omega_1 >\omega_2>...>\omega_K$. When fitted, the second series of terms will attempt to repair discrepancies between __X__ and __E__ and as such capture departures from independence. This is called the *reconstitution formula* since it decomposes the observed average count into its expectation under independence and a residual. The residual is then further decomposed into $k$ pieces.
The first two dimensions $(\omega_1\mathbf{v}_1,\omega_2\mathbf{v}_2)$ can then be plotted to represent samples that depart from independence in a similar way. $(\mathbf{w}_1,\mathbf{w}_2)$ can be added to show which taxa contribute to these departures, according to the biplot principle. Note that we give priority to correctly representing distances between _samples_ rather than _taxa_. The orthogonal projection of $(\omega_1 v_{ki}, \omega_2 v_{ki})$ on $(w_{j1},w_{j2})$ then equals $\frac{(\omega_1 v_{ki}, \omega_2 v_{ki})^t(w_{j1},w_{j2})}{||(w_{j1},w_{j2})||}$. This value is proportional to the departure from the independence model of taxon $j$ in sample $i$ in dimension $k$ ($(\omega_1 v_{ki}, \omega_2 v_{ki})^t(w_{j1},w_{j2}) = \omega_1 v_{ki} w_{j1} + \omega_2 v_{ki} w_{j2}$), with the norm of the species vector as proportionality constant. We could fit this model by maximum likelihood after assuming e.g. a Poisson distribution, but it is much more efficient to do this through a matrix decomposition

#### Correspondence analysis as a singular value decomposition

Corrsepondence analysis is usually done through singular value decomposition(SVD) of the matrix of departures from independence __X-E__. This is not done on the matrix of raw departures, but is usually weighted by row and column scores in one way or the other, to account for the heteroscedasicity of count data. Subtle differences in choice of weights lead to different versions of correspondence analysis. very often it is not mentioned which version is used, which complicates comparison of results of different packages. We are following "Understanding Biplots", from Gower _et al._ here.

CA performs a singular value decomposition of the following matrix

$$A_1 = R^{-1/2}(X-E)J^{-1/2}=U\Sigma V^T$$

with $\Sigma$ a diagonal matrix with the singular values of __X__ (all between 0 and 1) on the diagnonal and __R__ and __J__ diagonal matrices with row and column sums of __X__.

Elements of this matrix are $$a_{1ij} = \frac{x_{ij}-x_{i.}x_{.j}/n}{\sqrt{x_{i.}x_{.j}}} = \frac{1}{\sqrt{n}}\frac{x_{ij}-x_{i.}x_{.j}/n}{\sqrt{x_{i.}x_{.j}/n}}$$

, also known as also known as $\sqrt{n}$ times the Pearson standardized residuals. The Pearson standardized residuals standardize the departures from __E__ by dividing by the square root of the expected value. This is in line with the common assumption that for count data $E(X_{ij}) = Var(X_{ij})$ and would yield approximately standard normally distributed variables if this assumption holds and $E(X_{ij})$ is sufficiently large. Hence the sum of squared elements $$\sum_{i=1}^n\sum_{j=1}^pa_{1ij}^2$$ yields 1/n the Pearson $\chi^2$ statistic to test for association in the contingency table.

In matrix notation the reconstitution formula becomes

$$X = E_{independence} + R^{1/2}U\Sigma VJ^{1/2}$$

with $\mathbf{1'}R^{1/2}U=\mathbf{0}$ and $\mathbf{1'}J^{1/2}V=\mathbf{0}$ (weighted means of rows and columns equal zero) and $U'R^{1/2}U=\mathbf{1}$ and $V'K^{1/2}V=\mathbf{1}$ (weighted variances equal one) (see VanDerHeijden et al., 1985).

Note that the use of correspondence analysis implies unimodal response curves (see later). Hence it is not appropriate to represent species as arrows. Representing them as points is more appropriate.

#### Biplots based on correspondence analysis

Many biplots are possible, we will discuss three of the most important ones.

##### Pearson standardized residuals

We could proceed by plotting the first two columns of $U\Sigma$ to represent the samples and add the first two columns of __V__ to show which taxa contribute to the departure. This seems also the most logical course to me, since it allows to identify samples and species that contribute most to the divergence from independence.

##### Contingency ratio

To optimally represent the distances between samples, we could also plot the columns of $R^{-1/2}U\Sigma$ and add $VC^{-1/2}$ for the species. This representation has the merit of centering the sample and species vectors in the plot. Also in this configuration the site scores are at the _centroids_ or _barycentres_ of the sample scores as can be seen from:

$$R^{-1}X(J^{-1/2}V) = R^{-1/2}(R^{-1/2}XJ^{-1/2})V = R^{-1/2}(R^{-1/2}XJ^{-1/2})V = \\ R^{-1/2}(U\Sigma V' + R^{-1/2}EJ^{-1/2})V = R^{-1/2}(U\Sigma V' + R^{-1/2}EJ^{-1/2})V = R^{-1/2}(U\Sigma V' + R^{-1/2}R11^TJJ^{-1/2})V = R^{-1/2}U\Sigma$$
 
The last step of the derivation relies on the orthogonality of __V__ and on the property $\mathbf{1}^TJ^{-1/2}V = \mathbf{0}$. A centroid is defined as a centre of mass: the weighted average of the sample scores with taxon abundances in that sample as weights. This is the same way as in which the species scores are calculated in PCoA based on the sample scores (see later). This version of correspondence analysis is implemented in the _vegan_ and _phyloseq_ packages, as it was described in Legendre and Legendre, 2012, p466-471.

An alternative way to consider this version is to look at the svd of

$$A_2 = R^{-1}(X-E)C^{-1} = U'\Sigma V'^{T} = R^{-1/2}(U \Sigma V^T)C^{-1/2}$$

The elements of the matrix $A_2$ are $a_{2ij} = \frac{x_{ij}-e_{ij}}{e_{ij}} = \frac{x_{ij}}{e_{ij}}-1$. The ratio $\frac{x_{ij}}{e_{ij}}$ represents the relative departure from the expected counts and is called the _contingency ratio_. The biplot of $U'\Sigma$ vs $V'$ will thus visualize contributions to the departure of this contingency ratio.

##### Chi-squared distance

A third biplot is one based on the chi-squared distances. True to our original aim to give priority to samples, we will only discuss chi-squared distances between samples. 

The squared chi-squared distance between two samples $i$ and $i'$ is defined as:

$$d^2_{ii'} = \sum_{j=1}^p \frac{1}{x_{.j}}\Big(\frac{x_{ij}}{x_{i.}}-\frac{x_{i'j}}{x_{i'.}}\Big)^2$$
, i.e. squared differences in proportions weighted by column scores. A drawback of this distance is that it gives more weighted to taxa with low abundances. The lower dimensional representation of these distances can be achieved by the svd

$$A_3 = R^{-1}(X-E)C^{-1/2} = U'\Sigma V^T = R^{-1/2}U\Sigma V$$

It is evident that the row scores of these biplots will be equal to PCoA biplots based on the chi-squared distance, and to the row scores of the correspondence analysis based on the contingency ratio. The column scores on the other hand, are equal to those from the approximation to the Pearson residuals.

This form of correspondence analysis has been criticised for putting undue emphasis on rare species (Legendre and Gallagher, 2001 and Rao, 1997).

#### Joint plots

Some researchers prefer to plot $U\Sigma$ vs $\Sigma V^T$ to represent distances between both sites and species corrrectly. However, this loses the biplot property (inner product interpration) and end their discussion here.

#### Skewness

Because of the skewness of the distribution, the residuals are also very skewed. If we have an expectation of around 1, it is very likely to observe a zero, but much less likely to observe a 2. Still both deviations are weighted equally. I think a residual-based approach is ill suited to asymmetric, skewed data. Also the library size correction will never be appropriate, so that part of the biplot will reflect differences in library sizes. We considered decomposing

$$R^{-1}(X-E)J^{-1}$$

in the past to account for the fact that the data are not Poisson ($Var(X) = E(X)$) but rather negative binomial ($Var(X) = E(X) + \phi*E(X)^2$) and thus the variance for large means proportional to the squared expectation. We still use this decomposition to obtain starting values for our RC(M) model. Even more general we considered

$$R^{-a}(X-E)J^{-b}$$

and estimate $a$ and $b$ by maximum likelihood but we did not really see how to go on from here.

#### Detrended correspondence analysis

In regular correspondence analysis it is often observed that the second axis points form an arch with respect to the first axis. This effect can be removed by detrending. The procedure is then called Detrended correspondence analysis (DCA) (Hill and Gauch, 1980).

#### Link to Gaussian ordination and niche theory

Species can grow and reproduce best within certain ranges of parameter values that describe their envrionment. The multidimensional space spanned by environmental variables in which a certain species can grow is called its __niche__. The species prospers in the center of its niche, but its abundance decreases as the characteristics of the environment depart from this optimal conditions. In extreme cases the species simply cannot survive or reproduce in certain envrironments and is simply absent. Hence this niche is usually decribed by a unimodal function, often a Gaussian function (Gauc _et al._, 1974).

The expected counts are then modelled as:

$$log(E(X_{ij})) = \sum_{k=1}^K \big(a_{jk} - \frac{(h_{ik} - q_{jk})^2}{2m_{jk}^2}\big)$$

whereby 

 - $h_{ik}$ = a latent variable representing an environmental score
 - $a_{jk}$ is the maximal logged expected abundance achieved when the latent variable $h_{ik}$ equals the optimum $q_{jk}$
 - $m_{jk}$ is the tolerance, it determines how fast the expected value decreases as $\mathbf{c}_{i.}$ moves away from its optimal values $\boldsymbol{\alpha}_k^{-t}h_{ik}$

Under the assumptions that:

  1) All tolerances $m_{jk}$ are equal for all species ($m_k$)
  2) All maxima $a_{jk}$ are equal for all species
  3) The modi $q_{jk}$ are homogeneously distributed over a region larger than $m_k$
  4) Counts are Poisson distributed
  
  CA yields the ML solution of this Gaussian model see ter Braak, 1986).

### Log-linear analysis

Another modelling approach is to use log-linear modelling an thereby introduce the negative binomial as error term. We know from previous goodness of fit testing that this is an appropriate error model for microbiome data.

In log-linear analysis the logged expected count $l_{ij}$ is modelled as

$$log(E(X_{ij})) = l_{ij} = u + u_i + u_j + u_{ij}$$.

For $u_{ij} \neq 0$ this is the saturated model, which provides a perfect fit by setting all expected values equal to the observed ones. If $u_{ij} = 0$ this is the independence model presented above.

### The RC(M)-association model

For the purpose of making biplots, we might consider the following model (proposed by Leo Goodman, 1985. See also VanDerHeijden 1994)

$$log(E(x_{ij})) = l_{ij} = u + u_i + u_j + \psi_1r_{1i}s_{1j} + \psi_2r_{2i}s_{2j}$$

or in matrix form:

$$\mathbf{lE} = \mathbf{u_{sam,MLE}}\mathbf{u_{tax,MLE}}^t + \mathbf{r}_k \boldsymbol{\psi}_k \mathbf{s}_k^t$$

with $\mathbf{lE}$ the n-by-p matrix of logged expectations, $\mathbf{u_{sam,MLE}}$ the vector with length $n$ with the row intercepts estimated by ML, $\mathbf{u_{tax,MLE}}$ the vector of length $p$ with the column intercepts estimated by ML, $\mathbf{r}_k$ the n-by-k matrix of row scores, $\boldsymbol{\psi}_k$ a diagonal matrix with the $k$ $\psi$'s on the diagonal and $\mathbf{s}_k$ the p-by-k matrix of column scores

Constraints are needed to render this model identifiable, since we have three unknown regressors ($r_{ik}, s_{jk} and psi_k$) instead of one. We do restrict the importance parameter $psi_k$ to be positive, and center the (weighted) row and column scores around 0, which is a useful property for the biplot and is also the case for correspondence analysis. In addition the row and column scores are restricted to have (weighted) variance 1. $psi_k$ is the only parameter that can grow in size without restriction. As a result it will automatically figure as a measure of importance of the departure from independence in that direction, since all scores of the dimensions are normalized. This is also the case for correspondence analysis. Thridly, the scores of the different dimensions are orthogonal, so the solutions in different dimensions are linearly independent.

Centering:

$$\sum_{i=1}^nw_ir_{ki} = 0$$

with k=1,2 and

Normalization($k=k'$) and orthogonality ($k \neq k'$)

$$\sum_{i=1}^nw_ir_{ki}r_{k'i} = I(k=k')$$

$$\sum_{j=1}^pz_js_{kj} = 0$$

$$\sum_{j=1}^pz_js_{kj}s_{k'j} = I(k=k')$$
Hereby $w_i$ and $z_j$ are row and column weigths.

#### RC(M)-weights

The weights can be regarded as probability density functions, they represent the likelihood of sampling a certain sample or taxon from the population. On the population level we could say that

$$E(w_iX_{ik}) = 0$$,

i.e. the average row score on the population level is zero. This is a useful restriction to make sure that the biplot is centered around zero. Analogously we want that

$$E(w_iX_{ik}X_{ik'}) = I(k=k')$$

and accordingly for the column scores:
$$E(z_j S_{jk}) = 0$$

$$E(z_j S_{jk} S_{jk'}) = I(k=k')$$. 

But which weights $w_i$ and $z_j$ should we use?

Goodman proposes to use $w_i = x_{i.}$ and $z_j = x_{.j}$ thus uses _weighted_ constraints, to retain the relationship with correspondence analysis (see below). Becker _et al._ (1989) recommend using uniform weights not to let the marginal distribution affect the model fit. 

For the microbiome case, every subject comes from the same population under the null-hypothesis and all subjects are thus eqaully likely to be sampled and have the same importance. The library sizes as a purely technical artefact, unrelated to the biological importance of the subject. Consequently we use uniform row weights $w_i = 1/n$ (or $w_i = 1$, the magnitude is of no importance since the associated $\psi_k$ will grow or shrink accordingly). However, some taxa are more prevalent in the population than others. We want the average column scores on the population level to be centered around zero, have variance one and be orthogonal. That is why we set $z_j = exp(u_j)$; because the more abundant species are in fact more abundant in the population as a whole (as opposed to samples with a large library size), it makes sense to use a marginal weighting scheme for the column scores. The weights $z_j$ will be estimated by maximum likelihood, as explained in the next paragraph.

#### RC(M) margins

The most logical choices for the margins and the independence model have long seemed to use $u=-log(x_{..})$, $u_i = log(x_{i.})$ and $u_j = log(x_{.j})$.

However, the library sizes do not correspond to the maximum likelihood estimate of $u_i$ under the Negative Binomial model. As a result the first dimensional row scores $r_{1i}$ tried to correct for this effect and became related (linearly correlated) to the library sizes, which we want to avoid absolutley. 

To avoid this we set $u=0$ and estimate the $u_i$'s, $u_j$'s and overdispersions iteratively using maximum likelihood in a first step (this converges very quickly). The main disadvantage I see of this approach is that the independence model becomes model dependent. Next we'll have to iterate between fitting the overdispersions, the imporance parameters $\psi$, the $r's$ and the $s's$. Hereby we use marginal weights for the taxa ($z_j = exp(u_j)$) and uniform weights for the rows. 

In practice, the marginal sums differ more from the MLE for the library sizes than for the taxon abundances. We can think of the following mathematical explanation: when calculating library sizes or abundances by row and column sums, each observations receives the same weight. However, when we estimate the margins through ML, we solve the following score equations:

$$\frac{\partial L(\mathbf{X}_i|\mathbf{u}_{i}, \mathbf{u}_j, \boldsymbol{\theta})}{\partial u_{i}} = \sum_{j=1}^p \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}}$$

$$\frac{\partial L(\mathbf{X}_i|\mathbf{u}_{i}, \mathbf{u}_j, \boldsymbol{\theta})}{\partial u_{j}} = \sum_{i=1}^n \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}}.$$

Note that since we are estimating offsets the value of the regressor is 1. In this case the difference of $y_{ij}$ with the expected value $\mu_{ij}$ is weighted by a factor $\frac{1}{1+\frac{\mu_{ij}}{\theta_j}} = \frac{\theta_j}{\theta_j+\mu_{ij}}$. When estimating $u_j$, $\theta$ is a constant but when estimating $u_i$ it is different for every observation $y_{ij}$. Hence the weights put on every observation differ much more when estimating the sample offsets than when estimating the taxon offsets. That is the reason why the MLE differs more from the marginal sum for the library size than for the abundances.

Note also that when there is a very large overdispersion for a taxon _j_ ($\theta_j$ small), its observations carry little information and their weights are small in the calculation of the library sizes. However, when there is very little overdispersion ($\theta_j \rightarrow \infty$), the weights of the components of the score function equal 1, as with Poisson regression. These MLEs of the Poisson regression are also equal to the estimators based on the marginal sums. This means that the larger and the more diverse the overdispersion estimates are, the more the MLEs under the negative binomial model will depart from the marginal sums. Finally, we see that departures from the mean $\mu_{ij}$ are weighted down for large values of $\mu_{ij}$, acknowledging the fact that the variance increases faster than linear with the mean in the negative binomial model.

#### Imposing the restrictions

The centering, normalization and orthogonality of the row and column scores are enforced through Lagrange multipliers. This makes the systems of equations harder to solve but assures independence of the dimensions.

#### A note on gradient analysis

Note that this is a case of __indirect gradient__ analysis: the dimensions are estimated without incorporating gradient information (measured covariates) to maximally represent variability in the data. In a next step these obtained gradients can be compared with measured covariates (often graphically through a colour code) to see how well these covariates explain the gradients. 

#### Relationship between CA and log-linear analysis

According to Escoufier, 1985 if $a =\sum_{k=1}^K \omega_k v_{ki} w_{jk}$ is small (i.e. the deviation from independence is small) then $log(1+a) \approx a$ and

$$log(E(x_{ij})) = log(x_{i.}) + log(x_{.j}) - log(x_{..}) + log\big(1 + \sum_{k=1}^K \omega_k v_{ki} w_{jk}\big) \approx log(x_{i.}) + log(x_{.j}) - log(x_{..}) + \sum_{k=1}^K \omega_k v_{ki} w_{jk}$$

i.e. an equivalence between the RC(M)-model and correspondence analysis

Since the same restricitions apply to the scores $v_{ki}$ and $w_{jk}$ as to U and V, we can state that  $\psi_k \approx \omega_k$. The assumption that the departure from independence is small seems unlikely in our case, but it does provide us with some starting values.

#### The RC(M) model and time series

Often we have data from experiments with repeated measurements, e.g. the evolution of the infant microbiome over time. Should we take this into account while fitting the RC(M) model? I do not think so, since it is an exploratroy method and we just want to visualize the variation in the data. It would be interesting to see if the individuals form clusters or not. In the constrained version of this method (see below), we can always add labels for the individuals and  time variable.

### Fitting the RC(M) model

The models are usually fitted by ML. See page 55 of Goodman 1985 for details. Goodman assumed a Poisson distribution, but it might be possible to use a negative binomial model with unique dispersions for each taxon $j$ as well.

See Goodman 1985 (appendix), Becker 1988 for information on estimation. The code below is generic but we take k=2 for simplicity.

Parameters of interest to estimate are $\psi$, $r_1$, $r_2$, $c_1$, $c_2$. In the process we'll also need the nuisance parameters $\mathbf{\theta}$ and the offsets of the independence model $u_i$ and $u_j$. Assume $E(X_{ij}) = \mu_{ij}$ and  $Var(X_{ij})=\mu_{ij} + \frac{\mu_{ij}^2}{\theta}$

#### The negative binomial density function

For the sake of completeness we give the density function of the negative binomial distribution (see Lawless, 1987) in the ($\mu$, $\theta$) parametrization whereby $E(X_{ij}) = \mu_{ij}$ and $Var(X_{ij}) = \mu_{ij} + \frac{\mu_{ij}^2}{\theta_j}$ .

$$f_{NB}(X_{ij}) = \frac{\Gamma(X_{ij}+\theta_j)}{\Gamma(\theta_j)} \big(\frac{\mu_{ij}}{\theta_j+\mu_{ij}}\big)^{X_{ij}} \big(\frac{\theta_j}{\theta_j+\mu_{ij}}\big)^{\theta_j}$$

#### Starting values

1. Obtain a singular value decomposition as $R^{-c}(X-E)C^{-d} = U\Sigma V$. This gives us initial values $[r_{11}^{init}, r_{12}^{init},..., r_{1i}^{init}] = U_{(1.)}$, the first row of $U$, and correspondingly $\Sigma$ and $V$ give initial values for $\mathbf{\psi}$ and $\mathbf{s}$. The values for $c$ and $d$ were determind based on correlation with the final values, and $c=d=1$ performed best.

We still need to ensure that the (weighted) variances equal 1, so we set

$$r_{ki}^{init} = \big(\frac{w_ir_{ki}^{SVD}}{\sum_{i=1}^n{r^{SVD}_{ki}}^2w_i}\big)^{1/2}$$

and

$$s_{ik}^{init} = \big(\frac{z_js_{ik}^{SVD}}{\sum_{i=1}^n{s^{SVD}_{ik}}^2z_j}\big)^{1/2}$$

#### Iteration

2. If the independence model is estimated by MLE, estimate the $u_i$'s, $u_j$'s and $\theta_j$'s iteratively until convergence (set $u=0$). The estimating equeations are independent (Jacobian is diagonal) so this converges very quickly. Otherwise set $u_i = log(x_{i.})$ , $u_j = log(x_{.j})$ and $u = log(x_{..})$.

3. Estimate overdispersions

Theoretically we might estimate the dispersions by ML, but the estimates are very variable, which dramatically affects the rest of the fitting procedure.

 - Maximum likelihood estimation (not used) 
 
$$log(E(x_{ij})) = l_{ij} = offset\big(log(\frac{x_{i.}x_{.j}}{x_{..}}  \big) + \psi_1 (s_{1i}^{init}r_{1j}^{init}) + \psi_2 (s_{2i}^{init}r_{2j}^{init})\big)$$

to obtain estimates for the overdispersions $\theta_j$, whereby the means $\mu_{ij}$ are assumed known

 $$\sum_{i=1}^n \sum^{y_i^*}_{t=0} \big( \frac{t}{1+\frac{t}{\theta_j} } \big) + \theta_j^2 log(1+\frac{\mu_{ij}}{\theta_j})- \mu_{ij}\frac{y_{ij}+\theta_j}{1+\frac{\mu_{ij}}{\theta_j} }$$
 
 For this we can use the theta.ml() function in the _MASS_ package
 
 - edgeR robust
 
To get more stable estimates of the dispersions, RNA-seq packages such as _DESeq_ and _edgeR_ use an empirical Bayes approach to shrink dispersion estimates towards a fitted mean-dispersion trend. Here I use the machinery of edgeR robust (version `r packageVersion("edgeR")`) to estimate the dispersions
 
To reduce the computational cost and because the estimates do not change a lot anyway the estimation of the overdispersions is not repeated in every iteration

4. Estimate the psi parameters
 
 Solve the system of score equations (see Lawless 1987) for $\psi_1$ and $\psi_2$, assuming $\theta_j$'s given
 
 $$\sum_{i=1}^n \sum_{j=1}^p r_{im}s_{mj} \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}} = 0$$ for all m=1,...,k
 
 with
 
 $$E(X_{ij}) = \mu_{ij} = exp\big(log(\frac{x_{i.}x{.j}}{x_{..}}) + \psi_1^{MLE} s_{1j}^{init}r_{1i}^{init} + \psi_2^{MLE} s_{2j}^{init}r_{2i}^{init} \big)$$.
 
 These are non-linear equations for which we use the _nleqslv_ package, with its default the Broyden method. This is a variation of the Newton-Raphson method (or Fisher scoring in case of ML) whereby the Jacobian is recycled and not recalculated at every iteration. The Jacobian is a square matrix of dimension k*(k-1)/2. The diagonal entry at position $kk'$ is:
 
 $$\frac{\delta^2 L_{NB}(\mu, \theta)}{\delta \psi_k \delta \psi_{k'}} = -\sum_{i=1}^n \sum_{j=1}^p r_{ik}r_{ik'}s_{kj}s_{k'j}\frac{\mu_{ij}(1+\frac{x_{ij}}{\theta_j})}{(1+\frac{\mu_{ij}}{\theta_j})^2}$$
 
5. Estimate the row scores. 

To estimate the $r_{i}$'s we would like to use weighted least squares to minimize

$$\sum_{j=1}^p\Big(log(x_{ij}) - \big(u + u_i + u_j +\sum_{k=1}^M r_{ik} \big( \hat{\psi_ks_{jk}} \big)\Big)^2$$

for every $i$. Unfortunately we cannot calculate $log(x_{ij})$ because of the zeroes.

We'll have to use another way to estimate them, maximum likelihood in our case. I don't like using weighted least squares on the non-transformed counts,

$$\sum_{j=1}^p\Big(x_{ij} - exp\big(u + u_i + u_j +\sum_{k=1}^M r_{ik} \big( \hat{\psi_ks_{jk}} \big)\Big)^2,$$

because of the skewness of the residuals as explained above.

To enforce the constraints on the row scores one option would be to estimate all scores through full maximum likelihood and then modify a few of them to make them satisfy the constraints and repeat until convergence. This runs into numerical problems though, because the modified scores are usually very large initially which leads to overflow when exponenttiated. Instead we use the methods of the Lagrange multipliers to implement the constraints. We thus seek to maximize the Lagrangian function

$$Lag(r_{ik}, \mathbf{\lambda}) = L_{NB}(\mathbf{r}) + \sum_{k=1}^M \big( \lambda_{1k} \sum_{i=1}^n w_i r_{ik} \big) + \sum_{k=1}^M \big( \lambda_{2k} ( \sum_{i=1}^n w_i r_{ik}^2 ) - 1 \big) + \sum_{k \neq k'} \big(\lambda_{3kk'} \sum_{i=1}^n (w_ir_{ik}r_{ik'}) \big)$$

with $L_{NB}(\mathbf{r})$ the log-likelihood function of the negative binomial regression. The derivatives of this function for the following system of equations

$$\frac{\partial Lag(r_{ik}, \mathbf{\lambda})}{\partial r_{ik}} = \sum_{j=1}^p \hat{s}_{jk} \hat{\psi}_k \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}} + \sum_{k=1}^M w_i \lambda_{1k}  + \sum_{k=1}^M \big(2 \lambda_{2k} w_ir_{ik}\big) +  \sum_{k' \neq k} w_ir_{ki} \lambda_{kk'} = 0$$

and 

$$\frac{\partial Lag(r_{ik}, \mathbf{\lambda})}{\partial \lambda_{1k}} = \sum_{i=1}^n w_i r_{ik} = 0$$

and 

$$\frac{\partial Lag(r_{ik}, \mathbf{\lambda})}{\partial \lambda_{2k}} = (\sum_{i=1}^n w_i r_{ik}^2) - 1 = 0$$

and 

$$\frac{\partial Lag(r_{ik}, \mathbf{\lambda}))}{\partial \lambda_{3kk'}} = (\sum_{i=1}^n w_i r_{ik}r_{ik'}) - 1 = 0$$

. The latter three groups of equations represent the original centering, normalization and orthogonality constraints. 

There are additional equations in the system now, but also as many extra $\lambda$ parameter to optimize. These $\lambda$ parameters are also stored and used as initial values in the next iteration run. There size represent the extent to which the constraints pull away the solution from the optimal, unconstrained solution, but I don't see how we can use that in a biologically or statistically meaningful way.

We now truly have a system of equations to solve, which we do with the _nleqslv_ package. It is straightforward but a bit cumbersome to also provide the analytical Jacobian, which greatly speeds up the iterations (otherwise it has to be approximated numerically). The Jacobian is sparse and symmetric. with following non-zero entries:

 $$\frac{\delta^2 Lag(\mu, \theta)}{ \delta r_{ik} \delta r_{ik'}} = -\sum_{i=1}^n \sum_{j=1}^p \psi_{k}\psi_{k'}s_{kj}s_{k'j}\frac{\mu_{ij}(1+\frac{x_{ij}}{\theta_j})}{(1+\frac{\mu_{ij}}{\theta_j})^2}=0$$
 
  $$\frac{\delta^2 Lag(\mu, \theta)}{\delta r_{ik} \delta \lambda_{1k}} = w_i$$
  
  $$\frac{\delta^2 Lag(\mu, \theta)}{\delta r_{ik} \delta \lambda_{2k}} = 2w_ir_{ik}$$
  
  $$\frac{\delta^2 Lag(\mu, \theta)}{\delta r_{ik} \delta \lambda_{3kk'}} = w_ir_{ik'}$$
  
  All other entries are zero.
  
6. Estimate the column scores

Repeat step 4 but now estimate $s_{jk}$ column scores in the restrictions, e.g. the Lagrangian becomes:

$$Lag(s_{jk}, \mathbf{\lambda}) = L_{NB}(s_{jk}) + \sum_{k=1}^M \big( \lambda_{1k} \sum_{j=1}^p z_j s_{jk} \big) + \sum_{k=1}^M \big( \lambda_{2k}  ( \sum_{j=1}^p z_js_{jk}^2 ) - 1 \big) + \sum_{k \neq k'} \big(\lambda_{3kk'} \sum_{j=1}^p (z_js_{jk}s_{jk'}) - 1\big)$$

#### Termination

7. Repeat steps 3-6 until convergence. Convergence is assumed if between two iterations

 - None of the $\psi$ parameters change less than $0.01\%$ (infinity norm)
 
 AND
 
 - None of the square roots of the L2-norms of the relative changes of row and column scores exceeds 0.1%, or
 
 $$\forall \mathbf{r_k}, \mathbf{s_k} \Big(\sqrt{\sum_{i=1}^n (1-\frac{r^{(iter-1)}_{ik}}{r^{(iter)}_{ik}})^2} < 0.001 \Big) \cup \Big( \sqrt{\sum_{j=1}^p (1-\frac{s^{(iter-1)}_{kj}}{s^{(iter)}_{kj}})^2} < 0.001 \Big)$$
 
 and analogously for all the scores. Other norms, such as L1 or infinity norms can also be used. A norm between 1 and 2 will lead to fast convergence.
 
 Overdispersions are not taken into account in determining the convergence since they're merely nuisance parameters.

#### Biplot

8. Plot $\psi_1r_{1i}$ vs $\psi_2r_{2i}$ as sample points and add $s_{1j}$ vs $s_{2j}$ as arrows to make a biplot. This assures that the orthogonal projection of the vector ($\psi_1r_{1i}$, $\psi_2r_{2i}$) on ($s_{1j}$, $s_{2j}$) equals $cos (\eta) ||(\psi_1r_{1i}, \psi_2r_{2i})|| = \frac{(s_{1j}, s_{2j})^t(\psi_1r_{1i}, \psi_2r_{2i})}{||(s_{1j}, s_{2j})||}$ with $\eta$ the angle between the two vectors. This projection is thus proportional to the departure from independence in the first two dimensions combined. The larger the entries of the species and sample scores (the scaling between these two sets is arbitrary, we usually choose them in the same order of magnitude) and the smaller the angle, the larger the departure of this taxon in this sample. The choice of $\psi_1r_{1i}$ vs $\psi_2r_{2i}$ assures the distances between samples are well represented, since more weight is added to differences in scores in important dimensions.

In the end we'll have estimated  p (abundances) + n (library sizes) + p (dispersions) + kxp (column scores) + kxn (row scores) + k (importance parameters) = (k+2)p + (k+1)xn + k parameters out of np entries. We have imposed 4k + k(k-1) restrictions, so the final model is still very parsimonious for n and p in the hundreds.

##### Model quality

9. Assess the goodness of fit

It would be interesting to assess the goodness of fit of our models. Questions that come to mind are:

 - How do the different dimensions of the fit relate to one another in terms of importance? How well does our lower dimensional solution represent high-dimensional departures from independence?
 - How much better are quadratic or nonparametric response functions compared to the linear response functions?
 - Which samples and/or taxa are poorly represented in the low-dimensional solution? Which ones exhibit lack of fit to the negative binomial distribution?
 - Which samples/taxa have a strong influence on the final model fit?
 
###### Relative importance of dimensions

Mean

The first measure of differences in importance between the dimensions are of course the importance parameters $\psi_k$. Since all other parameters in both the unconstrained and constrained variables are normalized, these are the only ones that can grow in magnitude to give more weight to the departures of independence in their dimension. The same concept exists for correspondence analysis, where the size of the diagonal elements of $\Sigma$ is proportional to the importance of the corresponding dimension. In the unconstrained case this works very well, but in the constrained case the magnitude of the $\psi_k$'s is not always monotonically decreasing with the dimensions. This is because the order of the dimensions is determined by the way the response functions are separated, not just by the mean. Also we do not dispose of the "full" solution of importance parameters as in singular value or eigenvalue approaches, only the dimensions we fitted. This complicates the comparison with the total variance.

Likelihood

Another approach than to look at the mean is to look at likelihoods. After all our model is fitted largely with maximum likelihood. Since the model is overparametrized, classical ML theory (such as asymptotic behavious of maximum likelihood statistics) does not apply to our solution. Still we can compare the likelihoods of the independence, the RC(M) and the intermediate models to get an idea of the relative importance of the dimensions.

We can decompose the difference in log-likelihood between the independence and the full RC(K) model as

$$(LL_K - LL_0) = (LL_1 - LL_0) + (LL_2 - LL_1) + ... + (LL_K - LL_{K-1})$$

with $LL_k$ the log-likelihoods of RC(k) model, $LL_0$ being the independence model. Scaling by $(LL_{sat} - LL_0)$ with $LL-sat$ the log-likelihood of the saturated model, will provide interpretable fractions of the log-likelihood explained. The trouble is how to estimate the log likelihood of a saturated model, especially the overdispersion. We just use the likelihood of the Poisson distribution here, since the dispersions will be zero.

This approach leads to weird results for the constrained case as well, because the environmental gradients are not estimated to maximize the likelihood. For the the order of the dimensions is dictated by optimal separation of the species' niches, for good and for ill.

This criterion can also be used to assess the differences in likelihood between constrained methods with different shapes of the response functions (see later).

All in all the psis are the most interpretable measure of importance of each dimension. Means are more interpretable than likelihoods.

###### Comparison of response functions

In case the models are fitted on exactly the same dataset, we can compare the likelihoods of these models. Unconstrained models are freer and generally have more parameters, hence their likelihood is expected to be higher. On the other hand, the constrained models get more information, which they might exploit to beat the constrained ones. Within the constrained models, the likelihood is expected to increase from linear over quadratic to non-parametric response functions. However, if we estimate the environmental gradient $\alpha$ through the likelihood ratio criterion, this criterion does not necessarily maximize the likelihood or guarantee an increasing likelihood with the dimensions.

###### Detecting lack of fit

In our 2D or 3D representation, some samples and taxa may be very well represented, but others not. This may be because of a lack of fit of the negative binomial distribution _tout court_, or because its departure from independence cannot be represented in lower dimension. anyhow, our plots will not be a truthful representation for this sample or taxon, and the end user should be warned of this.

Deviance (in relation to the saturated model) would be one option. The deviance is porportional to the difference in log-likelihoods of a certain model and the saturated model. The taxon- and samplewise deviances could be calculated and plotted using a colour code. In the end all samples and taxa will be "poorly" fitted, we just need a relative measure of which ones are worst.

###### Identifying influential observations

Since we have explicitly expressed all score functions, we can easily identify influential observations using _influence functions_. They represent the influence a certain observation has on parameter, keeping the other sorts of parameters fixed. Because of the iterative algorithm this assumption is incorrect, maybe the influence functions cannot always be trusted. The long fitting times also exclude (leave-one-out) refitting procedures.

For the unconstrained case in a scenario without outliers the influence functions may not yield very surprising results on the level of the plot, observations mainly have influence on their own row and column scores. Coupling through the constraints is rather weak. It may however help to identify outlying abundances in case of outlying row- or column scores.

For the constrained case it may be enlightning to see which samples (and taxa) affect the estimation of the environmental gradient most.

### One-by-one (1B1)

Initially all dimension were estimated jointly, but for reasons of speed we moved to a one-by-one approach estimating first the first dimension, then the second given the first, then the third given the first two and so on. By splitting the estimation procedures into smaller parts like this the estimation speeds up considerably, presumably because the systems are easier to solve.

### Confounders (Conditioning)

Sometimes some covariates are known to affect the abundances but one is not interested in their effect. We think first and foremost of technical parameters such as sequencing center or technology. We would like to condition on these parameters and visualize remainign variability.

#### Conditioning in correspondence analysis

Conditioning in correspondence analysis occurs by performing CCA with a confounder matrix Z as covariates, and then perform regular CA on the matrix of residuals.

#### Conditioning in the RCM approach

Thanks to our log-linear approach we can filter out the effect of these parameters by including interaction terms between the taxa and the confounding covariate to be filtered out. This willl occcur after fitting the independence model but before fitting the RC(M) component. We thus fit the following model in case of $c$ confounding variables:

 $$log(E(X_{ij})) = \lambda + \lambda_i + \lambda_j + \textcolor{red}{\sum_{l=1}^c \zeta_{jl}e_{il}} + \sum_{k=1}^M \psi_k r_{ik} s_{jk}$$
 
with $e_{il}$ the value of covariate $l$ in sample $i$ and $\zeta_{jl}$ the interaction parameter between taxon $j$ and confounding variable $l$. Note that we assume that the categorical variables have already been converted to multiple dummy variables.

#### Confounders and zero counts

Filtering out the effect of confounders brings the problems of the zeroes to the fore again. If a taxon has only zero counts in one of the subgroups defined by the confounding variables, the model will fail to fit, since the parameters $\zeta_{jl}$ would need to equal minus infinity. For continuous confounders this is of course not an issue. Maybe there exists a more efficient solution, but for now we will have to trim the taxa for which this occurs from further analysis in order to render the model identifiable. This trimming occurs already before fitting the independence model.

#### Weaknesses

The main drawback of our method is of course its strong reliance on the assumption of counts following the negative binomial distribution. The robustness to the violation of this assumption will need to be investigated. 

Also our method does not take taxon-interaction networks into account. It assumes independence between taxon counts. However, this is something we could still add to the method.

Finally also phylogenetic relation ships between taxa are not used, but this need not be a problem. Our method is a real compositional method: it reveals differences in taxon compositions between samples, and need not be influenced by "prior" information such as evolutionary relationships. After all taxa in the same body site are not guaranteed to be evolutionarily related, and vice versa distinct body sites can harbour closely related taxa.

#### Implementation

For the filtering based on the prevalence and abundance, we need a confounder matrix

 1. Without intercept (Overall filtering has happened already)
 2. With __all__ levels of the categorical variables, so set $contrasts=FALSE$ to avoid hidden reference levels of the factors
 
 When actually filtering on the confounders, we can simply use a confounder matrix
 
 1. With intercept
 2. With treatment coding
 
 In this case all that matters is to change the offset formed by the independence model. We will still return the confoundes' parameters though.

### Zero-inflated poisson

We could also augment our model with a zero-inflated Poisson rather than a negative binomial distribution. This opens additional modeling perspectives. We can model the chance on a structural zeroes:

$$logit(P(X_{ij}=0)) = f + f_i + f_j + t_{i1}v_{j1}\chi_1 + t_{i2}v_{j2}\chi_2$$.

The probability of a strucutral zero does not depend on the sample: strucutural zeroes are assumed to be truly absent species and not due to undersampling. All samples come from the same population under $H_0$, and there the structural zeroes are only dependent on the columns. Therefor we set all $f_i=0$. In practice $f$ is not estimated either, instead $f_j$ is estimated without restrictions. The terms $t_{i1}v_{j1}\chi_1 + t_{i2}v_{j2}\chi_2$ indicate in which two independent directions the observed number of zeroes deviates from this base level. I think we need the term $f_i$ because otherwise the first dimension $t_{i1}$ scores will start correcting for it. The independence moel in this case is that the chance on a structural zero only depends on the marginal probabilities. A logit model without an intercept does not make sense, it assumes a baseline probability of structural zero of 0.5.

Fitting the mean part of this models encounters severe numerical problems, this part of the project is not retained and is put in the fridge.

#### Restrictions

The same restrictions apply to the scores $t_i$ and $v_j$ as to $r_i$ and $s_j$

#### Estimation (see Lambert 1992 for details)

The mean and zero probability are modelled independently, absence of a species is independent of its abundance when it is present. Say $Z=0$ when $X$ is from the Poisson state and $Z=1$ when X is a structural zero. Evidently, when $X=0$, Z is unknown.

EM algorithm: iterate between 

 - E: estimate Z, assuming  $\mathbf{r_i}$, $\mathbf{s_j}$ and $\mathbf{\Psi}$ known, through its expectation
 - M: Maximize the log-likelihood given Z
 
Unlike the RC(2)NB model, $f_j$ has to be estimated as well, there is no obvious candidate here. We estimate it marginally and then keep it fixed during the iterations. This way the score visualize departures in zero inflation from this independence of zero-inflation between rows and columns.

Newton-Raphson would be incredibly complicated here, the derivatives of the log-likelihood functions barely fit on a page!

## Constrained analysis

So far we have been doing unconstrained or unsupervised explorative visualization, or _indirect_ gradient analysis. We just visualized the strongest signals in the data, independent of sample covariates. Only then did we use the covariate information to try to correlate the covariates with the latent variables formed by the row and column scores. We usually do this by adding a colour code for the samples in our biplots.

A logical next step is to develop a __constrained__ method in the same framework, i.e. with the same error distribution. We argued that PCoA with Bray-Curtis distance or regular correspondence analysis did not perform well, and we expect the same for canonical correspondence analysis since it relies on the same distributional assumptions. We will visualize those departures from the independence model that can be explained by the sample covariates. Each dimension will thus consist of a linear combination of sample covariates, the weight of each of these covariates in the dimensions reflecting how much they contribute to explaining the variation. For background information see also Zhu _et al._ , 2005, and Zhang and Thas, 2012.

We call our n-by-d covariate matrix $C$, for every sample $i$, $d$ baseline covariates are measured. These covariates can be either discrete or continuous. $c_{iy}$ is the observed value of covariate $y$ in sample $i$, $\mathbf{c_{i.}}$ the whole covariate vector of length $d$ of sample $i$, and $\mathbf{c_{.y}}$ all recorded values of variable $y$.

We could of course just perform our classical, unconstrained methods and then regress the obtained sample scores on the envrionmental variables. However, it is to be expected that this is a less efficient use of the data, which has been shown for the case of correspondence analysis by Prodon and Lebreton (1994).

### Canonical correlation analysis

Canonical correlation analysis seeks to compare multivariate sets of random variables __X__ (n-by-p) and __Y__ (n-by-m) with variables in the columns. This way one discovers which variables correlate best between both data sets. Usually both data sets represent different kinds of measurements, e.g. dietary habits vs. health condition, but all measured on the same individual. It looks for linear transformations __Xa__ and __Yb__ that maximize the correlation $\rho = cor(\mathbf{Xa}, \mathbf{Yb})$. We call these transformations the _canonical covariates_ $\mathbf{Xa_i} = \mathbf{U}_i$ and $\mathbf{Yb_i} = \mathbf{V}_i$ and the corresponding correlation $\rho_i$ the _canonical correlation_. The consecutive canonical covariates are linearly independent, the canonical correlation coefficients decrease over the dimensions of the solution.

__X__ and __Y__ are column centered to obtain __A__ and __B__, and then we look for projecting vectors __x__ and __y__ that maximize correlation between $\mathbf{z}_x = \mathbf{Ax}$ and $\mathbf{z}_y = \mathbf{Ay}$. Note that there is no notion of percentage of variance explained, and that this method is symmetric with respect to datasets __X__ and __Y__.

### Redundancy analysis (RDA)

Redundancy analysis extracts variation between variables of a n-by-p dataset __X__ that can be explained by variables from another n-by-d dataset __Y__ (column centered). It is a constrained version of principal components analysis (PCA), whereby principal components are restricted to be linear combinations of the set of constraining variables.

In a first step the rows of __X__ are fitted on those of __Y__ in multiple linear regression:

$$\widehat{X} = X (Y^tY)^{-1}Y^t X$$

Next the covariance matrix $\widehat{X}^t\widehat{X}$ is subjected to regular PCA:

$$\widehat{X}^t\widehat{X} = W \Lambda W^t$$
We plot the first columns of $\widehat{X} W \Lambda$ to represent the variability of __X__ explained by __Y__. Components of $B = (Y^tY)^{-1}Y^t X$ may be added to show which columns of __Y__ explain most variance of __X__.

#### Partial redundancy analysis

In partial redundancy analysis, the effect of a second covariate matrix Z is conditioned on prior to PCA or RDA. __X__ is regressed on __Z__, PCA or RDA proceeds on the residual matrix of this regression.

### Constrained/canonical correspondence analysis (CCA)

The idea behind canonical or constrained correspondence analysis is to have unimodal response functions as with regular CA, but to restrict the row scores to be linear functions of the environmental variables. Also, it is Gaussian ordination with the scores equally restricted under the same conditions as the unrestricted Gaussian ordination. CCA relates to CA as RDA relates to PCA.

Of course one could perform regular correspondence analysis, and then relate the obtained gradients to known envrionmental variables in a two-step approach. This is again an instance of _indirect_ gradient analysis (see Whittaker 1967).

There are many ways to introduce and explain CCA, with many parallels to other (forgotten) methods, but we will stick to the approach based on the singular value decomposition.

In contrast to redundancy analysis, constrained correspondence analysis (CCA) assumes bell-shaped, unimodal response curves with respect to environmental variables. A __response curve__ describes the expected abundance of a species in function of an environmental score, which is a function fo environmental variables. Redundancy analysis asssumes linear responses to the environmental variables, which is less useful in the ecological context. Classical ecological theory assumes species have an optimal combination of values of the environmental variables, and their expected abundances decrease as the environment departs from these optimal values.

Ter Braak (1987) proposes a method of direct gradient analysis based on CA: canonical correspondence analysis (CCA). It basically assumes that species have a Gaussian response curve in function of an environmental score which is a _linear_ conbination of environmental variables. The linear projection vector $\boldsymbol{\alpha}$ is called the __environmental gradient__. The row scores of the ordination are restricted to be linear combinations of the environmental variables. Prior to the analysis the environmental variables are centered and scaled to avoid arbitrariness in the choice of the units and render the elements of the envrionmental gradient comparable. The estimating equations are:

1) Species scores $\mathbf{s} = \mathbf{J}^{-1}\mathbf{X}^t\mathbf{r}$
2) Sample scores (weighted average taxon scores) $\mathbf{r*} = \mathbf{R}^{-1}\mathbf{Xs}$
3) Environmental gradient $\boldsymbol{\alpha} = (\mathbf{C}^t\mathbf{RC})^{-1}\mathbf{C}^t\mathbf{Rr*}$: weighted regression on environmental variables
4) Sample scores (environmental) $\lambda\mathbf{r} = \mathbf{C}\boldsymbol{\alpha}$

whereby $\lambda$ is a measure of the importance of that dimension. We can write this also as the following estimating equation:

$$(C^TXJ^{-1}X^TC - \boldsymbol{\lambda}C^TRC)\boldsymbol{\alpha} = \mathbf{0}$$

or as 

$$(C^TX(C^TRC)^{-1}X^TC - \boldsymbol{\lambda}J)\mathbf{s} = \mathbf{0}$$

The first equation represents a generalized eigenvalue problem, the second a regular eigenvalue problem since __J__ is a diagonal matrix.

Another approximation to CCA follows more the path we have before with CA. Hereby the rows of the chi-square transformed matrix $\mathbf{R^{-1/2}(X-E)C^{-1/2}}$ are subjected to weighted least squares regression on the environmental variables, with the sample totals $\mathbf{R}$ as weights. The matrix of environmental variables undergoes weighted centering prior to model fitting, with the library sizes as weights. The row-weight-centered matrix $C'$ is thus calculated as $C' = C(I - \frac{1}{c_{..}d}R\mathds{1}$. Then singular value decomposition is applied to the matrix of fitted values $F = Y (C^T R C)^{-1} C' R R^{-1/2}(X-E)C^{-1/2}$ of this regression, such that $F = U'\Sigma'V'$. Using the fitted values assures that only the systematic part explained by the covariates plays a role. 

A third approach by ter Braak in _"The history of canonical correspondence analysis"_ (2014) uses a direct SVD. Here he also references Goodman's work. The singular value approach he proposes is:

$$(Y^tRY)^{-1/2}Y^tXC^{-1/2} = U\Sigma V^t$$

The environmental gradients are then $(Y^tRY)^{-1/2}U \Sigma$, the taxon scores $C^{-1/2}V$.

CCA is related to weighted Redundancy analysis (RDA) of the Chi-squared distances, just like regular CA is related to PCoA of Chi-squared distances.

Legendre and Legendre (2012) describe an even different way of fitting the CCA. Because of all these algorithms and the use of weights, CCA is a very difficult method to interpret and compare, since it is not always clear which version is being used in which case.

#### CCA triplots

In a triplot of a CCA ordination, the samples and species are plotted as points, and the elements of the environmental gradient as arrows. The species scores are the right singular vectors, the sample scores can be calculated as linear combinations of environmental variables (environmental scores), or as the weighted average of their species scores (weighted by species abundances). In the latter case, the closer the species points lie to to the sample point, the higher the expected abundance. The orthogonal projection of the species point onto the environmental arrow represents how much the expected abundance increases (decreases) with changes in this environmental variable.

#### Conditioning in CCA

Conditioning occurs in the same fashion as with regular CA: the residuals of the CCA on the conditioning matrix Z are the input for the CCA on the environmental variables of interest.

### Redundancy analysis (RDA)

Redundancy analysis (RDA) is a constrained version of PCA, whereby the loadings are constrained to be linear combinations of a set of exploratory variables. This result can be compared to unconstrained PCA to see how much of the total variance is accounted for by the constrained variance. This type of analysis is indicated in case of linear relationships between the response variables (species counts) and explanatory variables. In our case this would e.g. be a environmental gradient in dimension $k$ that is dominated by the concentration of a toxic substance, which only decreases the expected abundance as its concentration increases. Another option is that the gradient is dominated by a variable with a short gradient.

### Constrained Analysis of Principal Coordinates

Constrained Analysis of Principal Coordinates (CAP) can be seen as a constrained version of PCoA. It is similar to RDA but extends to dissimilarity measures other than the Euclidean distance. See Anderson and Willis (2003).

### Constrained RC(M) model

The following approaches to constrained analysis of ecological data exist, or have been considered

#### The classical approach for gradient analysis

This model comes from ecology and is the one used by Zhu _et al._ (2005) and Zhang and Thas (2012 and 2016). This assumes each taxon has an optimal combination of covariates, and its expected abundance decreases as the covariates depart from this optimal combination. The expected abundance follows a Gaussian distribution in function of this departure.

The expected counts are modelled as 

$$log(E(X_{ij})) = \sum_{k=1}^K \big(a_{jk} - \frac{(h_{ik} - q_{jk})^2}{2m_{jk}^2}\big)$$

whereby 

 - $h_{ik} = \boldsymbol{\alpha}_k^t \mathbf{c}_{i.}$ the optimal linear combination of $\mathbf{c}_{i.}$ in dimension $k$, also called the _environmental score_
 - $\boldsymbol{\alpha}_k$ is called the _environmental gradient_
 - $\alpha_{ky}$ is element corresponding to covariate $y$ of the environmental gradient in dimension $k$
 - $a_{jk}$ is the maximal logged expected abundance achieved when the linear combination of covariates $\boldsymbol{\alpha}_k^t \mathbf{c}_{i.}$ equals the optimum $q_{jk}$
 - $m_{jk}$ is the tolerance, it determines how fast the expected value decreases as $\mathbf{c}_{i.}$ moves away from its optimal values $\boldsymbol{\alpha}_k^{-t}h_{ik}$
 
 The scales of $h_{ik}$, $q_{jk}$ and $m_{jk}$ are undefined, so we need an additional restriction to render the model identifiable. Zhu _et al._, 2005 propose setting $\sum_{j=1}^p m_{jk}^2/p=1$. Note that $a_{jk}$, $q_{jk}$ and $m_{jk}$ are species specific parameters whereas $\boldsymbol{\alpha}_k$ is specific for the dimension but common to all species.
 
 Does the environmental score really need to be a result of a _linear_ projection vector? From the modelling perspective perhaps not, but it does really ease the interpretation of course, which is hard enough already.
 
##### Triplot

- $q_{jk}$ is plotted as points as optimum per species
- The components of $\boldsymbol{\alpha}$ are plotted as arrows for the environmental variables.
- The samples are plotted as their site scores $\boldsymbol{\alpha}_k^t \mathbf{c}_{i.}$
 
##### Estimation

The estimation iterates between estimating $\boldsymbol{\alpha}_k$ and $\boldsymbol{\beta}_{jk} = \big(a_{jk}, q_{jk}, m_{jk})$.

Note that this model is fitted without an intercept

<!---This model was designed with the ecological context in mind, whereby (continuous) environmental variables such as pH, temperature, sunlight intensity, ion concentrations, distance to landscape elements etc. were measured. However for microbiome data, we usually do not have direct measurements of variables in the ecosystem, but rather characterstics of the patient, which are very often categorical variables.

Let's take a look at the nature of the covariate data.

#```{r Nature of the covariates, eval=TRUE, purl=FALSE}
load("/home/stijn/PhD/American Gut/AGphylo.RData")
load("/home/stijn/PhD/Simulations/data/physeqList4Trim.RData")
load("/home/stijn/PhD/Simulations/data/zellerData.RData")
load("/home/stijn/PhD/Biplots/Kostic_data/phyloD.RData")
tableSamVar = function(x){
  table(sapply(sample_data(x), class))
}
phyList = list("American gut"=AGphylo, "Kostic"=phyloD, "HMP" = physeqList4Trim[["Throat"]], "zeller16S" = zellerSphy, "zellerMetagenomics" = zellerMphy)
sapply(phyList, tableSamVar)
#```

Most variables are factors or booleans. Below are the names of the variables shown:

#```{r covariate names, purl=FALSE}
sapply(phyList, sample_variables)
#```

I wonder how realistic the Gaussian response model still is with so many categorical variables and so many species. I guess we would best seek some expert advice of microbiome scientists.

--->

#### RC(M) approach to gradient analysis

#### Constrained RC(M), overparametrized version

If we want to retain analogy with the unconstrained version and model only the departure from independence, we could fit the following model:

$$log(E(X_{ij})) = u + u_i + u_j  + \sum_{k=1}^K \psi_k s_{jk} f_{jk}(\boldsymbol{\alpha}_k^t \mathbf{c}_{i.}) $$

with $f_j$ the species-specific response function, which can be linear, quadratic (Gaussian on count scale), non-parametric, ... . The usual restrictions apply to the column scores $\mathbf{s}$:

$$\mathbf{s^tu_{tax}} = \mathbf{0}$$
$$\mathbf{s_k^tUs_{k'}} = I(k = k')$$

with $u_{tax}$ the vector of $exp(u_j)$'s and  $\mathbf{U}$ a diagonal p-by-p matrix with $u_{tax}$ on the diagonal.

For $\alpha$ (a d-by-k matrix) we enforce the following restrictions:

- Normalization restricts the size, and dimensions are orthogonal (if $k \geq 3$)

$$\boldsymbol{\alpha}^t\boldsymbol{\alpha} = \mathbb{1}_d$$

with $\mathbb{1}_d$ the identity matrix of dimension $d$.

However, this model is overparametrized: $s_{jk}$ and $f_{jk}$ both model the response of a taxon $j$ in the $k$-th dimension.

### Constrained RC(M), identifiable version

A more parsimonious version of this model can be obtained by dropping the column scores.

For any response function with $q$ parameters $\boldsymbol{\beta}_j$ we model the mean as:

$$log(E(X_{ij})) = log(\mu_{ij}) = u_i + u_j + \sum_{k=1}^K \psi_k f_j \big( \boldsymbol{\alpha}_k^t \mathbf{C}_{i}|\boldsymbol{\beta}_j \big) .$$

Basically this represents a marriage of the RC(M) models of Goodman and the gradient models of Zhu and Hastie (2005). For the response function previously a quadratic function (Gaussian on count scale) has been proposed but this may not be realistic. Also the setting is different: we are looking for response function that maximize the __departure from independence__, no longer the abundances themselves! In a first stage we will try fit the linear model with $f(x) = \beta_0 + \beta_1 x$ but later we might estimate $f$ non-parametrically as in Zhu _et al._ (2005). 

Note that the concept of a "row score" is lost here: since every taxon reacts differently to changes in the environment $C_{i.}$. Still this will not yield a perfect fit since $\boldsymbol{\alpha}_k^t$ is only unique to the dimension and the number of parameters $q$ estimated per response function $f_j$ is much smaller than the number of samples $n$.

#### Shape of the response function

Note that our definition of the response function differs from the response function defined by Zhu and Hastie (2005): it no longer models mean abundance but mean departure from independence.

A linear response function may be most appropriate for problems with __short gradients__ i.e. whereby the difference in observed environmental variables is too short to devine more than an increase or decrease in departure from independence. Also in this case it is easy to interpret the effect of each of the environmental variables on the departure from independence. As so often in statistics, the linearity assumption may not be realistic, but renders models that are easy to interpret ("All models are wrong, but some models are useful"). 

For problems with __long gradients__, i.e. whereby species' departures of independence rise and drop (or drop and rise) within the score of the observed environmental scores, quadratic response functions may be more appropriate. This corresponds e.g. with the scenario whereby a species' abundance does bot depart a lot from independence for extreme values of the environmental score, and but does depart the independence model for an intermediate value of the environmental score, e.g. it thrives in this environment and its abundance is higher than under independence. In essence this is the same as the approach of Zhu and Hastie, only now the baseline is the independence model rather than 0. Every taxon thereby has its own baseline, the response function models departures from this baseline. 

Note that we usually do not choose the ranges of the environmental variables or scores, so that we cannot guarantee a range long enough for the quadratic response function to be appropriate. Note also that what is a long gradient for one species, may be a short one for another.

Even though the quadratic response function can be fitted, and/or has a "significantly" (quotation marks due to non-likelihood framework) higher likelihood than the linear response function, it may still be pointless if the maximum lies outside the range of the observed values for the environmental score. The peak location would then merely be an extrapolation, and a linear response function may be preferable. Even though the fit is worse than for the parabolic curve, it represents more truthfully the way the species reacts to the given values of the environmental gradient. Therefore we also provide a "dynamic"-option for the response function, whereby initially a quadratic model is fitted but discarded in favour of a linear one if the optimum lies outside of the range of observed environmental scores. Does this invalidate the likelihood-ratio criterion used to estimate the environmental gradient? I do not think this should be a problem. And should we use the same criterion then for the overall response function ignoring taxon labels? Probably yes, although this scenario seems highly unlikely. For plotting it is not very attractive to have different shapes of the response function for the same taxon in different dimensions.

If the user is unsure and has enough data, he may use non-parametric response funcion. This may improve the sample and covariate ordination, although the result for the species becomes harder to interpret.

 All in all we see a trade-off between quality of the ordination of the samples versus interpretability of the role of the taxa. Personally I tend to give the second factor most weight, to maximise the amount of information we visualize.

##### Relationship to constrained RC(M)

If we fit this method with only one covariate, namely a factor with a unique level for every sample, and all $\beta_{0j}$ (and $\beta_{2j}$) equal to zero , we expect to find the same solution as the unconstrained case. This means we do not constrain the ordination at all in this case and $\beta_{1jk} = s_{jk}$ and $\boldsymbol{\alpha}_{k}^t\mathbf{C}_i = r_{ik}$.

#### Restricitions

##### Restrictions on the envrionmental gradient

The $\boldsymbol{\alpha}$'s of different dimensions will be constrained as follows:

$$\boldsymbol{\alpha}_k^t\boldsymbol{\alpha}_k = \mathbf{1}$$

$$\boldsymbol{\alpha_k}^t\boldsymbol{\alpha_{k'}} = 0$$

or in shorter notation

$$\boldsymbol{\alpha}^t \boldsymbol{\alpha} = \mathds{1}$$

One restriction I intended to include but which turns out to be a __very bad idea__ is to center the $\alpha$'s:

$$\boldsymbol{\alpha}^t\mathbf{1} = \mathbf{0}$$

This would force the parameter of a single continuous covariate to be 0, the parameters of two continuous variables to be each other's opposite. In short it is not very desirable and is not implemented.

##### Restrictions on parameters of categorical variables in the environmental gradient

For continuous variables the previous discussion suffises, but for categorical variables special attention is required. In a regular, unrestricted setting one would use treatment coding, setting one of the levels of the variable to be the reference level and estimate no parameter for it. However, the above normalization and orthogonality constraints render the result dependent on the choice of reference level, which is of course highly undesirable. Also for quick understanding of the biplot it would be convenient to plot all the values of the categorical variable, and save the reader the trouble of looking up the reference value. Because of these two reasons we will need a coding scheme that includes all parameters.

Say we have $d_{cont}$ continuous covariates and $d_{cat}$ categorical variables with $d = d_{cont} + d_{cat}$, and $m_l$ levels for every categorical variable $l$. Then $\boldsymbol{\alpha}_k$ compromises

$$ q = d_{cont} + \sum_{l=1}^{d_{cat}} (m_l-1) $$
rows, or $q$ parameters to be estimated. Note that there is no intercept needed in this case, intercepts have already be included in the $f()$ function, such that samples with $\alpha_k = \mathbf{0}$ can still have departures from independence.  We can write $\boldsymbol{\alpha}_k$ then as:

$$\boldsymbol{\alpha}_k = (\beta_{cont_1}, ... , \beta_{cont_{d_{cont}}}, \beta_{cat_{11}}, \beta_{cat_{12}}, ..., \beta_{cat_{1(m_1-1)}}, ...,  \beta_{cat_{d_{cat}(m_{d_{cat}}-1)}})_k$$
, i.e. one paramater less than the number of levels. For imposing the restrictions and for plotting we want a representation whereby all covariate levels are represented. We therefore include all parameters, and impose that the parameters sum to 0 within each covariate.

$$\boldsymbol{\alpha}_k' = (\beta_{cont_1}', ... , \beta_{cont_{d_{cont}}}', \beta_{cat_{11}}', \beta_{cat_{12}}', ..., \beta_{cat_{1m_1}}', ...,  \beta_{cat_{d_{cat}m_{d_{cat}}}}')_k$$
Whereby $q' = d_{cont} + \sum_{l=1}^{d_{cat}}(m_l)$, but under the restriction that for all categorical variables $l$

$$\sum_{m=1}^{m_l} \beta_{cat_{lm}k}' = 0$$ 

We again have $g$ degrees of freedom. It is this $\boldsymbol{\alpha}_k'$ that will need to be centered, normalized and orthogonalized _in order to treat each variable level equally_ and _avoid dependence of the outcome on the choice of reference level_. Also this representation will come in handy for plotting, centering the levels of the same covariate around zero. We will call this the __zero-sum__ representation.

##### Restricitions on the parameters of the parametric response functions

The parameters of the response functions are restricted as follows (for linear and quadratic response functions):

$$\boldsymbol{\beta}_1^t\boldsymbol{\beta}_1 = \boldsymbol{\beta}_0^t\boldsymbol{\beta}_0 = 1$$
and for quadratic response functions:

$$\boldsymbol{\beta}_2^t\boldsymbol{\beta}_2 = 1$$

Again, only the $\psi$ parameters can grow in size to reflect the importance of the dimension, other paramters are normalized.

Since $\lambda_j$ already represents the offset for taxon $j$, should we fit the response functions without offsets (all $\beta_{0j} = 0$)? This means that if the environmental scores equals zero, there is no departure from independence. However, this does not give species fully the chance to react in a different way to the environmental gradient. For a certain value, some species might depart from independence while others might not. Therefore we leave $\beta_{0j}$ to be freely estimated, under the variance 1 restriction.

#### Scaling

To render the values of the continuous variables in the environmental gradient comparable, it is clear that they need to be centered and scaled prior to model fitting, as in PCA. This means that their corresponding elements of $\boldsymbol{\alpha}$ represent the contribution to the environmental score of one standard deviation away from the mean of this variable (in case of linear response functions). A perfect quantitative comparison to the magnitude of the values of the paramaters of the dummies of the categorical variables will never be possible. In our case, with 0-1 dummy coding, equal parameters for a dummy and a continuous variable imply that this level of the categorical variable contributes as much to the environmental score as one standard deviation away from the overall mean of the continuous variable.

#### A note on the importance parameters

For the constrained model we have noted that the ordering of the importance parameters $\psi_k$ is not always decreasing. This is because of the way the $\alpha$'s are estimated: by maximizing the separation of the niches of the species. The first dimension is thus the one that maximally separates these niches, regardless of the total impact on the modelled mean. Therefore I think it is appropriate to keep the ordering of the dimensions as our algorithm spits it out.

#### Estimation of the constrained RC(M) model

##### Environmental gradient

###### Log-likelihood criterion

Another aspect from niche theory is that species have eveolved to occupy maximally distinct niches. $\boldsymbol{\alpha}$ could be estimated by ML as before, but in order to ensure maximum separation of the response functions of all the species we will consider a version of the log-likelihood ratio approach from Zhu _et al._ (2005):

$$LR(\boldsymbol{\alpha}) = log \frac{\prod_{i=1}^n \prod_{j=1}^p \big(p_j^{(\alpha)}(x_{ij};\boldsymbol{\alpha}^T \mathbf{c}_i,\boldsymbol{\beta}_j) \big)}{\prod_{i=1}^n \prod_{j=1}^p \big(p^{(\alpha)}(x_{ij};\boldsymbol{\alpha}^T \mathbf{c}_i,\boldsymbol{\beta}) \big)}$$

with $p^{(\alpha)}$ and $p^{(\alpha)}_j$ estimated probability density functions without and with taxon labels and $\boldsymbol{\beta}_l$ and $\boldsymbol{\beta}$ the parameters of the response functions. $p^{(\alpha)}$ and $p^{(\alpha)}_j$ are calculated under a certain error distribution (e.g. negative binomial). $LR(\boldsymbol{\alpha})$ actually compares two models. In the restricted model each species reacts in the same way to its environment, relative to its own baseline defined by $u_j$, and all do not depart from independence for a certain combination of environmental variables. In the extended model each species is left to react to the environment in its own way. In case of a linear response function it means that it gets a unique intercept and slope.

###### Full ML

However, the niche concept is not accepted by all ecologists. If niches are really maximally separated, how can species co-occur then?
An alternative option would be to estimate $\alpha$ through full ML. Surprisingly, the solutions of both approaches are very similar, although not exactly identical. The full ML solution is also much faster.

Naive approach

The naive approach to optimize $LR(\boldsymbol{\alpha})$ would be to use constrained optimization (constraining the $\boldsymbol{\alpha}$'s to have variance one etc.) and estimate $p_j^{(\boldsymbol{\alpha})}$ and $p^{(\boldsymbol{\alpha})}$ in every evaluation of this function. This implies a analytical jacobian cannot be estimated and requires a huge amount of re-estimation of the response functions. This naive approach is computationally unfeasible

Iterative approach

A smarter approach (also the one proposed by Zhu and Hastie, 2005) is to iterate between estimating $\boldsymbol{\alpha}$ by optimizing $LR(\boldsymbol{\alpha})$ given $p_j^{(\boldsymbol{\alpha})}$ and $p^{(\boldsymbol{\alpha})}$, and estimating $p_j^{(\boldsymbol{\alpha})}$ and $p^{(\boldsymbol{\alpha})}$ given $\boldsymbol{\alpha}$. This results in much fewer estimations of $p_j^{(\alpha)}$ and $p^{(\alpha)}$ and allows to optimize $LR(\boldsymbol{\alpha})$ by finding the roots of $\frac{\partial LR(\boldsymbol{\alpha})}{\partial \boldsymbol{\alpha}} = \mathbf{0}$ (and analytically specifying the second order derivative).

Convergence is assumed when every single change in $\alpha$ is below a tolerance level (infinity norm), or when the L2-norm of alpha is below a tolerance level.

Estimating $p^{(\alpha)}_k$ non-parametrically in this iterative algorithm will be computationally more demanding.

###### Lagrangian

Taking into account the restrictions mentioned above, the associated Lagrangian becomes

$$ Lag_{LR}(\boldsymbol{\alpha}_k| \boldsymbol{\beta}_{jk}, \boldsymbol{\beta}_{k}) =  \sum_{i=1}^n  \sum_{j=1}^p \Big(log \big(\frac{f_{NB}(x_{ij}|\psi_k, \mathbf{C}_i, \boldsymbol{\alpha}_k, \phi_j, f_j)}{f_{NB}(x_{ij}| \psi_k, \mathbf{C}_i, \boldsymbol{\alpha}_k, \phi_j, f_0)} \big) \Big) + \lambda_{1k} \boldsymbol{\alpha}_k^t\mathbf{M} + \lambda_{2k} (\boldsymbol{\alpha}_k^t \boldsymbol{\alpha}_k - 1) +  \boldsymbol{\alpha}_k^t \boldsymbol{\alpha}_{-k} \boldsymbol{\lambda}_{orth,k}$$

with $\boldsymbol{\alpha}_{-k}$ the $\boldsymbol{\alpha}$-matrix from the first to the (k-1)th column, and $f_{NB}$ the density function of the negative binomial distribution. $\boldsymbol{\lambda}_{orth,k}$ is a diagonal matrix with largangian multipliers for orthogonality on the diagonal.

###### Score equations

####### Linear response function

The first order derivatives becomes (in case of a linear response function, and remember we set $s_{jk} = 1$)

$$\frac{\partial Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k} = \psi_k \Big( \sum_{i=1}^n  \sum_{j=1}^p  \mathbf{C}_i \beta_{1j} \frac{x_{ij} - \mu_{ij}}{1 + \frac{\mu_{ij}}{\theta_j}} - \sum_{i=1}^n  \sum_{j=1}^p \mathbf{C}_i \beta_{1} \frac{x_{ij} - \mu_{ij0}}{1 + \frac{\mu_{ij0}}{\theta_j}} \Big) + \mathbf{M}\boldsymbol{\lambda}_{1k} + 2\lambda_{2k} \boldsymbol{\alpha}_k +  \boldsymbol{\alpha}_{-k}^t \boldsymbol{\lambda}_{orth,k} = \mathbf{0}$$

with $\mu_{ij0}$ the mean under the null model of equal response functions and $\mathbf{M}$ a q-by-(q-$d_cont$+1) centering matrix. M has ones in the first column, the next columns are all zero except for the entries corresponding to the positons of the dummies of this categorical variable in $\boldsymbol{\alpha}_k$, which are 1. As such, M forces the coefficients of each categorical variable to sum to zero.  In theory we should perhaps reestimate $\theta_j$ on every iteration between the estimation of $\boldsymbol{\alpha}_k$ and $\boldsymbol{\beta}_j$ and $\boldsymbol{\beta}$, and also estimate different overdispersion for the models with equal and species-wise response function, but we think it will complicate the estimation and not make a big difference anyway, so we ignore this for now.

####### Quadratic response function

For a quadratic response function ($f_j(z_i = \mathbf{C}_i\boldsymbol{\alpha}|\boldsymbol{\beta}) = \beta_0 + \beta_1 z_i + \beta_2 z_i^2$) this derivative becomes

$$\frac{\partial Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k} = \psi_k \Big( \sum_{i=1}^n  \sum_{j=1}^p\mathbf{C}_i (\beta_{1j}+2 \beta_{2j}\boldsymbol{\alpha}_k^t\mathbf{C}_i) \frac{x_{ij} - \mu_{ij}}{1 + \frac{\mu_{ij}}{\theta_j}} - \sum_{i=1}^n  \sum_{j=1}^p \mathbf{C}_i (\beta_{1}+2 \beta_{2}\boldsymbol{\alpha}_k^t\mathbf{C}_i) \frac{x_{ij} - \mu_{ij0}}{1 + \frac{\mu_{ij0}}{\theta_j}} \Big) + \mathbf{M}\boldsymbol{\lambda}_{1k} + \\ 2\lambda_{2k} \boldsymbol{\alpha}_k + \boldsymbol{\alpha}_{-k}^t \boldsymbol{\lambda}_{orth,k} = \mathbf{0}$$

For a non-parametric density function this derivative is not available, which which greatly increase the optimization time.

The other derivatives are (independent of the shape of the response function):

$$\frac{\partial Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\lambda_{1k}}} = \boldsymbol{\alpha}_k^t \mathbf{M} = \mathbf{0}$$
$$\frac{\partial Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \lambda_{2k}} = \boldsymbol{\alpha}_k^t \boldsymbol{\alpha}_k - 1 = 0$$
$$\frac{\partial Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\lambda_{3k}}} = \boldsymbol{\alpha}_k^t \boldsymbol{\alpha}_{-k} = \mathbf{0}$$

###### Jacobians

####### Linear response function

The second order derivatives are for the linear response function

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k^2} = - {\psi_k}^2 \Big( \sum_{i=1}^n  \sum_{j=1}^p (\mathbf{C}_i \beta_{1j})^2 \frac{\mu_{ij}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij}}{\theta_j})^2} - \sum_{i=1}^n  \sum_{j=1}^p (\mathbf{C}_i \beta_{1})^2 \frac{\mu_{ij0}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij0}}{\theta_j})^2} \Big) + 2\lambda_{2k} $$

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial {\alpha}_{yk}{\alpha}_{y'k}} = - {\psi_k}^2 \Big( \sum_{i=1}^n  \sum_{j=1}^p  \beta_{1j}^2 c_{iy} c_{iy'}\frac{\mu_{ij}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij}}{\theta_j})^2} - \sum_{i=1}^n  \sum_{j=1}^p \beta_{1}^2 c_{iy} c_{iy'} \frac{\mu_{ij0}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij0}}{\theta_j})^2} \Big) $$

####### Quadratic response function

For the quadratic response function these second order derivatives are:

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial {\alpha}_{yk}{\alpha}_{y'k}} = - {\psi_k}^2 \Big[ \sum_{i=1}^n \sum_{j=1}^p \Big((\beta_{1j} + 2\beta_{2j}\boldsymbol{\alpha}_k^t\mathbf{C}_i)(\beta_{1j} + 2\beta_{2j}\boldsymbol{\alpha}_k^t\mathbf{C}_i) c_{iy}^2\frac{\mu_{ij}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij}}{\theta_j})^2}   \\ 
- (\beta_{1} + 2\beta_{2}\boldsymbol{\alpha}_k^t\mathbf{C}_i)(\beta_{1} + 2\beta_{2}\boldsymbol{\alpha}_k^t\mathbf{C}_i) c_{iy}^2\frac{\mu_{ij}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij}}{\theta_j})^2}  \Big) \Big] - 2\psi_k \Big[\sum_{i=1}^n \sum_{j=1}^p c_{iy}^2 \big(\beta_{2j} \frac{x_{ij} - \mu_{ij}}{1 + \frac{\mu_{ij}}{\theta_j}}-\beta_{2} \frac{x_{ij} - \mu_{ij0}}{1 + \frac{\mu_{ij0}}{\theta_j}}\big)\Big] + 2\lambda_{2k} $$
$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial {\alpha}_{yk}{\alpha}_{y'k}} = - {\psi_k}^2 \Big[ \sum_{i=1}^n \sum_{j=1}^p \Big((\beta_{1j} + 2\beta_{2j}\boldsymbol{\alpha}_k^t\mathbf{C}_i)(\beta_{1j} + 2\beta_{2j}\boldsymbol{\alpha}_k^t\mathbf{C}_i) c_{iy} c_{iy'}\frac{\mu_{ij}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij}}{\theta_j})^2}   \\ 
- (\beta_{1} + 2\beta_{2}\boldsymbol{\alpha}_k^t\mathbf{C}_i)(\beta_{1} + 2\beta_{2}\boldsymbol{\alpha}_k^t\mathbf{C}_i) c_{iy} c_{iy'}\frac{\mu_{ij}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij}}{\theta_j})^2}  \Big) \Big] - 2\psi_k \Big[\sum_{i=1}^n \sum_{j=1}^p c_{iy}c_{iy'} \big(\beta_{2j} \frac{x_{ij} - \mu_{ij}}{1 + \frac{\mu_{ij}}{\theta_j}}-\beta_{2} \frac{x_{ij} - \mu_{ij0}}{1 + \frac{\mu_{ij0}}{\theta_j}}\big)\Big] $$

<!---+ \\ 2\beta_{2j} c_{iy} c_{iy'}\frac{x_{ij} - \mu_{ij}}{1 + \frac{\mu_{ij}}{\theta_j}}--->

The other derivatives are again independent of the response function:

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k \partial \boldsymbol{\lambda_{1k}}} =  \mathbf{M}$$

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k \partial \lambda_{2k}} =  2\boldsymbol{\alpha}_k$$

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_{k} \partial \boldsymbol{\lambda}_{orth,k}} =  \boldsymbol{\alpha}_{-k}$$
All other second order derivatives equal zero. 

###### Non-parametric response function

The response function $f(\boldsymbol{\alpha}^t\mathbf{C})$ need not have a parametric form. We can also estimate it non-parametrically as in Zhu and Hastie (2005). Like these authross, we use the locfit library. We regress $\frac{X}{exp(\lambda_i+\lambda_j)}$ on $\boldsymbol{\alpha}^t\mathbf{C}$ with a log-link. We limit the surface between this curve and the x-axis to sum to one as a means of normalization, only the importance parameter $\psi$ can differ in size between the different dimensions.

##### Estimation of the parameters of the parametric response functions

We fit all parameters of the different taxa jointly, and enforce normalization through Lagrange multipliers again.

The derivative of the Lagrangian for $\beta_{jv}$ under the negative binomial model with $v$ a parameter index running along $\boldsymbol{\beta}_j$ (for k=1 and omitting dimension indices)

$$ \sum_{i=1}^n \frac{\partial Lag_{NB}(X_{.j}|\boldsymbol{\beta}_j, \theta_j)}{\partial \beta_{jv}} = 2\beta_{jv}\lambda_{2v} +  \sum_{i=1}^n \psi \frac{\partial f_j \big( \boldsymbol{\alpha}^t \mathbf{C}_{i.}|\boldsymbol{\beta}_j \big)}{\partial \beta_{jv}} \frac{x_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}} = 0$$

The second order derivative for parameters from the same taxon is:

$$\sum_{i=1}^n \frac{\partial^2 Lag_{NB}(X_{.j}|\boldsymbol{\beta}_j, \theta_j)}{\partial \beta_{jv} \partial \beta_{jv'}} = -\psi^2 \sum_{i=1}^n \Big( \frac{\mu_{ij}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij}}{\theta_j})^2} \frac{\partial f_j \big( \boldsymbol{\alpha}^t \mathbf{C}_{i.}|\boldsymbol{\beta}_j \big)}{\partial \beta_{jv}} \frac{\partial f_j \big( \boldsymbol{\alpha}^t \mathbf{C}_{i.}|\boldsymbol{\beta}_j \big)}{\partial \beta_{jv'}} \Big) + 2\lambda_{2v} \mathds{I}(v=v')$$

$$\sum_{i=1}^n \frac{\partial^2 Lag_{NB}(X|\boldsymbol{\beta}_j, \theta_j)}{\partial \beta_{jv} \partial \lambda_{2v'}} = 2\beta_{2v} \mathds{I}(v=v')$$

all other second order derivatives equal zero.

#### Algorithm

The whole estimation of the constrained RC(M)-model would then proceed as follows:

 1. Estimate the independence model ($u_i$'s, and $u_j$'s)
 2. (Optional): Filter out confounders by modifying offset
 3. Find starting values for $\alpha$, based on CCA
 4. Iterate between
  - Estimate $f_j$'s and $f_0$ given $\boldsymbol{\alpha}$
  - Find $\boldsymbol{\alpha}$ by optimizing $LR(\boldsymbol{\alpha})$
 5. Estimate overdispersions using empirical Bayes
 6. Estimate importance parameter $\psi_k$
 7. Repeat steps 4-6 for every dimension $k$ until convergence

Filtering of rare taxa is not needed in this case, as all covariates are summarized in one "continuous" row score. to check for convergencen, for $\psi_k$  we apply an infinity norm, for $\boldsymbol{\alpha}_k$ the default is an L2-norm.

#### Triplot

The interpretation of the triplot is different from the one from Zhu _et al._ (2005), and also different from that of the unconstrained RC(M).

 - The components of $\boldsymbol{\alpha}_k$ are plotted as arrows and represent the extent to which the covariates contribute to environmental score in dimension $k$. Categorical variables are centered, all their levels are shown. The continuous variables have been centered and scaled, so the magnitude of their compponents in $\boldsymbol{\alpha}_k$ can be compared to one another.
 - For the samples, the environmental scores are plotted as points and represent environmental scores of each sample. Samples close together in one dimension come from similar environments, considering that environmental gradient. This does _NOT_ mean that they have similar values of the environmental variables. 
 - The plot for the taxa depends on the response function used. 
    * In case of a linear response function we can plot arrows originating from $-\frac{\beta_{0j}}{\beta_{1j}}$, which represents the point of no departure from independence explained by the covariates. The direction and size of the arrow then depends purely on $\beta_{1j}\psi$. We can make two sorts of projections:
      - The projection of ($-\frac{\beta_{0j}}{\beta_{1j}}$, $-\frac{\beta_{0j}}{\beta_{1j}} + \beta_{1j}\psi$) onto $\boldsymbol{\alpha}^t\mathbf{C}_i$ (the environmental scores), then corresponds to the departure from independence for taxon $j$ in sample $i$ explained by its environmental variables $\mathbf{C}_i$. The arrow is appropriate since a larger environmental score always results in a larger departure from indepedence under this model. As often in statistics, linearity may not be a very credible assumption, but it leads to models that are easy to interpret.
      - The projection of ($-\frac{\beta_{0j}}{\beta_{1j}}$, $-\frac{\beta_{0j}}{\beta_{1j}} + \beta_{1j}\psi$) onto $(0,\alpha_y)$ reflects the sensitivity of taxon $j$ to changes in covariate $y$.
      We cannot make a direct connection between components of the environmental gradient $\boldsymbol{\alpha}$ and the observed environmental scores $\boldsymbol{\alpha}^t\mathbf{C}_i$, even though this may seem so intuitively. Often it is so that a large $\alpha_y$ points in the direction of samples with a large $c_{iy}$, but this is not guaranteed. Many different combinations of covariates $\mathbf{C}_{i}$ can lead to the same environmental score for a given $\boldsymbol{\alpha}$. This is because of the dimension reduction from $d$ to 1 resulting from the projection by $\boldsymbol{\alpha}$. We will have to warn the user for this pitfall, and facilitate the depiction of the environmental variables of each samples through colour codes.
    * In case of quadratic response function we should plot the extremum of the response functions in each dimension as a point (taking the importance parameter $\psi_k$ into account, which is multiplied by each of the parameters $\beta$ of the response functions). If this is a minimum, this represents the value of the environmental score with the largest drop in expected abundance compared to the independence model, in case of a maximum the largest rise. With colour and shape codes we can show which kind of extremum it is, and which kind of departure from indepedence (negative or positive effect on the expectation). To represent the tolerances, i.e. how steep these valleys and mountains of expected abundance are, we can use ellipses to designate e.g. points with 50% the departure from independence from the top. For CCA this feature is not needed since there all tolerances are assumed equal.
    * In case of non-parametric response function I do not see I clear way to plot the species. The benefit of this method will have to be the improved ordination of the environmental gradient and the samples. One can always look at the individual species response functions if one is interested.
  
It is clear that the triplot is not trivial to interpret, but then again: neither is a triplot based on CCA.

## Some general thoughts on the RC(M)

Contrary to most other methods to make biplots for microbiome data, the RC(M)-method is a truly statistical method that explicitly models the mean. This renders it very easy to interpret. It's main weaknesses are the reliance on a parametric distribution (which may be incorrect), and it's relatively long running times. However, many other biplot methods, such as CA ot PCoA with Hellinger distances, also make distributional assumptions, mostly without stating them explicitly.

\newpage

# Implementation

## Correspondence analysis

```{r Auxfuns Correspondence analysis, purl=TRUE}
## A function to perform correspondence analysis
# The unconstrained approach is the approximation to the Pearson's chi-squared, as defined by Gower _et al._ in their book "Understanding Biplots". The constrained approach is the one implemented in vegan, based on a weighted regression of the departure matrix on the environmental scores. We cannot reproduce its results by the code given by ter Braak based on eigenvlaue or singular value decompositions.
caSVD = function(X, Y = NULL, Z = NULL, version = "Pearson"){
# @param X: the nxp count matrix
# @param Y(optional): a nxd matrix of covariates, or a vector of variable names if X is a phyloseq object. If null regular CA is performed
# @param Z(optional): a nxz matrix of variables to condition on, or a vector of variable names if X is a phyloseq object.
  
# @return: the singular value decomposition of the matrix of pearson residuals
  if(class(X)=="phyloseq"){
  if(!is.null(Y)){Y = model.matrix(data = data.frame(sample_data(object = X))[,Y], ~.)}
  if(!is.null(Z)){Z = model.matrix(data = sample_data(object = X)[,Z], ~.)} 
  X = if (taxa_are_rows(X)) t(otu_table(X)@.Data) else otu_table(X)@.Data
  }
  C = colSums(X)
  R = rowSums(X)
  E = outer(R,C)/sum(X) #Expected counts under independence
  if(is.null(Y) & is.null(Z)){ #Unconstrained analysis
  Goal = diag(1/sqrt(R)) %*% (X-E) %*% diag(1/sqrt(C))
  dimnames(Goal) = dimnames(X)
  svd(Goal)
  } else { #Constrained analysis
  vegan:::cca(X = X, Y=Y, Z=Z)
  }
}
```

## Unconstrained RC(M)

### Negative binomial

```{r NBauxFuns, purl=TRUE}
#-------------------------------------------#

#A function to extract the influence for a given parameter index
getInflRow = function(score, InvJac, parIndex){
  score* InvJac[, parIndex]
}
```

#### Test the software

Some preliminary test to make sure everything runs without errors

```{r tests, purl = FALSE, eval = FALSE}
n = 100; p = 500
counts = matrix(rnbinom(n*p, mu = 5, size = 3), n, p)
covars = data.frame(age = rnorm(n, 35,5), gender = sample(c(TRUE, FALSE), n , replace = TRUE), country = factor(sample(c("France","Belgium","Germany"),n, replace=TRUE)), BMI = rnorm(n , 27,6), status = sample(c("Treatment","Control"),n, replace=TRUE), siblings = rpois(n, lambda = 0.9))
physeq = phyloseq(otu_table(counts, taxa_are_rows = FALSE), sample_data(covars))
confounderVec = get_variable(physeq, "country")
d = 30
v = 2
X = matrix(rpois(n*p,2), n, p)
thetaMat = abs( matrix(rnorm(p), n, p, byrow = TRUE))
psi = 3
muMarg = outer(rowSums(X), colSums(X))/sum(X)
NB_params = betas = matrix(rnorm(p*v),v,p)
reg = model.matrix(~rnorm(n))

#Unconstrained one dimension matrix
un1Dmat = RCM(counts, k=1)
# Add two dimensions
un3Dmat = RCM(dat = counts, prevFit = un1Dmat, k=2)

#Unconstrained one dimension phyloseq
un1DmatPhy = RCM(physeq, k=1)
# Add two dimensions (even accepts a matrix!)
un3DmatPhy = RCM(dat = counts, prevFit = un1DmatPhy, k=3)

#Unconstrained one dimension matrix with confounder filtering
un1DmatConf = RCM(counts, k=1, confounders = data.frame(confounderVec))
# Add two dimensions
un3DmatConf = RCM(dat = counts, prevFit = un1DmatConf, k=3)

#Check some requirements
str(un3DmatConf$confParams)
colSums(un3Dmat$rMat^2)
crossprod(un3Dmat$rMat)
colSums(t(un3Dmat$cMat) * un3Dmat$abunds)
rowSums(un3Dmat$cMat^2)
sum(un3Dmat$cMat[1,]* un3Dmat$cMat[2,] * un3Dmat$abunds)
# All is well!

#Unconstrained one dimension physeq with confounder filtering
un1DmatConfPhy = RCM(physeq, k=1, confounders = "country")
# Add two dimensions
un3DmatConfPhy = RCM(dat = counts, prevFit = un1DmatConf, k=3)

#Constrained one dimension matrix without confounder filtering
con1Dmat = RCM(physeq, k=1, covariates =  sample_variables(physeq))
con1DmatML = RCM(dat = physeq, k=1, covariates =  sample_variables(physeq), envGradEst = "ML")
# Add two dimensions
con3Dmat = RCM(dat = physeq, prevFit = con1Dmat, k=3, covariates =  sample_variables(physeq))


con2DmatLin = RCM(physeq, k = 2, covariates =  sample_variables(physeq),  responseFun = "linear")

con2DmatDyn = RCM(physeq, k = 2, covariates =  sample_variables(physeq),  responseFun = "dynamic")
apply(con2DmatDyn$NB_params,3, function(x){mean(x[3,]==0)}) #19 and 16% of the taxa prefer linear response functions
con2DmatDyn$NB_params_noLab

con2DmatQuad = RCM(physeq, k = 2, covariates =  sample_variables(physeq),  responseFun = "quadratic")
# The zeller data?
con2DmatNon = RCM(physeq, k = 2, covariates =  sample_variables(physeq),  responseFun = "nonparametric")

plot.RCM(con2DmatLin, colour = "BMI")
plot.RCM(con2DmatQuad, colour = "BMI")
plot.RCM(con2DmatNon, colour = "BMI")

con2DmatLin$alpha
con2DmatQuad$alpha
con2DmatNon$alpha
```

#### Real data examples

Finally we also apply the method to real datasets. First the AGP and HMP datasets

```{r RC(M)_NB real datasets, purl = FALSE}
load("/home/stijn/PhD/American Gut/AGphylo.RData")
load("/home/stijn/PhD/Simulations/data/physeqListV13.RData")
realNames = c("AGP", "Hard.palate","Buccal.mucosa","Anterior.nares","Left.Antecubital.fossa", "Posterior.fornix", "Mid.vagina", "Left.Retroauricular.crease", "Vaginal.introitus", "Stool", "Hard.palate" )
if(!file.exists(file = "resListRCMagphmp.RData")){
fileNames = paste(realNames, "RCM.RData")#"Kostic", "zeller16S", "zellerMeta",
resListRCM = mapply(fileNames, c(AGphylo, physeqListV13[realNames[-1]]),  SIMPLIFY = FALSE, FUN=function(x,y){
  load(x)
  RCMres$physeq=y
  RCMres
})
names(resListRCM) = realNames
save(resListRCM, file = "resListRCMagphmp.RData")
} else {load(file = "resListRCMagphmp.RData")}
```

##### The HMP dataset: sample plots

```{r HMPplot, purl = FALSE, include = FALSE, fig.cap="Sample plots of the RC(M) ordination of the HMP dataset. Colours indicate gender. \\label{fig:samHMP}"}
SClevels  = unique(c(unlist(sapply(physeqListV13, function(x){
  levels(get_variable("RUNCENTER",physeq=x))
}))))
#Give similar colours for the same sequencing center
cols = c("cadetblue","cyan", "darkblue","blue","magenta","orange","yellow","brown","black",  "grey80","grey70","pink") 
palette(cols)
physeqListV13_SC = lapply(physeqListV13, function(x){
  sample_data(x)[["RUNCENTER"]] = factor(sample_data(x)[["RUNCENTER"]], levels = SClevels, labels = SClevels)
  x
})
phyList = c(AGP = AGphylo, physeqListV13_SC)
resListRCM2 = lapply(realNames, function(x){
  tmp = resListRCM[[x]]
  tmp$physeq = prune_samples(rownames(tmp$X),prune_taxa(colnames(tmp$X), phyList[[x]]))
  tmp
}) 

names(resListRCM2) = realNames

par(mfrow = c(3,4))
lapply(names(resListRCM2)[-1], function(x){plotRCM(resListRCM2[[x]], biplot=FALSE, samColour = "sex", main=x)})
```

```{r HMPsamRuncenter, fig.cap="Sample plots of the RC(M) ordination of the HMP dataset. Colours indicate sequencing center. it is clear that there is a strong effect of sequencing center \\label{fig:samHMP}", purl = FALSE}
par(mfrow = c(3,4))
foo = lapply(names(resListRCM2)[-1], function(x){plotRCM(resListRCM2[[x]], biplot=FALSE, samColour = "RUNCENTER", main=x, libInset = c(1.1,-0.1))})
rm(resListRCM, resListRCM2)
```

\clearpage

##### The HMP dataset: sample plots after conditioning

Now let's filter out the impact of this variable using conditioning

```{r HMP filter out variable effect, purl=FALSE, fig.cap="Sample plots of the RC(M) ordination of the HMP dataset after conditioning. Colours indicate sequencing center. Even when fitted with a rough grouping of sequencing centers, it is clear that we have conditioned out its effect. \\label{fig:samHMP}"}
# antNarJob = mcparallel(  RCM(RCMhmpFilt[["Anterior.nares"]], distribution="NB", k = K, nleqslv.control= nleqslv.control, maxItOut=5e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform", marginEst = "MLE", round=TRUE, confounders = "RUNCENTER2"))
# antNar = mccollect(antNarJob, wait=FALSE)[[1]]
# save(antNar, file="antNarConfounders.RData")

if(!file.exists(file="/home/stijn/PhD/Biplots/filtResHMP.RData")){
  K=2
nleqslv.control = list()
#Group the runcenters together

RCMhmpFilt = lapply(physeqListV13, function(x){
  runcenters = data.frame(sample_data(x))[ "RUNCENTER"]
runcenters[["RUNCENTER"]][runcenters[["RUNCENTER"]] %in% c("BCM","BI,BCM","BCM,BI","BCM,WUGC", "BCM,JCVI")] = "BCM"
runcenters[["RUNCENTER"]][runcenters[["RUNCENTER"]] %in% c("JCVI","JCVI,BI","JCVI,WUGC")] = "JCVI"
runcenters[["RUNCENTER"]][runcenters[["RUNCENTER"]] %in% c("WUGC,JCVI","WUGC","WUGC,BCM")] = "WUGC"
sample_data(x) = data.frame(sample_data(x) ,RUNCENTER2 = runcenters[[1]])
x})
filtResJob = mcparallel(lapply(RCMhmpFilt, function(x){
  RCM(x, distribution="NB", k = K, nleqslv.control= nleqslv.control, maxItOut=5e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform", marginEst = "MLE", round=TRUE, confounders = "RUNCENTER2")
  }))
filtRes = mccollect(filtResJob, wait=FALSE)[[1]]
save(filtRes, file="/home/stijn/PhD/Biplots/filtResHMP.RData")
} else {load(file="/home/stijn/PhD/Biplots/filtResHMP.RData")}
par(mfrow = c(3,3))
foo = lapply(names(filtRes), function(x){plotRCM(filtRes[[x]], biplot=FALSE, samColour = "RUNCENTER", main=x, libInset = c(-1.9,-0.1))})
# lapply(names(filtRes), function(x){plotRCM(filtRes[[x]], biplot=FALSE, samColour = "RUNCENTER2", main=x, libInset = c(-0.9,-0.1))})
par(mfrow = c(1,1))
rm(filtRes, physeqListV13_SC, phyList)
```


```{r filterTwoConfounders, purl = FALSE, eval = FALSE}
#With two confounders
confTestJob = mcparallel(RCM(RCMhmpFilt[["Anterior.nares"]], distribution="NB", k = K, nleqslv.control= nleqslv.control, maxItOut=5e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform", marginEst = "MLE", round=TRUE, confounders = c("sex","RUNCENTER2")))
confTest = mccollect(confTestJob, wait=FALSE)[[1]]
save(confTest, file="/home/stijn/PhD/Biplots/confTestHMP.RData")
```

\clearpage

##### The AGP dataset: sample plots

```{r AGPplots, purl = FALSE, eval = FALSE}
par(mfrow = c(3,3))
AGPvars = c("IBD","SEX","PREGNANT","LACTOSE","ASTHMA","DIABETES", "DIET_TYPE","COUNTRY")
sapply(AGPvars, function(x){
plotRCM(resListRCM[["AGP"]], biplot=FALSE, samColour = x, main=x, libInset = c(-3.9,-0.1), libLeg=TRUE)
})
```

The AGP dataset is presumably too noisy to find much of a signal

```{r fitting process, purl = FALSE, eval = FALSE}
lapply(names(resListRCM), function(x){with(resListRCM[[x]],{ plot(main=x, rowRec[2,1,1:iter[1]])})})
```

##### The Zeller data: sample plots

```{r load zeller RC(M) NB results, purl = FALSE, include = FALSE}
load("/home/stijn/PhD/Simulations/data/zellerData.RData")
zellers = c("zellerMeta","zeller16S")
# zellerRCM  = lapply(paste(zellers, "RCM.RData"), function(x){
#   load(x)
#   RCMres
# })
if(!file.exists("zellerSphyRCM.RData")){
zellerSphyRCM = RCM(zellerSphy, k=2, round = TRUE) # Less than one minute!
save(zellerSphyRCM, file="zellerSphyRCM.RData")
} else {load("zellerSphyRCM.RData")}

if(!file.exists("zellerMphyRCM.RData")){
zellerMphyRCM = RCM(zellerMphy, k=2, round = TRUE, prevCutOff = 0.05, minFraction = 0.15) # Less than one minute! More trimming needed here
save(zellerMphyRCM, file="zellerMphyRCM.RData")
} else {load("zellerMphyRCM.RData")}

zellerRCM  =list(zellerMphyRCM, zellerSphyRCM)
names(zellerRCM) = zellers

sapply(zellerRCM,function(x){x$converged}) # All converged
sapply(zellerRCM,function(x){x$runtime})
sapply(zellerRCM,function(x){x$iter})

cols = c("darkblue","orange","darkgreen", "brown","black", "blue","magenta","cadetblue","cyan", "yellow", "grey80","grey70","pink") 
palette(cols)
```

```{r zellerOldPlots, purl = FALSE, eval = FALSE, echo = FALSE}
par(mfcol = c(2,3))
sapply(c("Country","Diagnosis","Gender"), function(y){
sapply(names(zellerRCM), function(x){plotRCM(zellerRCM[[x]], biplot=FALSE, samColour = y, main=paste(x,y, collapse="_"))})
})
```

For the 16S data we see signals for Country and Cancer diagnosis. For the metagenomics data we do not see a trend. Notice we are doing indirect gradient analysis again, but this is just to look for confirmation that the unconstrained analysis is detecting true groups. 

```{r plotZellerSphy, purl = FALSE, eval = FALSE}
lapply(c("Country","Diagnosis","Gender", "BMI","Age"), function(x){
  plot.RCM(zellerSphyRCM, colour = x)
})
```

```{r plotZellerSphyExec, purl = FALSE}
lapply(c("Country","Diagnosis"), function(x){
  plot.RCM(zellerSphyRCM, samColour = x)
})
rm(zellerRCM, zellerSphyRCM, zellerMphyRCM)
```

```{r zeller country vs diagnosis, purl = FALSE, eval = FALSE}
#Are country and diagnosis related?
with(sample_data(zellerMphyRCM$physeq),table(Country, Diagnosis))
#Yes, only cancer patients from Germany!
```

\clearpage

##### The Zeller data: Constrained RC(M)

```{r zellerConstrained, purl =FALSE}
if(!file.exists("zellerSphyRCMconstr.RData")){
zellerSphyRCMconstrLin = RCM(zellerSphy, k=2, round = TRUE, covariates = c("Age","Gender","BMI","Country", "Diagnosis"), responseFun = "linear") # Use smallest possible cndtol!
zellerSphyRCMconstrLinML = RCM(zellerSphy, k=2, round = TRUE, covariates = c("Age","Gender","BMI","Country", "Diagnosis"), responseFun = "linear", envGradEst = "ML")
zellerSphyRCMconstrQuad = RCM(zellerSphy, k=2, round = TRUE, covariates = c("Age","Gender","BMI","Country", "Diagnosis"), responseFun = "quadratic")
zellerSphyRCMconstrNonParam = RCM(zellerSphy, k=2, round = TRUE, covariates = c("Age","Gender","BMI","Country", "Diagnosis"), responseFun = "nonparametric")
zellerSphyRCMconstrNonParamML = RCM(zellerSphy, k=2, round = TRUE, covariates = c("Age","Gender","BMI","Country", "Diagnosis"), responseFun = "nonparametric", envGradEst = "ML")
save(zellerSphyRCMconstrLin, zellerSphyRCMconstrLinML, zellerSphyRCMconstrQuad, zellerSphyRCMconstrNonParam,file="zellerSphyRCMconstr.RData")
} else {load("zellerSphyRCMconstr.RData")}  #It looks like we have to drop the centering restriction on the alphas. It forces one continuous variable to be 0, two to be each others opposite
```

```{r zeller liks, purl = FALSE, eval= FALSE}
liksZeller = lapply(list(zellerSphyRCM, zellerSphyRCMconstrLin, zellerSphyRCMconstrQuad, zellerSphyRCMconstrNonParam), liks)
names(liksZeller) = c("unconstrained","linear","quadratic","nonparametric")

liksZeller2 = rbind(liksZeller, (liksZeller[c("Dim 1", "Dim 2"),]-liksZeller[c("independence", "Dim 1"),])/matrix(liksZeller["saturated",]-liksZeller["independence",], byrow = TRUE, nrow = 2, ncol = 4))
rownames(liksZeller2)[5:6] = c("Dim 1 rel", "Dim 2 rel")
liksZellerMelt = melt(liksZeller2, varnames = c("Dimension", "responseFun"), value.name = "logLik")
ggplot(aes(x = Dimension, y = logLik, colour =responseFun), data = liksZellerMelt[!grepl("rel",liksZellerMelt$Dimension),]) + geom_point() +geom_line(aes(group = responseFun))
```

The "fraction of likelihood explained" appears to be very small.

```{r ZellerConstrainedBiplot, purl=FALSE, eval = FALSE}
plot.RCM(zellerSphyRCMconstrLin)
plot.RCM(zellerSphyRCMconstrLin, samColour = "Diagnosis")
plot.RCM(zellerSphyRCMconstrLin, samColour = "Gender")
plot.RCM(zellerSphyRCMconstrLin, samColour = "Country")
plot.RCM(zellerSphyRCMconstrLin, samColour = "Age")
#apply(zellerSphyRCMconstr$NB_params^2, c(1,3) ,sum)

plot.RCM(zellerSphyRCMconstrQuad, samColour = "Diagnosis", Palette = rainbow(5))
```

```{r ZellerConstrainedBiplotExec, purl=FALSE, fig.cap="Constrained RC(M) analysis on zeller data with linear response functions. Origins of species arrows indicate points of no departure from independence. \\label{fig:triZellerDiagnosis}"}
plot.RCM(zellerSphyRCMconstrLin, samColour = "Diagnosis", plotTaxa = TRUE)
```

```{r ZellerConstrainedBiplotExec2, purl=FALSE, fig.cap="Constrained RC(M) analysis on zeller data with linear response functions. Origins of species arrows indicate points of no departure from independence. \\label{fig:triZellerAge}"}
plot.RCM(zellerSphyRCMconstrLin, samColour = "Age", plotTaxa = TRUE)
rm(zellerSphyRCMconstrLin)
```

Cancer turns out to be the main driver of explained variability in the first dimension, country in the second

```{r AGPconstr, eval=FALSE}
##### Constrained analysis of the AGP data
AGPconstr = RCM(AGphylo, k=2, round = TRUE, covariates = "all")
```

##### Turnbaugh et al. (2008)

Also try the Turnbaugh (2008) dataset: lean and obese twins

```{r turnTwins, eval = FALSE, purl = FALSE}
if(!file.exists("turntwinsRCm.RData")){
  load(file = "/home/stijn/PhD/Datasets/TurnbaughObeseTwins/TurnTwins.RData")
turntwinsRCm = RCM(TurnTwins, k=3)
#turntwinsRCmConstr = RCM(TurnTwins, k=3, covariates = c("Family_p","Status_p")) #Not enough variables for constrained analysis
save(turntwinsRCm, file = "turntwinsRCm.RData")
} else {load("turntwinsRCm.RData")}
plot.RCM(turntwinsRCm, colour = "Family_p")
plot.RCM(turntwinsRCm, colour = "twin")
plot.RCM(turntwinsRCm, colour = "Status_p")
rm(turntwinsRCm)
```

##### Turnbaugh et al. (2009)

15 gnotobiotic mice were inoculated with human feces, and for one group the diet was switched to a Western diet after one month. Then a second generation of mice was inoculated with cecal samples from the previous groups, and here also diet was varied.

- Diet: current diet
- Generation_p: "Recipient1" if second generation, otherwise diet of cecal samples with which they were inoculated
- Time: age of mouse at sampling
- DGS: Diet and previous diet
- DTG: combines sampling diet, time, and previous generation (if any) 

```{r turnmice, purl = FALSE}
if(!file.exists("turnMiceRCM.RData")){
  load(file = "/home/stijn/PhD/Datasets/Humanized mouse/HumMicePhy.RData")
HumMiceRCM = RCM(HumMicePhy, k=3)
HumMiceRCMlin = RCM(HumMicePhy, k=3, covariates = c("Diet","Generation_p","DTG","DGS"))
HumMiceRCMlinML = RCM(HumMicePhy, k=3, covariates = c("Diet","Generation_p","DTG","DGS"), envGradEst = "ML")
save(HumMiceRCM, HumMiceRCMlin, HumMiceRCMlinML, file = "turnMiceRCM.RData")
} else {load("turnMiceRCM.RData")}
```

```{r humMicePlot, purl = FALSE, eval = FALSE}
plot.RCM(HumMiceRCM) # Two or three distinct groups
plot.RCM(HumMiceRCM, samColour = "Diet") #Diet is a main factor
plot.RCM(HumMiceRCM, samColour = "Individual") #Individuals play less of a role
plot.RCM(HumMiceRCM, samColour = "Generation_p") #Microbiome changes quickly with feed pattern
plot.RCM(HumMiceRCM, samColour = "Time") #Later samples also form a cluster
```

```{r humMicePlotExec, purl = FALSE}
plot.RCM(HumMiceRCM, samColour = "Diet", main =  "Humanized mice") #Diet is a main factor
plot.RCM(HumMiceRCM, samColour = "Time", main =  "Humanized mice") #Later samples also form a cluster
rm(HumMiceRCM)
```

```{r humMicePlot13D, purl = FALSE, eval = FALSE}
plot.RCM(HumMiceRCM, samColour = "Diet", Dim = c(1,3)) #Diet is a main factor
plot.RCM(HumMiceRCM, samColour = "Individual", Dim = c(1,3)) #Individuals play less of a role
plot.RCM(HumMiceRCM, samColour = "Generation_p", Dim = c(1,3)) #Microbiome changes quickly with feed pattern
plot.RCM(HumMiceRCM, samColour = "Time", Dim = c(1,3)) #Later samples also form a cluster
plot.RCM(HumMiceRCM, samColour = "DTG", Dim = c(1,3))
plot.RCM(HumMiceRCM, samColour = "DGS", Dim = c(1,3))
```

##### Kostic colorectal cancer

This is a study on the microbiome of colorectal cancer in humans. Nine tumor patients were matched with 9 healthy patients, samples were taken repeatedly. The researchers find a enrichment of Fusobacteria in the tumor and a depletion of Bacteroidetes and Firmicutes.

```{r load Kostic, purl=FALSE}
if(!file.exists("Kostic.RData")){
filepath = system.file("extdata", "study_1457_split_library_seqs_and_mapping.zip", package="phyloseq")
kostic = microbio_me_qiime(filepath)
save(kostic, file = "Kostic.RData")
} else {load("Kostic.RData")}
```

```{r KosticRCM}, purl = FALSE, eval = FALSE}
if(!file.exists("kosticRCM.RData")){
kosticRCM = RCM(kostic, k = 3)
save(kosticRCM, file = "kosticRCM.RData")
} else {load("kosticRCM.RData")}
plot(kosticRCM)
plot(kosticRCM, samColour = "DIAGNOSIS")
plot(kosticRCM, samColour = "OSH_DIAGNOSIS")
```

##### Dr. armpit data

The dataset contains 16S sequencing data from armpit swabs of volunteers.

```{r ArmPit, purl =  FALSE}
if(!file.exists("armpit.RData")){
load("Armpit/okseldata.rda") 
load("Armpit/div.rda") 
load("Armpit/odor.rda")
db<-data.frame(odor,info,food,washing)
db$subject = db$subject.1
db$subject.2 = db$subject.1 = NULL
load("Armpit/okseldata_20160229.rda") # data in object counts
Counts<-counts[,-903]
rownames(Counts) = rownames(db) = db$subject
OTUa = Counts[,-903]
armpit = phyloseq(otu_table(apply(as.matrix(OTUa), 1:2, as.numeric), taxa_are_rows =FALSE), sample_data(data.frame(db)))
save(armpit, file = "armpit.RData")
} else {load(file = "armpit.RData")}

if(!file.exists(file = "armpitRCM.RData")){
  armpitRCM = RCM(armpit, k=3)
  armpitRCMconstrLin = RCM(armpit, covariates = sample_variables(armpit)[!sample_variables(armpit) %in% c("subject")], k=3, responseFun = "linear")
  armpitRCMconstrQuad = RCM(armpit, covariates = sample_variables(armpit)[!sample_variables(armpit) %in% c("subject")], k=3, responseFun = "quadratic")
  armpitRCMconstrNonParam = RCM(armpit, covariates = sample_variables(armpit)[!sample_variables(armpit) %in% c("subject")], k=3, responseFun = "nonparametric")
  save(armpitRCM, armpitRCMconstrLin, armpitRCMconstrQuad, armpitRCMconstrNonParam, file = "armpitRCM.RData")
} else {load(file = "armpitRCM.RData")}
```

```{r ArmpitPlot1, purl = FALSE}
plot(armpitRCM)
plot(armpitRCM, samColour = "shaving")
plot(armpitRCM, samColour = "odor_panel")
plot(armpitRCM, samColour = "gender")
plot(armpitRCM, samColour = "age")
plot(armpitRCM, samColour = "antimicrobial")
rm(armpitRCM)
```

```{r ArmpitPlot2, purl = FALSE}
plot(armpitRCMconstrLin, plotType = "samples")
plot(armpitRCMconstrLin, plotType = "variables")
plot(armpitRCMconstrLin, plotType = c("samples", "species"), taxNum = 6)
plot(armpitRCMconstrLin, plotType = c("variables", "species"), taxNum = 6)
plot(armpitRCMconstrLin, samColour = "shaving", plotType = "samples")
plot(armpitRCMconstrLin, samColour = "odor_panel", plotType = "samples")
plot(armpitRCMconstrLin, samColour = "gender", plotType = "samples")
rm(armpitRCMconstrLin)
```

Corynebacteria react most strongly to the environmental gradient. Note how different the results of the constrained and unconstrained analyses are.

```{r ArmpitPlotNonParam, purl = FALSE}
plot(armpitRCMconstrNonParam, plotType = "samples")
plot(armpitRCMconstrNonParam, plotType = "variables")
plot(armpitRCMconstrNonParam, samColour = "gender", plotType = "samples")
plot(armpitRCMconstrNonParam, samColour = "shaving")
plot(armpitRCMconstrNonParam, samColour = "odor_panel")
plot(armpitRCMconstrNonParam, samColour = "gender")
plot(armpitRCMconstrNonParam, samColour = "age")
plot(armpitRCMconstrNonParam, samColour = "wash_p_week")
plot(armpitRCMconstrNonParam, samColour = "deo_p_week")
```

Gender is clearly associated with the microbiome composition, but also correlated with other variables such as shaving and deodorant use.

```{r armPitplotNonParamsVaribales, purl = FALSE}
plot(armpitRCMconstrNonParam, plotType = "variables")
rm(armpitRCMconstrNonParam)
```

##### CMET data

This is a longitudinal dataset on growth in a water cooling system. It has the unique property of providing cell concentrations from flow cytometry from the same samples as used for the 16S sequencing. We could use this information to alleviate the compositional effect. This is relatively new in the literature, and no standard way exists of incorporating this information. In general, the cell concentratios and library sizes correlate poorly.

See separate document for extended discussion.

```{r readInCMET, purl = FALSE, eval = FALSE}
if(!file.exists(file = "CMETdata.RData")){
  library(openxlsx)
  samData = read.xlsx("/home/stijn/PhD/Datasets/CMET/Metadata.xlsx")
  rownames(samData) = samData$sample_title
  samData$Reactor.cycle = as.character(samData$Reactor.cycle)
  samData$cellDensity = samData$`Cell.density.(cells/mL)`
  samData$`Cell.density.(cells/mL)` = NULL
  otuTab = read.csv("/home/stijn/PhD/Datasets/CMET/OTU_table_raw.csv", sep = ";")
  rownames(otuTab) = otuTab$Sample_ID
  otuTab$Sample_ID = NULL
  #Select only samples present in samData
  otuTab = otuTab[as.character(samData$sample_title),]
  #Multiply relative abundances by cell counts in samData
  otuTab = round(as.matrix(otuTab)/rowSums(as.matrix(otuTab))*samData$cellDensity)
  taxTab = read.csv("/home/stijn/PhD/Datasets/CMET/tax_table_raw.csv", sep = ";")
  rownames(taxTab) = taxTab$X
  taxTab$X = NULL
  samData = samData[rowSums(otuTab)>0,]
  taxTab = taxTab[ colSums(otuTab)>0,]
  otuTab = otuTab[rowSums(otuTab)>0, colSums(otuTab)>0] #Number of zeroes is not related to rounding
  CMETwater = phyloseq(otu_table(otuTab, taxa_are_rows = FALSE), sample_data(samData), tax_table(as.matrix(taxTab)))
  save(CMETwater, file = "CMETdata.RData")
}else {load(file = "CMETdata.RData")}
```

```{r analysCMET, purl = FALSE}
if(!file.exists(file = "CMETRCM.RData")){
  CMETRCM = RCM(CMETwater, k = 3, prevCutOff = 0.05)
  envVarsCMET = c("ph","temp","Conductivity","cellDensity", "Reactor.phase", "Reactor.cycle")
  CMETRCMlin = RCM(CMETwater, k = 3, responseFun = "linear", covariates = envVarsCMET, prevCutOff = 0.05)
  CMETRCMlinML = RCM(CMETwater, k = 3, responseFun = "linear", covariates = envVarsCMET, prevCutOff = 0.05, envGradEst = "ML")
  CMETRCMquad = RCM(CMETwater, k = 3, responseFun = "quadratic", covariates = envVarsCMET, prevCutOff = 0.05)
  CMETRCMnonParam= RCM(CMETwater, k = 3, responseFun = "nonparametric", covariates = envVarsCMET, prevCutOff = 0.05)
  save(CMETRCM, CMETRCMlin, CMETRCMquad,CMETRCMnonParam, file = "CMETRCM.RData")
} else {load(file = "CMETRCM.RData")}
```

No convergence for quadratic case. That one seems hard to fit.

```{r CMETliks, purl = FALSE, eval = FALSE}
liksCMET = sapply(list("unconstrained" = CMETRCM, "linear" = CMETRCMlin,"quadratic" = CMETRCMquad, "nonParametric" = CMETRCMnonParam), liks)
#The comparison is complicated here because we trimmed the linear case.
liksCMERmelt = melt(liksCMET, varnames = c("Dimension","responseFun"),value.name = "logLik")
ggplot(aes(x = Dimension, y = logLik, colour =responseFun), data = liksCMERmelt) + geom_point() +geom_line(aes(group = responseFun))
```

```{r plotCMET, purl = FALSE}
plot(CMETRCM)
plot(CMETRCM, samColour = "ph")
plot(CMETRCM, samColour = "temp")
plot(CMETRCM, samColour = "Conductivity")
plot(CMETRCM, samColour = "cellDensity")
plot(CMETRCM, samColour = "Reactor.phase")
plot(CMETRCM, samColour = "Timepoint")
plot(CMETRCM, samColour = "collection_date")
plot(CMETRCM, samColour = "Reactor.cycle")
```

We clearly have two groups in the second dimension, partly explained by the _Reactor.phase_ and _collection.date_ variables. Still feels as if we're missing a confounder here. In the first dimension we have a strong time signal.

```{r plotCMETlin, purl = FALSE}
plot(CMETRCMlin, plotTaxa = FALSE, Dim = c(2,1))
plot(CMETRCMlin, plotTaxa = FALSE, square = FALSE, Dim = c(2,1))
plot(CMETRCMlin, samColour = "ph", plotTaxa = FALSE, Dim = c(2,1))
plot(CMETRCMlin, samColour = "temp", plotTaxa = FALSE, Dim = c(2,1))
plot(CMETRCMlin, samColour = "Conductivity", plotTaxa = FALSE,  Dim = c(2,1))
plot(CMETRCMlin, samColour = "cellDensity", plotTaxa = FALSE,  Dim = c(2,1))
plot(CMETRCMlin, samColour = "Reactor.phase", plotTaxa = FALSE, Dim = c(2,1))
plot(CMETRCMlin, samColour = "Timepoint", plotTaxa = FALSE,  Dim = c(2,1))
plot(CMETRCMlin, samColour = "collection_date", plotTaxa = FALSE, Dim = c(2,1))
plot(CMETRCMlin, samColour = "Reactor.cycle", plotTaxa = FALSE, Dim = c(2,1))
```

Reactor phase clearly explains most of the variation in the data

```{r plotCMETquad, purl = FALSE}
plot(CMETRCMquad, plotTaxa = FALSE, Dim = c(2,1))
plot(CMETRCMquad, plotTaxa = FALSE, square = FALSE, Dim = c(2,1))
plot(CMETRCMquad, samColour = "ph", plotTaxa = FALSE, Dim = c(2,1))
plot(CMETRCMquad, samColour = "temp", plotTaxa = FALSE, Dim = c(2,1))
plot(CMETRCMquad, samColour = "Conductivity", plotTaxa = FALSE,  Dim = c(2,1))
plot(CMETRCMquad, samColour = "cellDensity", plotTaxa = FALSE,  Dim = c(2,1))
plot(CMETRCMquad, samColour = "Reactor.phase", plotTaxa = FALSE, Dim = c(2,1))
plot(CMETRCMquad, samColour = "Timepoint", plotTaxa = FALSE,  Dim = c(2,1))
plot(CMETRCMquad, samColour = "collection_date", plotTaxa = FALSE, Dim = c(2,1))
plot(CMETRCMquad, samColour = "Reactor.cycle", plotTaxa = FALSE, Dim = c(2,1))
```

The control phase seems to be very different from the others, also conductivity plays a role. Also the second dimension is the most important. Should we reorder the dimensions? Also the quadratic graph differs quite a lot from the linear one.

# Competing methods

## Unconstrained RC(M)

### PCoA/MDS

Principal coordinates analysis (PCoA) or multidimensional scaling (MDS) are dimension reduction techniques specifically for samples. The consist in calculating dissimilarity matrices with dissimilarities between samples based on some chosen dissimilarity measure. These dissimilarity matrices are than decomposed through eigenvalue decomposition, and the first two dimensions are plotted. More formally, for a n-by-p data matrix $\mathbf{X}$, all pairwise distances between samples $i$ and $i'$ $D_{ii'} = d(\mathbf{X_{i.}}, \mathbf{X_{i'.}})$ are calculated and summarized in a n-by-n dissimilarity matrix $\mathbf{D}$. If the dissimilarity measure is a true distance, then $d_{ii'} = d_{i'i}$ and $\mathbf{D}$ is symmetric. Next $\mathbf{D}$ is squared and undergoes double centering:

$$B = -1/2 J D^2 J$$

with $\mathbf{J} = \mathbb{I} - \frac{1}{n}\mathbf{11'}$ a centering matrix. This transformed matrix __B__ then undergoes eigendecomposition into

$$ \mathbf{B} = \mathbf{V}^t\boldsymbol{\Lambda}\mathbf{V} $$
  with the eigenvectors of $\mathbf{D}$ in the columns of $\mathbf{V}$ and $\boldsymbol{\Lambda}$ a diagonal matrix with eigenvalues of $\mathbf{D}$ on the diagonal. $\mathbf{v}_1\lambda_1$ can then be plotted _vs_ $\mathbf{v}_2\lambda_2$ in a 2D scatterplot to represent distances between the samples optimally in 2D. In the calculation of $\mathbf{D}$, the information on the individual taxa is lost. Taxa scores can be calculated by weighted averaging of the sample scores for each taxon, weighted by the taxon abundances in these samples. The taxon score of taxon _j_ in dimension 1 then becomes
   
   $$t_{j1} = \sum_{i=1}^n v_{i1}*\frac{x_{ij}}{x_{.j}}$$
   
  This is a two step approach: the taxon scores are only calculated post-hoc and not as part of the eigendecomposition.
   
  In the following paragraphs we discuss a number of popular distance measures for microbiome data that are commonly used for PCoA.
   
#### Bray-Curtis dissimilarity

This distance measure was invented in 1957 in the context of forest ecology. It is calculated as

$$BC_{12} = 1-\frac{2*S_{12}}{S_1 + S_2}$$

with $S_{12}$ the counts or relative abundances of all species observed in \textbf{both samples}, and $S_{i}$ the counts or relative abundances of species only observed in sample $i$. This dissimilarity measure varies between 1 when no species are shared (maximal dissimilarity), and 0 when all species shared, and the community composition is identical. It is not a distance measure since it does not fulfill the triangle inequality $d(a,b) \leq d(a,c) + d(b,c)$. For use in microbiome research see e.g. Barfod _et al._, 2017 and many others. You can apply it to eihter absolute or relative counts, although the latter appears to be the most popular. In Callahan _et al._ (2016) they also apply it to log-transformed counts, even though they note that this leads to correlations with the library sizes.

#### UniFrac distance

The UniFrac distance was developed specifically for the microbiome by Lozupone and coworkers (2005), and includes phylogenetic information in the distance calculation. It is defined as the fraction of the phylogenetic tree length that leads to species found in only one of the two samples. As opposed to Bray-Curtis dissimilarity, it is a true distance metric.

##### Weighted UniFrac

To account for differences in sequencing depth, in 2007 weighted UniFrac was proposed. This weights every branch by the absolute difference in relative abundances the species of both samples as follows:

$$d_{UniFrac}(1,2) = \sum_{i=1}^b l_i |\frac{x_{1b}}{x_{1.}} - \frac{x_{2b}}{x_{2.}}|$$
with $x_{zb}$ the total abundance of species in sample _z_ pertaining to branch _b_. This means that also branches with species from both samples can contribute to the distance between the samples, if their compositions are uneven.

#### Jenssen-Shannon distance
   
We include the Jenssen-Shannon distance because of its role in the enterotypes publication (Arumugam _et al._, 2011). It is the square root of the Jenssen-Shannon _divergence_, a symmetrized version of the Kullback-Leibler divergence.

$$d_{Jenssen-Shannon}(a,b) = \sqrt{0.5 \sum_{j=1}^p x_{aj} log \frac{x_{aj}}{\frac{x_{aj}+x_{bj}}{2}} + 0.5 \sum_{j=1}^p x_{bj} log \frac{x_{bj}}{\frac{x_{aj}+x_{bj}}{2}}}$$

It is a true distance.

#### Hellinger distance

This came up during our own research into a version of correspondence analysis that would accurately model the variance of our count data. Still, it was originally designed for distances between distributions rather than between real data ratios.

The Hellinger distance divides by the square root of the observation as means of normalization:

$$\frac{X}{\sqrt{Var(X)}} \approx \frac{X}{\sqrt{X}} = \sqrt{X}$$

The rationale is that each observation x (rather than the margins of the table) is the best estimate of its expectation E(X), and under the Poisson model also of its variance. Still also these plots are susceptible to differences in library sizes. See Rao (1997), who suggests this method as an alternative to correspondence analysis. The drawback of correspondence analysis is that the estimate for the expected abundances used to standardize departures from independence $X-E$ is based on the taxon sums, and thus on other samples. With the Hellinger distance only, the sample itself is used to estimate the expected abundances, namely by the count itself. It is defined as 

$$d_{hellinger}^2(a,b) = \sum_{j=1}^p \Big(\sqrt{\frac{x_{aj}}{x_{a.}}}-\sqrt{\frac{x_{bj}}{x_{b.}}}\Big)^2 $$

. It does works on the relative abundances. It has been used in publications on microbiome, e.g. Kreisinger _et al._, 2015, Hollister _et al._ 2015, Vital _et al._, 2015.

For a biplot we find the singular value decomposition

$$R^{1/2}(P-Q) = STV^t$$

with $p_{ij} = \sqrt{\frac{x_{ij}}{x_{i.}}}$ and $q_{ij} = \sqrt{\frac{x_{.j}}{x_{..}}}$. To represent the samples we plot the first $k$ columns of $TR^{1/2}S$, and to add the taxa we can add the first $k$ columns of $\Delta^{-1}V$ with $\delta_{ij} = \Big(\sqrt{(P-Q)^tR(P-Q)}\Big)_{ij}$ if $i=j$ and 0 otherwise.

#### Transformations

##### Log transform

In Callahan _et al._ (2016), which serves as a manual for the popular _phyloseq_ package, it is suggested to first log-transform the counts(after adding pseudocount 1) as a variance stabilizing transformation, prior to applying methods like PCoA

##### Variance stabilizng transformation

The _DESeq_ packages provides a variance stabilizing transformation (VST), based on the estimated mean-variance trend. The point is to get transformed values whose variance is independent of the mean.

##### Regularized log-transform

Apart from the VST, the _DESeq2_ packages also offers the _rlog_ transformation. It results in a $log_2$ transformed expected value of the count, based on an intercept and a sample-wise coefficient that is shrunk based on the mean variance trend. This transformation should account for differences in library sizes. See _DESeq2_ manual.

#### Related material

   * See Warton _et al._ (2012): Distance-based multivariate analyses confound
location and dispersion effects
   * See technical report bio BioRankings: "Dealing with High-Dimensional Data". Pairwise distances all become similar in high dimensions.
   
#### Non-metric multidimensional scaling

Non-metric multidimensional scaling (NMDS) can be considered the little brother of MDS, since it also represents distance matrices in lower dimensions. Still there are some important differences: 

 - NMDS is fitted numerically instead of through singular value decomposition
 - The number of axes is chosen beforehand. There is no ordering in the axes, the solution depends on the number of axes chosen
 - It makes no assumption on the distribution of the data, in contrast to PCA which assumes linear relationships and CCA which assumes unimodal ones.
 
 The ordination is fitted iteratively, by minimizing the stress between the original distances and the distances of the actual ordination. See the tutorial by Steven M. Holland for more information.
 
### Double principal coordinates analysis

Double principal coordinates analysis (DPCoA) intends to include dissimilarities between species into the analysis. These could be morphological differences, but in the microbiome case these will be phylogentic distances. 

We have given a n-by-p data matrix __X__ and a p-by-p symmetric dissimilarity matrix between the species $\Sigma$.

1) Use PCoA to find the optimal _n-1_th dimensional representation of the the dissimilarity matrix $\Sigma$
2) Add the samples points at the barycentres (weighted averages) of the species points. In this representations Euclidean distances between samples equal the square roots of their Rao dissimilarity.
3) Use PCA to find a lower dimensional representation of the samples.

It is similar to Unifrac in the sense that it uses phylogenetic information. It is also reminiscent of PCoA, only that here distances between _species_ rather than samples are used. Also the dimension reduction is more complex.

### PCA

Principal components analysis (PCA) is perhaps the most widely used of all dimension reduction methods. Out os a number of possibly correlated continuous variables, it creates linear combinations of these variables that are orthogonal to each other, and for which each linear combinations has maximal variance, conditional on the previous ones.

PCA is performed as an eigenvalue decomposition of the variance covariance matrix

$$Y^tY = W \Lambda W^t$$

with $W^TW = I$. The variables (columns) of __Y__ are presumed to be centered and scaled. This dimensionality reduction optimally represents Euclidean distances between samples in lower dimensions. Its use with dummies of categorical variables is problematic, since they are inherently correlated and affect the ordination of the PCA. This analysis seems usefull for explorative visualization of continuous covariate data, rather than counts.

#### PCA on ranks

In the phyloseq manual it is proposed to perform PCA on ranked abundance data. Thereby low ranks are set equal to 1 to avoid creating artificially large differences between lowly abundant taxa.

### Correspondence analysis

This is the classical, statistical way of analysing contingency tables of count data. For discussion see the beginning of this document.

### CoDa

In recent years there have been many publications on analysing microbiome data as compositional. Gloor and Reid (2016) and Gloor _et al._ propose a method to extend this philosophy to biplots. In a frist step, zero counts are replaced by a non-zero pseudocount, either 0.5 or through a Bayesian method. Next the data are normalized to realtive abundances, divided by the geometric mean and log-transformed. This is called the central logratio transform (clr). PCA is applied to these transformed data (through eigendecomposition of the variance-covariance matrix) and sample scores mutliplied by eigenvalues are plotted. Row scores can be added as weighted sample means.

Theoretical concerns I have about this method are the following:

 - Heteroscedasticity of count data is not addressed. Authors claim this should be handled "downstream".
 - Negative correlations due to compositionality might be swamped by true biological correlations
 - The library sizes are not always of merely technical nature, and very often measures of biomass or cell count are including. This questions the compositional nature of these data

### Bayesian Nonparametric Ordination 

(Ren _et al._ (2016))
 
```{r CoDA}
# The compositional approach:
 # Zeroes are fixed somehow
 # log-ratio transform of relative abundances
 # PCA on transformed values
# Main concern is that this approach ignores the fact that we have count data that are heteroscedastic

# This function is build after the supplementary files of Gloor and Reid, 2016. Only the abundance filtering is made less severe
CoDa = function(dat, abCutOff = 1e-6, scale = FALSE){
  require(zCompositions)
  classDat = class(dat) #The class provided
  
  ##The count data##
  if (classDat=="matrix"){
  X = dat
  }else  if(classDat=="phyloseq"){
  X = if (taxa_are_rows(dat)) t(otu_table(dat)@.Data) else otu_table(dat)@.Data
  } else {stop("Please provide a matrix or a phyloseq object! \n")}
  
Xzero <- as.matrix(cmultRepl(X = X, method="CZM", output="counts")) #Correct zeroes in a Bayesian way

# convert to proportions 
Xnorm = Xzero/rowSums(Xzero)

#Trim on abundance in any sample
Xtrim = Xnorm[,apply(Xnorm,2,min)>abCutOff]

# make our compositional dataset
Xcrl <- t(apply(Xtrim, 1, function(x){log(x) - mean(log(x))}))

#Column center
Xsca = scale(Xcrl, center = TRUE, scale = FALSE) #Shouldn't this be scaled? It is not the default, and was not done by Gloor and Reid

# Apply PCA
Svd = svd(Xsca)
rowScores = Svd$u %*% diag(Svd$d) #=Xsca %*% Svd$v #%*% diag(Svd$d)
colScores = Svd$v #scale = 0 in biplot
rownames(rowScores) = rownames(Xsca)
rownames(colScores) = colnames(Xsca)

# rowScores2 = Svd$u
# colScores2 = Svd$v %*% diag(Svd$d) #scale = 1 in biplot

list(rowScores = rowScores, colScores = colScores)
}
```

```{r nonparametricBayesian}
# A bayesian approach, this is quite slow
#data(JRphylo) # A vaginal dataset
nonParamBayes = function(dat, hyper = list( nv = 3, a.er = 1, b.er = 0.3, a1 = 3, a2 = 4, m = 10, alpha = 10, beta = 0 )){
  require(DirFactor)
  classDat = class(dat) #The class provided
  ##The count data##
  if (classDat=="matrix"){
  X = dat
  }else  if(classDat=="phyloseq"){
  X = if (taxa_are_rows(dat)) t(otu_table(dat)@.Data) else otu_table(dat)@.Data
  } else {stop("Please provide a matrix or a phyloseq object! \n")}
  DirFactor(t(X), hyper, step = 100)
}
```
   
# Method evaluation

After implementing the method and proving its concept, it is time to compare it with existing methodologies. We need to define a way to obtain datasets, and a way to evaluate the ordinations in a high-throughput way. For the simulation we will need multiple repetitions to prove the superiority of our method, so we need an automatic evaluation of the clustering accuracy.

## overall vs. within distance

A first intuitive approach would be to compare the within cluster distances on the plot to the distances to the origin. Clusters are defined as samples generated with the same taxon distribution, either parametrically or non-parametrically. Say we have $m = 1,...,l$ groups with (multidimensional) means $\mathbf{M}_m$.  The within distance is then defined as $d_{m, within} = \frac{1}{n_m}\sum_{i=1}^{n_m} d_{euclidean}(\mathbf{M}_m, Y_i)$, with $d_{euclidean}$ the euclidean distance. The overall distance of a group then equals $d_{m, overall} = \frac{1}{n_m}\sum_{i=1}^{n_m} d_{euclidean}(\mathbf{0}, Y_i)$ with $\mathbf{0}$ the origin. This calculation can be done for each dimension separately or in the multidimensional space.

We could perhaps look for the distance based method that maximizes $\frac{d_{m, overall}}{d_{m, within}}$? This is related to Ward's minimum variance method, but with scaling to the overall distance. This takes differences in the scales of solutions of different methods into account.

This approach can only be applied to the row scores (samples), since there the importance parameters are multiplied and thus distances are represented correctly.

```{r within vs. between distances}
# A function to calculate these within and overall distances
distanceFun = function(coord, clusters, Dim = c(1,2)){
  # @param coord: an nxk matrix of coordinates
  # @param clusters: a factor of length n with cluster memberships
  # @param Dim: The dimensions to look at, defaults to the first two
  
  # @return: a list with
    # -withinDist: within distances
    # -overalDist: the n overal distances
    # -ratio: 
  
  coord = coord[,Dim, drop=FALSE]
  
  withinDist = tapply(seq_len(nrow(coord)), clusters, function(x){
    coordsX = coord[x,]
    centerX = colMeans(coordsX)
    dists = apply(coordsX, 1, function(y){
      dist(rbind(y, centerX))
    })
    return(dists)
  })
  
  overalCenter = colMeans(coord)
  overalDist = apply(coord, 1, function(y){
    dist(rbind(y, overalCenter))
  })
  
  ratio = mean(unlist(sapply(unique(clusters), function(x){overalDist[clusters==x]/withinDist[[x]]})))
  list(withinDist = withinDist, overalDist =  overalDist, ratio = ratio)
  }
```

## Silhouette

See "Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis", Rousseeuw, 1987. The silhouette can be calculated with any distance measure, but we will just use the euclidean distance again. For each point $i$, calculate the distance to each other point in the ordination, and average these distances within the cluster. Call $a(i)$ the average distance to its own cluster and $b(i)$ the smallest of the average distances to the other cluster. The the silhouette of observation i $s(i)$ is defined as:

$$s(i) = \frac{b(i) - a(i)}{max\big(a(i), b(i)\big)}$$

The silhouette can take values between +1 for optimal separation and -1 for wrong classification. If a poin i close to the centroid of its own cluster but very far from all the others the methods has performed well and is rewarded with a high silhouette. We can average the silhouttes over the ordination or jus look at their distribution to compare the ordination methods.

```{r silhouette}
silhouette = function(coord, clusters, Dim = c(1,2)){
  # @param coord: an nxk matrix of coordinates
  # @param clusters: a factor of length n with cluster memberships
  # @param Dim: The dimensions to look at, defaults to the first two
 
  # @return a vector of n silhouettes
  
  coord = coord[,Dim, drop=FALSE]
  
  distMat = as.matrix(dist(coord)) # all distances
  avDists = apply(distMat, 1 , function(x){
    tapply(x[x!=0], clusters[x!=0], mean) #Exclude observation itself
  }) # Average distances within clusters
  ais = sapply(seq_along(clusters), function(i){
    avDists[clusters[i], i]
  }) #Average distance to own cluster
  bis = sapply(seq_along(clusters), function(i){
    min(avDists[clusters[i]!=levels(clusters), i])
  }) #Average distance to nearest cluster
  
  silhouette = mapply(ais, bis, FUN = function(a,b){(b-a)/max(a,b)}) #The silhouette
  
  return(silhouette)
}
```

## Correlation with library sizes and abundances

The motivating problem to develop the whole method was to find an approach that would not show correlation between the row scores and the library sizes, which is a technical artefact.

Pearson correlations of row scores with library sizes could be another criterion to evaluate the quality of the biplot. Equivalently correlations with average relative abundance can be investigated.

```{r libSizeCorrelation}
libCor = function(coord, margins, Dim = c(1,2)){
  if(length(margins)!=nrow(coord)) {return(rep(NA, length(Dim)))}
  apply(coord[, Dim, drop=FALSE], 2, cor, margins)
}
```

<!---
The between distance between groups m and m' is then defined as $d_{mm'} = d_{euclidean}(\mathbf{M}_m, \mathbf{M}_{m'})$ with $d_{euclidean}$ the euclidean distance.
--->

## Contribution of taxa to the separation of the clusters

For the methods that do yield taxon scores, we can also verify if the correct taxa contribute to the separation of the clusters. A useful measure in this case seems the relative contribution of the taxa with signal in the direction of the cluster centroid. It is calculated as the ratio of the average contribution in the direction of the centroid of taxa with signal to all the average contribution of all other taxa. With $\mathbf{H}_l$ the vector from the origin to the centroid of group $l$, $\mathbf{s}_{l,sig}$ the $n_{l,sig}$-by-k matrix with scores of the $n_{l,sig}$ taxa in group l with signal and $\mathbf{s}_{l,noSig}$ the $n_{l,noSig}$-by-k matrix with scores of the $n_{l,noSig}$ taxa without signal, the quantity we are looking for is:

$$ \frac{n_{l,noSig} \mathbf{s}_{l,sig}\mathbf{H}_l \mathbb{1}}{n_{l,sig} \mathbf{s}_{l,noSig}\mathbf{H}_l \mathbb{1}}$$
with $\mathbb{1}$ a column matrix of one of the appropriate dimension. However, this will very often lead to negative and/or very large ratios. Therefore we look at the ratio of the signal taxa to the average signal of the 50\% non-signal taxa with highest signal. Because the mean is very sensitive to outliers, we also include the median, whcih makes our method come out as best. it is very important to mention this in a publication and show both results!

```{r contrTaxa}
contrTaxa = function(rowMatPsi, colMat, groupFactor, idTax, sigFrac = 0.5, Dim = 1:3, upDown = FALSE, centerFun = median){
  #FIX ME! Hellinger!
  rowMatPsi = rowMatPsi[,Dim]
  colMat = t(colMat)[Dim,]
  if(upDown){
  idTaxNum = matrix(-1,nrow(idTax) , ncol(idTax))
  idTaxNum[idTax=="up"] = 1
  }
  centroids = matrix(unlist(tapply(seq_len(nrow(rowMatPsi)), groupFactor, function(i){
    colMeans(rowMatPsi[i,])
  })), nrow = ncol(rowMatPsi)) #The group centroids
  colnames(centroids) = levels(groupFactor)
  overalCenterSam = colMeans(rowMatPsi) # The overall sample centroid(not always c(0,0,0)!!)
  overalCenterTax = colMeans(colMat)
  contrib = sapply(seq_along(levels(groupFactor)), function(i){
        crossprod(colMat, centroids[, i])
  }) # The inner product of all species' arrows with the vectors locating the centroids
  sigContr = if(upDown) {sapply(seq_along(levels(groupFactor)), function(i){
    centerFun(contrib[match(rownames(idTax[[i]]), colnames(colMat), nomatch = 0),i]*idTaxNum[,i])
  })} else {
    sapply(seq_along(levels(groupFactor)), function(i){
    centerFun(contrib[match(idTax[[i]], colnames(colMat), nomatch = 0),i])
  })
  } # The mean/median contribution of the taxa with signal
  allContr = if(upDown) {sapply(seq_along(levels(groupFactor)), function(i){
    centerFun(
      apply(contrib[!colnames(colMat) %in% rownames(idTax[[i]]),],2,sort, decreasing = TRUE)[seq_len(round(sigFrac*(ncol(colMat)-length(idTax[[i]])))), ])
  })} else {
    sapply(seq_along(levels(groupFactor)), function(i){
    centerFun( apply(contrib[!colnames(colMat) %in% idTax[[i]],],2,sort, decreasing = TRUE)[seq_len(round(sigFrac*(ncol(colMat)-length(idTax[[i]])))), ])
  })
  } # The mean/median contribution of the 50% taxa with highest contribution among non-significant taxa
  ratios = sigContr/allContr # The ratio of these mean contributions
  mean(ratios) # Mean ratio
}
```

## Auxiliary functions

```{r extract row vectors}
# A function to perform all analyses except for RC(M)
otherAnalyses = function(RCMlist, unifrac = FALSE, treeList = NULL, cores = 1){
    if(unifrac) {
      RCMlist = mapply(SIMPLIFY = FALSE,RCMlist, treeList, FUN = function(x,tree){
        x$physeq = phyloseq(otu_table(x$X, taxa_are_rows = FALSE), tree)
        sample_names(x$physeq) = seq_len(nsamples(x$physeq))
        x
      })
  }
mclapply(mc.cores = cores, RCMlist, function(x){
  tmp = x$X; rownames(tmp) = seq_len(nrow(tmp))
  R = rowSums(tmp)
  E = outer(R, colSums(tmp)/sum(tmp))
  otuTab = otu_table(tmp, taxa_are_rows = FALSE)
  otuTabRel = otu_table(tmp/R, taxa_are_rows = FALSE)
  otuTabLog = otu_table(log(tmp+1), taxa_are_rows = FALSE)
  list(
    RCM = x, CA = caSVD(tmp), DCA = ordinate(otuTab, method = "DCA"), CoDa = CoDa(tmp),
    BC = ordinate(otuTab, method = "PCoA", distance = "bray"),
    BClog = ordinate(otuTabLog, method = "PCoA", distance = "bray"),
    JSD = ordinate(otuTab, method = "PCoA", distance = "jsd"),
    BCrel = ordinate(otuTabRel, method = "PCoA", distance = "bray"),
    BCrelNMDS = ordinate(otuTabRel, method = "NMDS", distance = "bray", k=3),
    Hellinger = svd(diag(1/sqrt(R)) %*% (sqrt(tmp)-sqrt(E))),
    UniFrac = if(unifrac) ordinate(x$physeq, method = "PCoA", distance = "unifrac") else NULL, 
    wUniFrac = if(unifrac) ordinate(x$physeq, method = "PCoA", distance = "wunifrac") else NULL,
    DPCoA = if(unifrac) DPCoA(x$physeq) else NULL
    )
})
} #JSD works on relative abundances by default
#---------------------------------------#
#A function to extract the row vectors multiplied by importance elements
extractRows = function(x){
tmpList = with(x, list(
    RCM = RCM$rMat %*% diag(RCM$psis),
    CApearson = CA$u %*% diag(CA$d),
    CAcontRat = diag(1/sqrt(rowSums(RCM$X))) %*% CA$u %*% diag(CA$d),
    CAchisq  = diag(1/sqrt(rowSums(RCM$X))) %*% CA$u %*% diag(CA$d),
    DCA = DCA$rproj,
    CoDa = CoDa$rowScores,
    BC = BC$vectors %*% diag(BC$values$Eigenvalues[seq_len(ncol(BC$vectors))]),
    JSD = JSD$vectors %*% diag(JSD$values$Eigenvalues[seq_len(ncol(JSD$vectors))]),
    BCrel = BCrel$vectors %*% diag(BCrel$values$Eigenvalues[seq_len(ncol(BCrel$vectors))]),
    BCrelNMDS = BCrelNMDS$points,
    UniFrac = if(is.null(UniFrac)) NULL else UniFrac$vectors %*% diag(UniFrac$values$Eigenvalues[seq_len(ncol(UniFrac$vectors))]),
    wUniFrac = if(is.null(wUniFrac)) NULL else wUniFrac$vectors %*% diag(wUniFrac$values$Eigenvalues[seq_len(ncol(wUniFrac$vectors))]),
    DPCoA = if(is.null(DPCoA)) NULL else DPCoA$li,
    Hellinger = diag(sqrt(1/rowSums(RCM$X))) %*% Hellinger$u %*% diag(Hellinger$d)
    ))
tmpList = tmpList[!sapply(tmpList, is.null)]
lapply(tmpList, function(y){
  colnames(y) = NULL; y
})
}
#---------------------------------------#
#A function to extract the column vectors. For PCoA and CoDa this uses weighted averaging
extractCols = function(x, row){
  tmpList = with(x, list(
    RCM = t(RCM$cMat),
    CApearson = CA$v,
    CAchisq = CA$v,
    CAcontRat = diag(1/sqrt(colSums(RCM$X))) %*% CA$v,
    DCA = DCA$cproj,
    CoDa = CoDa$colScores,
    BC = weightedTaxonScores(RCM$X, row$BC),
    JSD = weightedTaxonScores(RCM$X, row$JSD),
    BCrel = weightedTaxonScores(RCM$X, row$BCrel),
    UniFrac = if(!is.null(UniFrac))  {weightedTaxonScores(RCM$X, row$UniFrac)} ,
    wUniFrac = if(!is.null(wUniFrac)) {weightedTaxonScores(RCM$X, row$wUniFrac)} ,
    DPCoA = if(is.null(DPCoA)) NULL else DPCoA$dls,
    BCrelNMDS = BCrelNMDS$species,
    Hellinger = diag(1/diag((crossprod(sqrt(RCM$X/rowSums(RCM$X))-sqrt(colSums(RCM$X)/sum(RCM$X)), diag(rowSums(RCM$X))) %*% (sqrt(RCM$X/rowSums(RCM$X))-sqrt(colSums(RCM$X)/sum(RCM$X)))))) %*% Hellinger$v
    ))
  tmpList = tmpList[!sapply(tmpList, is.null)]
  rowNames = rownames(tmpList$RCM)
  lapply(tmpList, function(x){
    colnames(x) = NULL
    if(nrow(x)==length(rowNames)) rownames(x) = rowNames
    x
  })
}
#---------------------------------------#
# An auxiliary function to get weighted taxon scores
weightedTaxonScores = function(X, rowScores){
  crossprod(X %*% diag(1/colSums(X)),rowScores)
}
#-----------------------------------------#
# A functions to plot the correlations of the library sizes with the row scores, or of the abundances with the column scores
plotCor = function(scores, datList, Dims = 1:3, scoreDim = "rows", dataMat = TRUE, yloc = 0.2, groupMeth = factor(c(as.character(groupsMeth[names(groupsMeth) %in% names(scores[[1]])]), "Control"), levels = c(levels(groupsMeth), "Control"))){
if(!dataMat){
  datList = lapply(datList, function(x){x$dataMat})
}
  margins = switch(scoreDim, "rows" = rowSums(datList[[1]]), "columns" = colSums(datList[[1]]))
cor0 = lapply(Dims, function(Dim){
  rbind(
  mapply(scores, datList, FUN = function(x,z){
  margins = switch(scoreDim, "rows" = rowSums(z), "columns" = colSums(z))
  sapply(x, function(y){libCor(y, margins = margins, Dim = Dim)}) 
 }, SIMPLIFY = TRUE),
 "Control" = replicate(length(datList),cor(rnorm(length(margins)), margins)))}) #Include a control

cor0df = lapply(cor0, melt, value.name = "Correl", varnames = c("Method"))
par(mfrow = c(1,max(Dims)))
for (i in Dims){
  if(i==max(Dims)){
  parTmp = par(no.readonly = TRUE)
  par(mar = c(4,4,4,5))
}
data = orderDF(cor0df[[i]])
boxplot(Correl ~ Method, main = paste(
  switch(scoreDim, "rows" = "Correlations of library sizes \n with row scores of dimension",
         "columns" = "Correlations of species abundances \n with column scores of dimension"
           ),i), 
  data = data, ylim = c(-1,1), ylab = "Pearson correlation", col = groupMeth, las = 2)
meansCor = tapply(data$Correl, data$Method, mean)
points(meansCor, col="red", pch=18, cex = 1.6)
abline(h=0, lty = "dashed")
if(i==max(Dims)){
  par(parTmp)
}
}
addLegend(groupMeth = groupMeth, x = length(groupMeth), cex = 0.6)
par(mfrow = c(1,1))

}
#----------------------------------#
# A function to plot the silhouettes
plotSil = function(resListRows, groupFactor, method = NULL, groupMeth = droplevels(groupsMeth[names(groupsMeth) %in% names(resListRows[[1]])]),las = 2,...){
  parTmp = par(no.readonly = TRUE)
  par(mfrow=  c(1,1))
sil1  = lapply(resListRows, function(x){
  sapply(x, silhouette, clusters = groupFactor)
})
meanSil1 = sapply(sil1, function(x){
  colMeans(x)
})
meanSil1df = orderDF(melt(meanSil1, value.name = "Silhouette", varnames = c("Method")))

if(!is.null(method)){
  meanSil1df = subset(meanSil1df, meanSil1df$Method %in% method)
  }
boxplot(Silhouette~Method, main = "Mean silhouettes per Monte Carlo simulation by method", data = meanSil1df, col = groupMeth,las = las, ...)
meansSil = tapply(meanSil1df$Silhouette, meanSil1df$Method, mean)
points(meansSil, col="red", pch=18, cex = 1.6)
abline(h=0, lty = "dashed")
addLegend(groupMeth = groupMeth)
par(parTmp)
}
#---------------------------------#
distRatioPlot = function(resListRows, groupFactor, method = NULL, groupMeth = droplevels(groupsMeth[names(groupsMeth) %in% names(resListRows[[1]])]),log = FALSE, las = 2,...) {
dist = sapply(resListRows, function(x){
  sapply(x, function(y){distanceFun(y, clusters = groupFactor)$ratio})
})
meanDist1df = orderDF(melt(dist, value.name = "DistanceRatio", varnames = c("Method")))
if(!is.null(method)){
  meanDist1df = subset(meanDist1df, meanDist1df$Method %in% method)
}
boxplot(DistanceRatio~Method, main = "Mean ratios of overal to within distance by method", data = meanDist1df, col = groupMeth, ylim = if(log) NULL else c(0, ceiling(max(meanDist1df$DistanceRatio))), las = las,log = ifelse(log,"y",""),...)
meansDist = tapply(meanDist1df$DistanceRatio, meanDist1df$Method, mean)
points(meansDist, col="red", pch=18, cex = 1.6)
abline(h=1, lty = "dashed")
addLegend(groupMeth = groupMeth, yloc = 5)
}
#---------------------------#
#A function to make a scatterplot of a randomly selected MC isntance
randomScatter = function(list, Dim = c(1,2), col = "black"){
  parTmp = par(no.readonly = TRUE)
  if(length(list)<=9) {
    par(mfrow = c(3,3), mar = c(3,3,3,3))
  } else if(length(list)<=12){
     par(mfrow = c(3,4), mar = c(3,3,3,3))
  } else {
    par(mfrow = c(4,4), mar = c(3,3,3,3))
  }
ID = sample(size=1, seq_along(list))
obj = list[[ID]]
lapply(names(obj),function(x){plot(obj[[x]][,Dim], main=x, col = col, xlab = paste("Dim", Dim[1]),ylab = paste("Dim", Dim[2]), pty = "s")})
par(parTmp)
}
#------------------------------#
#A function to wrap the calculation of the contribution of the taxa
contrTaxaWrap = function(resListRows, resListCols, IDlist, groupFactor, upDown = FALSE, centerFun = median){
dat = mapply(resListRows, resListCols, IDlist, SIMPLIFY = TRUE, FUN = function(row, col, id){
  mapply(row, col, SIMPLIFY = TRUE, MoreArgs = list(idTax = id, groupFactor = groupFactor, upDown = upDown, centerFun = centerFun), FUN = contrTaxa)
})
as.data.frame(t(dat))
}
#-----------------------------#
# A function to plot the result of this as boxplots. Horizontal boxplots have been tried but the overview over the best methods is a bit lost
contrPlot = function(contr, method = NULL, las = 2, expFac = 1.2, groupMeth = droplevels(groupsMeth[names(groupsMeth) %in% names(contr)]),...) {
moltenContr = orderDF(melt(contr, variable.name = "Method", value.name = "contrRatio", id.vars = NULL))
if(!is.null(method)){
  moltenContr = subset(moltenContr, moltenContr$Method %in% method)
}
meansDist = tapply(moltenContr$contrRatio, moltenContr$Method, mean, na.rm = TRUE)
at = seq_along(meansDist)*expFac
boxplot(contrRatio~Method, main = "Mean ratios of contribution of DA taxa \n relative to 50% most contributing non-DA taxa by method", data = moltenContr, ylab = "Mean contribution ratio", col = groupMeth, las = las, at = at,...)
points(y = meansDist, x = at, col="red", pch=18, cex = 1.6)
abline(h=1, lty = "dashed")
addLegend(groupMeth = groupMeth, expFac = expFac)
}
#-----------------------------#
# A function to calculate correlation with the starting values
startFun = function(list){
  X = list$RCM$X
  R = rowSums(X)
  C = colSums(X)
  E = outer(R, C)/sum(R)
  cMat = list$RCM$cMat
  rMat = list$RCM$rMat
  trended.dispersion.ind  <- estimateGLMTrendedDisp(y = t(X), design = NULL, method = "bin.loess",offset=t(log(E)))
  thetas = estDisp(X = X, muMarg = E, cMat = cMat, rMat = rMat, psis = rep(0, ncol(rMat)), trended.dispersion = trended.dispersion.ind)
  svd1 = svd(diag(1/sqrt(R)) %*% (X-E) %*% diag(1/sqrt(C)))
  svd2 = svd(diag(1/R) %*% (X-E) %*% diag(1/C))
  svd3 = svd(diag(R^(-3/2)) %*% (X-E) %*% diag(C^(-3/2)))
  svd4 = svd(diag(R^(-3)) %*% (X-E) %*% diag(C^(-3)))
  svd5 = svd(diag(1/R^2) %*% (X-E) %*% diag(1/C^2))
  svd6 =  svd((X-E)/((outer(R,C)+rowMultiply(outer(R^2, C^2), 1/thetas))/sum(R)))
  list(corRows1 = getCor(svd1$u, rMat), corRows2 = getCor(svd2$u, rMat), corRows3 = getCor(svd3$u, rMat), corRows4 = getCor(svd4$u, rMat), corRows5 = getCor(svd5$u, rMat), corRows6 = getCor(svd6$u, rMat) ,corCols1 = getCor(svd1$v, t(cMat)), corCols2 = getCor(svd2$v, t(cMat)), corCols3 = getCor(svd3$v, t(cMat)), corCols4 = getCor(svd4$v, t(cMat)), corCols5 = getCor(svd5$v, t(cMat)), corCols6 = getCor(svd6$v, t(cMat)))
} 
getCor = function(a,b){
  sapply(1:ncol(b), function(x){
    cor(a[,x], b[,x])
  })
}
#--------------------------------#
# A function to reorder the methods
orderDF = function(df){
  df$Method = factor(df$Method, labels = labelsMeth, levels = levelsMeth, ordered = TRUE)
  df$groupMethod = groupsMeth[match(df$Method, levelsMeth)]
  droplevels(df)
}
#-------------------------------#
#A function to add the legend
addLegend = function(yloc = 0.2, groupMeth, x = length(groupMeth)+1, expFac = 1,...){
legend(legend = levels(groupMeth), col = factor(levels(groupMeth), levels = levels(groupMeth), ordered=TRUE),pch = 16, x = x*expFac, y = yloc, xpd = TRUE,...)
}
```

# Simulations

## Unconstrained RC(M)

As ways of generating test datasets we see three different options

  1) Parametric simulation with the negative binomial with known abundances. 
  2) Non-parametric simulation with SimSeq
  3) Real datasets with biological signal allegedly known
  
  Parametric simulation is needed as a first check but evidently favours our method (if we use the negative binomial distribution). SimSeq is a reasonably neutral tool, we will only use WMW to obtain the lfdr. Real datasets will need to complement our analysis but it may not be possible to evaluate them in a high-throughput way, this will need to happen empirically.
  
### Parametric simulations

#### Negative binomial

The negative binomial is the assumed distribution of our method. As a result we should outperform all other methods in this setting.

\clearpage

##### Null setting

In this setting we simulate all distributions from the same template, thus there is no signal in this dataset

```{r NBnull, purl = FALSE}
p = 1000 #Number of taxa
n = 60 #Number of samples
nPop = 4 #Number of underlying populations
nPerPop = n/nPop #Number of samples per population
reps = 100 #replicates
nDim  = 3 #Number of dimensions
load("/home/stijn/PhD/Simulations/data/MLES.RData") #Load observed parameter values
rhoEst = unlist(rhoMLEs) #observed relative abundances, pooled
thetaEst = 1/unlist(phiMLEs) #observed dispersions abundances, pooled

#Library sizes
libSizesEst = unlist(sapply(physeqListV13, sample_sums)) #Resample
if(!file.exists(file = "RCMsim/resList0.RData")){
  if(!file.exists(file = "RCMsim/datList0.RData")){
datList0 = mclapply(mc.cores=1, 1:reps, function(i){
## Sample parameters
rhosSampled = sample(rhoEst, p)
thetasSampled = sample(thetaEst, p) #Sampled dispersions are the same in every population, this is the ideal case assumed by our method.
libSizesSampled = sample(libSizesEst, n) #Sampled libSizes

## Create dataset
meanMat = outer(libSizesSampled, rhosSampled)
dataMat = matrix(rnbinom(prod(dim(meanMat)), mu = meanMat, size = matrix(thetasSampled,n,p,byrow = TRUE)),n,p)
dataMat = dataMat[rowSums(dataMat)>0,colSums(dataMat)>0]
list(dataMat = dataMat, libSizesSampled = libSizesSampled, thetasSampled = thetasSampled, rhosSampled = rhosSampled)
})
save(datList0,  file = "RCMsim/datList0.RData")
} else {load(file = "RCMsim/datList0.RData")}

# Results
  if(!file.exists("RCMsim/resList0RCM.RData")){
RCMresList0 = mclapply(mc.cores=4, datList0, function(y){
  RCM(y$dataMat, k = nDim, distribution = "NB")
  })
save(RCMresList0, file = "RCMsim/resList0RCM.RData")
} else { load("RCMsim/resList0RCM.RData")}
resList0 = otherAnalyses(RCMresList0)
resList0Rows = lapply(resList0, extractRows)
resList0Cols = mapply(resList0, resList0Rows, FUN=extractCols, SIMPLIFY=FALSE)
save(resList0, resList0Rows,resList0Cols,  file = "RCMsim/resList0.RData")
} else {load(file = "RCMsim/resList0.RData")}
```

```{r reslist0Runningtimes, purl = FALSE, eval = FALSE, fig.cap = "Boxplot of running times of the RC(M) method in minutes for 100 Monte Carlo simulations. \\label{fig:runtimes}"}
boxplot(sapply(resList0, function(x){x$RCM$runtimeInMins}))
```

```{r resList0 sampleBiplot, purl = FALSE, include=FALSE}
#A sample biplot
randomScatter(resList0Rows)
# Our method and CoDa show only white noise, CA and PCoA show artificial clusters. Confounding mean and dispersion effects?
#randomScatter(resList0Rows, Dim = c(1,3))
```

```{r libSizeCorr0, purl = FALSE, fig.cap = "Correlations of row scores with library sizes, results of 100 simulations shown for first two dimensions. Red diamonds indicate means.\\label{fig:libCor1}"}
datList0 = lapply(resList0, function(x){x$RCM$X})
plotCor(resList0Rows, datList0, scoreDim = "rows", dataMat = TRUE, groupMeth = groupsMeth)
```

Hellinger distance, CoDa ana Bray-Curtis on absolut abundances show correlations with library sizes.

The taxa

```{r resList0cols sampleBiplot, purl = FALSE, include = FALSE}
#A sample biplot
randomScatter(resList0Cols)
# CA has outliers, CoDa and Hellinger reveal weird patterns
#randomScatter(resList0Cols, Dim = c(1,3))
```

```{r AbsCorr0, purl = FALSE, fig.cap = "Correlations of column scores with average relative abundances, results of 100 simulations shown for first two dimensions. Red diamonds indicate means.\\label{fig:libCor1}"}
plotCor(resList0Cols, datList = datList0,scoreDim = "columns", dataMat = TRUE)
rm(resList0Rows, resList0, datList0)
```

Only our RC(M) method has correlations at the level of random noise.

```{r Starting Values, purl = FALSE, eval = FALSE}
startList= lapply(resList0, startFun)
moltStart = melt(startList, value.name = "correlation")
names(moltStart) = c("correlation", "scores","run")
boxplot(correlation ~ scores, data = moltStart, las=2)
abline(h=0, col ="red", lty = "dashed")
#Squared weighing clearly gives the best results, although no fantastic correlations.
```

\clearpage

##### Simulate from different distributions

In this setting we will simulate data from 4 unrelated negative binomial distributions. Because of this clustering should be very good.

```{r NBunrelatedDistr, purl = FALSE}
p = 1000 #Number of taxa
n = 60 #Number of samples
nPop = 4 #Number of underlying populations
nPerPop = n/nPop #Number of samples per population
reps = 100 #replicates
nDim  = 3 #Number of dimensions

#Load observed parameter values
rhoEst = unlist(rhoMLEs) #observed relative abundances, pooled
thetaEst = 1/unlist(phiMLEs) #observed dispersions abundances, pooled

groupInd = rep(1:nPop, each = nPerPop) #Group indicator, as integers
groupFactor1 = factor(groupInd, levels = 1:nPop, labels =  paste("Population", 1:nPop), ordered = TRUE) #as factor

#Library sizes

libSizesEst = unlist(sapply(physeqListV13, sample_sums)) #Resample
if(!file.exists(file = "RCMsim/resList1.RData")){
  if(!file.exists(file = "RCMsim/datList1.RData")){
datList1 = mclapply(mc.cores=1, 1:reps, function(i){
## Sample parameters
rhosSampled = sapply(1:nPop, function(x){tmp=sample(rhoEst, p); tmp/sum(tmp)})[,groupInd]
thetasSampled = sample(thetaEst, p) #Sampled dispersions are the same in every population, this is the ideal case assumed by our method.
libSizesSampled = sample(libSizesEst, n) #Sampled libSizes

## Create dataset
meanMat = libSizesSampled * t(rhosSampled)
dataMat = matrix(rnbinom(prod(dim(meanMat)), mu = meanMat, size = matrix(thetasSampled,n,p, byrow = TRUE)),n,p)
list(dataMat = dataMat, group = groupFactor1, libSizesSampled = libSizesSampled, thetasSampled = thetasSampled, rhosSampled = rhosSampled)
})
save(datList1,groupFactor1,  file = "RCMsim/datList1.RData")
} else {load(file = "RCMsim/datList1.RData")}

# Results

  if(!file.exists("RCMsim/resList1RCM.RData")){
RCMresList1 = mclapply(mc.cores = 4, datList1, function(y){
  RCM(y$dataMat, k = nDim, distribution = "NB")
  })
save(RCMresList1, file = "RCMsim/resList1RCM.RData")
} else {load("RCMsim/resList1RCM.RData")}
resList1 = otherAnalyses(RCMresList1)
resList1Rows = lapply(resList1, extractRows)
resList1Cols = mapply(resList1, resList1Rows,FUN = extractCols, SIMPLIFY = FALSE)
save(resList1,resList1Rows, resList1Cols, groupFactor1, file = "RCMsim/resList1.RData")
} else {load(file = "RCMsim/resList1.RData")}
```

```{r resList1 sampleBiplotE, purl = FALSE}
#A sample biplot
datList1 = lapply(resList1, function(x){x$RCM$X})
randomScatter(resList1Rows)
# It seems like our method performs better, only two groups do not get separated very well. But then again this is under the model assumption of the NB, although one might expect the other methods to indentify the clusters. Jensen Shannon divergence also performs rather well, separating all (but one) clusters in two dimensions. Look in first and third dimension. Our method gets outperformed by PCoA according to this statistic. Would this be because PCoA focusses explicitly on samplewise distances?
```

```{r resList1Evalsil, purl = FALSE, fig.cap = "Mean silhouettes by method, results of 100 simulations shown for first two dimensions. Red diamonds indicate means. A silhouette of 1 point to perfect classification, 0 to borderline cases, -1 to perfect wrong classification \\label{fig:silhouette1}"}
plotSil(resList1Rows, groupFactor1)
```

```{r resList1Evaldist, purl = FALSE, fig.cap = "Mean ratios of within to overal distance by method, results of 100 simulations shown for first two dimensions. Red diamonds indicate means. \\label{fig:dist1}"}
distRatioPlot(resList1Rows, groupFactor1)
```

```{r libSizeCorr1, purl = FALSE, fig.cap = "Correlations of row scores with library sizes, results of 100 simulations shown for first two dimensions. Red diamonds indicate means. \\label{fig:libCor1}"}

plotCor(resList1Rows, datList1, dataMat = TRUE)
rm(resList1Rows, resList1, datList1)
```

The library sizes of the CoDa approach do negatively correlate with the row scores, confirming our concern about the heteroscedasticity.

\clearpage

##### Simulate from related distributions

In this setting we will simulate data from 4 related negative binomial distributions, all obtained from the same mother distributions but 10\% of the taxa have modified abundances

```{r NBrelatedDistr, purl = FALSE}
p = 1000 #Number of taxa
n = 60 #Number of samples
nPop = 4 #Number of underlying populations
nPerPop = n/nPop #Number of samples per population
reps = 100 #replicates
nDim  = 3 #Number of dimensions
TPR = 0.1
FC = 5

groupInd = rep(1:nPop, each = nPerPop) #Group indicator, as integers
groupFactor2 = factor(groupInd, levels = 1:nPop, labels =  paste("Population", 1:nPop), ordered = TRUE) #as factor
if(!file.exists(file = "RCMsim/resList2.RData")){
  if(!file.exists(file = "RCMsim/datList2.RData")){
datList2 = mclapply(mc.cores = 1, 1:reps, function(i){
## Sample parameters
rhosSampled0 = sample(rhoEst, p)
rhosSampled0 = rhosSampled0/sum(rhosSampled0)
#For each population, modify a fraction TPR of the abundances (compensation is not so badly needed here, we do not test individual species). Record the taxa that have been modified.
rhosIDSampled = lapply(1:nPop, function(x, rhos){
  id = sample(seq_len(p), round(TPR*p))
  rhos[id] = rhos[id]*FC
  cbind(rhos = rhos/sum(rhos), id = id) #renormalize
}, rhos = rhosSampled0)

rhosSampled00 = sapply(rhosIDSampled, function(x){x[, "rhos"]})
rhosSampled = rhosSampled00[,groupInd]
IDsampled = sapply(rhosIDSampled, function(x){x[, "id"]})

thetasSampled = sample(thetaEst, p) 
libSizesSampled = sample(libSizesEst, n) #Sampled libSizes

## Create dataset
meanMat = libSizesSampled * t(rhosSampled)
dataMat = matrix(rnbinom(prod(dim(meanMat)), mu = meanMat, size = matrix(thetasSampled,n,p,byrow = TRUE)),n,p)
colnames(dataMat) = rownames(rhosSampled) 
rownames(dataMat) = groupFactor2
dataMat = dataMat[rowSums(dataMat)>0,colSums(dataMat)>0]
list(dataMat = dataMat, libSizesSampled = libSizesSampled, thetasSampled = thetasSampled, rhosSampled = rhosSampled00, IDsampled = IDsampled)
})
save(datList2,groupFactor2,  file = "RCMsim/datList2.RData")
} else {load(file = "RCMsim/datList2.RData")}

# Results

  if(!file.exists("RCMsim/resList2RCM.RData")){
RCMresList2 = mclapply(mc.cores=4, datList2, function(y){
  RCM(y$dataMat, k = nDim, distribution = "NB")
  })
save(RCMresList2, file = "RCMsim/resList2RCM.RData")
} else {load("RCMsim/resList2RCM.RData")}
resList2 = otherAnalyses(RCMresList2)
IDlist2 = lapply(datList2, function(x){
   id = apply(x$IDsampled, 2, unique)
   taxa = apply(id, 2, function(y){rownames(x$rhosSampled)[y]})
   })
resList2Rows = lapply(resList2, extractRows)
resList2Cols = mapply(resList2, resList2Rows,FUN = extractCols, SIMPLIFY = FALSE)
datList2 = lapply(resList2, function(x){x$RCM$X})
save(datList2, resList2Rows, resList2Cols, IDlist2, groupFactor2, file = "RCMsim/resList2.RData")
} else {load(file = "RCMsim/resList2.RData")}
```

```{r resList2 sampleBiplot, purl = FALSE}
#A sample biplot
randomScatter(resList2Rows, col = groupFactor2)
# In this setting, the RCM really appears to outperform the other methods
# par(mfrow = c(2,4), mar = c(3,3,3,3))
# ID = sample(size=1, seq_along(resList2))
# obj = resList2Rows[[ID]]
# lapply(names(obj),function(x){plot(obj[[x]][,c(1,3)], col = groupFactor2, main=x, xlab = "Dim 1",ylab = "Dim 3", pty = "s")})
# rm(ID, obj)
# par(mfrow = c(1,1))
```

```{r resList2Evalsil, purl = FALSE, fig.cap = "Mean silhouettes by method, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. A silhouette of 1 point to perfect classification, 0 to borderline cases, -1 to perfect wrong classification \\label{fig:silhouette2}"}
plotSil(resList2Rows, groupFactor2)
```

The RC(M) method has the highest silhouettes, PCoA is second best but far below.

```{r resList2Evaldist, purl = FALSE, fig.cap = "Mean ratios of within to overal distance by method, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. \\label{fig:dist2}"}
distRatioPlot(resList2Rows, groupFactor2)
```

RCM comes out as best, CA as worst

```{r libSizeCorr2, purl = FALSE, fig.cap = "Correlations of row scores with library sizes, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. 'Control' is a negative control setting of an independent standard normal variable. \\label{fig:libCor2}"}
plotCor(resList2Rows, datList2, dataMat = TRUE)
```

```{r AbCorr2, purl = FALSE, fig.cap = "Correlations of column scores with abundances, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. 'Control' is a negative control setting of an independent standard normal variable. \\label{fig:abCor2}"}
plotCor(resList2Cols, datList2, dataMat = TRUE, scoreDim = "columns")
```

The CoDa method is clearly affected by the library sizes, as is CA and Bray-Curtis on absolute abundances.

```{r taxRat2, purl = FALSE, fig.cap = "Mean ratio of signal taxa to top 50% non-signal taxa in terms of contribution to cluster separation, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. \\label{fig:taxRat2}"}
contr2 = contrTaxaWrap(resList2Rows, resList2Cols, IDlist2, groupFactor2)
contrPlot(contr2)

# contr2Mean = contrTaxaWrap(resList2Rows, resList2Cols, IDlist2, groupFactor2, centerFun = mean)
# contrPlot(contr2Mean)
```

The RC(M) and CA methods are best at identifying taxa that contribute to the separation of the clusters. This is not surprising since they are the only ones that have genuine taxon scores, for CoDA and PCoA we have to rely on weighted sample scores for the taxa. Even though this is often used, our results reveal their poor performance. Hellinger does well because of outliers, it does not really paint a good picture.

##### Simulate from related distributions, confounded by library sizes

In this setting we will again simulate data from 4 related negative binomial distributions, all obtained from the same mother distributions but 10\% of the taxa have modified abundances. In this case we will make the library sizes different in each group. Th initial library sizes are all sampled from the same pool of library sizes observed in real data. The first group has unmodified library sizes, the second group library sizes multiplied by 1.5, the third by 2 and the fourth by 3.

```{r NBrelatedDistrConf, purl = FALSE}
p = 1000 #Number of taxa
n = 60 #Number of samples
nPop = 4 #Number of underlying populations
nPerPop = n/nPop #Number of samples per population
reps = 100 #replicates
nDim  = 3 #Number of dimensions
TPR = 0.1
FC = 5

groupInd = rep(1:nPop, each = nPerPop) #Group indicator, as integers
groupFactor3 = factor(groupInd, levels = 1:nPop, labels =  paste("Population", 1:nPop), ordered = TRUE) #as factor
if(!file.exists(file = "RCMsim/resList3.RData")){
  if(!file.exists(file = "RCMsim/datList3.RData")){
datList3 = mclapply(mc.cores = 4, 1:reps, function(i){
## Sample parameters
rhosSampled0 = sample(rhoEst, p)
rhosSampled0 = rhosSampled0/sum(rhosSampled0)
#For each population, modify a fraction TPR of the abundances (compensation is not so badly needed here, we do not test individual species). Record the taxa that have been modified.
rhosIDSampled = lapply(1:nPop, function(x, rhos){
  id = sample(seq_len(p), round(TPR*p))
  rhos[id] = rhos[id]*FC
  cbind(rhos = rhos/sum(rhos), id = id) #renormalize
}, rhos = rhosSampled0)

rhosSampled00 = sapply(rhosIDSampled, function(x){x[, "rhos"]})
rhosSampled = rhosSampled00[,groupInd]
IDsampled = sapply(rhosIDSampled, function(x){x[, "id"]})

thetasSampled = sample(thetaEst, p) 
libSizesSampled = sample(libSizesEst, n) #Sampled libSizes, confounded with group membership
libSizesSampled[groupInd==2] = round(libSizesSampled[groupInd==2]*1.5)
libSizesSampled[groupInd==3] = round(libSizesSampled[groupInd==3]*2)
libSizesSampled[groupInd==4] = round(libSizesSampled[groupInd==4]*3)

## Create dataset
meanMat = libSizesSampled * t(rhosSampled)
dataMat = matrix(rnbinom(prod(dim(meanMat)), mu = meanMat, size = matrix(thetasSampled,n,p,byrow = TRUE)),n,p)
colnames(dataMat) = rownames(rhosSampled) 
rownames(dataMat) = groupFactor3
dataMat = dataMat[rowSums(dataMat)>0,colSums(dataMat)>0]
list(dataMat = dataMat, libSizesSampled = libSizesSampled, thetasSampled = thetasSampled, rhosSampled = rhosSampled00, IDsampled = IDsampled)
})
save(datList3,groupFactor3,  file = "RCMsim/datList3.RData")
} else {load(file = "RCMsim/datList3.RData")}

# Results
  if(!file.exists("RCMsim/resList3RCM.RData")){
RCMresList3 = mclapply(mc.cores=4, datList3, function(y){
  RCM(y$dataMat, k = nDim, distribution = "NB")
  })
save(RCMresList3, file = "RCMsim/resList3RCM.RData")
} else {load("RCMsim/resList3RCM.RData")}
resList3 = otherAnalyses(RCMresList3)
IDlist3 = lapply(datList3, function(x){
   id = apply(x$IDsampled, 2, unique)
   taxa = apply(id, 2, function(y){rownames(x$rhosSampled)[y]})
   })
resList3Rows = lapply(resList3, extractRows)
resList3Cols = mapply(resList3, resList3Rows,FUN = extractCols, SIMPLIFY = FALSE)
save(resList3,resList3Rows, resList3Cols, IDlist3, groupFactor3, file = "RCMsim/resList3.RData")
} else {load(file = "RCMsim/resList3.RData")}
```

```{r resList3 sampleBiplot, purl = FALSE}
#A sample biplot
resList3Rows = lapply(resList3, extractRows)
randomScatter(resList3Rows, col = groupFactor3)
```

```{r resList3Evalsil, purl = FALSE, fig.cap = "Mean silhouettes by method, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. A silhouette of 1 point to perfect classification, 0 to borderline cases, -1 to perfect wrong classification \\label{fig:silhouette3}"}
plotSil(resList3Rows, groupFactor3)
```

The RC(M) method has the highest silhouettes, PCoA is second best but far below.

```{r resList3Evaldist, purl = FALSE, fig.cap = "Mean ratios of within to overal distance by method, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. \\label{fig:dist3}"}
distRatioPlot(resList3Rows, groupFactor3)
```

RCM comes out as best, CA as worst

```{r libSizeCorr3, purl = FALSE, fig.cap = "Correlations of row scores with library sizes, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. 'Control' is a negative control setting of an independent standard normal variable. \\label{fig:libCor3}"}
datList3 = lapply(resList3, function(x){x$RCM$X})
plotCor(resList3Rows, datList3, dataMat = TRUE)
```

```{r AbCorr3, purl = FALSE, fig.cap = "Correlations of column scores with abundances, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. 'Control' is a negative control setting of an independent standard normal variable. \\label{fig:abCor3}"}
plotCor(resList3Cols, datList3, dataMat = TRUE, scoreDim = "columns")
```

The CoDa method is clearly affected by the library sizes, as is CA and Bray-Curtis on absolute abundances.

```{r taxRat3, purl = FALSE, fig.cap = "Mean ratio of signal taxa to top 50% non-signal taxa in terms of contribution to cluster separation, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. \\label{fig:taxRat3}"}
contr3 = contrTaxaWrap(resList3Rows, resList3Cols, IDlist3, groupFactor3)
contrPlot(contr3)
```

##### Simulate under H0, but with differences in dispersion

Inspired by Warton _et al._ (2012): "Distance-based multivariate analyses confound location and dispersion effects" we set up a simulation without any signal, but whereby the dispersions in the different groups are different, as a result of the group effect itself or related sampling effects. Note that this also violates the assumption of our RC(M) method of constant dispersions

```{r NBdispsersion, purl = FALSE}
p = 1000 #Number of taxa
n = 60 #Number of samples
nPop = 4 #Number of underlying populations
nPerPop = n/nPop #Number of samples per population
reps = 100 #replicates
nDim  = 3 #Number of dimensions

groupInd = rep(1:nPop, each = nPerPop) #Group indicator, as integers
groupFactor4 = factor(groupInd, levels = 1:nPop, labels =  paste("Population", 1:nPop), ordered = TRUE) #as factor
if(!file.exists(file = "RCMsim/resList4.RData")){
  if(!file.exists(file = "RCMsim/datList4.RData")){
datList4 = mclapply(mc.cores = 4, 1:reps, function(i){
## Sample parameters
rhosSampled = sample(rhoEst, p)
thetasSampled = sample(thetaEst, p) 
libSizesSampled = sample(libSizesEst, n) #Sampled libSizes
#Sampled dispersions differ between the population
thetaMat = matrix(thetasSampled, n, p, byrow = TRUE) * c(rep(c(0.2,0.5,1,5), eac = nPerPop)) #Modify dispersions to 5 times, 2 times and 0.2 times the normal levels

## Create dataset
meanMat = outer(libSizesSampled, rhosSampled)
dataMat = matrix(rnbinom(prod(dim(meanMat)), mu = meanMat, size = thetaMat),n,p)
dataMat = dataMat[rowSums(dataMat)>0,colSums(dataMat)>0]
list(dataMat = dataMat, libSizesSampled = libSizesSampled, thetaMat = thetaMat, rhosSampled = rhosSampled)
})
save(datList4,groupFactor4,  file = "RCMsim/datList4.RData")
} else {load(file = "RCMsim/datList4.RData")}

# Results
  if(!file.exists("RCMsim/resList4RCM.RData")){
RCMresList4 = mclapply(mc.cores=4, datList4, function(y){
  RCM(y$dataMat, k = nDim, distribution = "NB")
  })
save(RCMresList4, file = "RCMsim/resList4RCM.RData")
} else {load("RCMsim/resList4RCM.RData")}
resList4 = otherAnalyses(RCMresList4)
resList4Rows = lapply(resList4, extractRows)
resList4Cols = mapply(resList4, resList4Rows,FUN = extractCols, SIMPLIFY = FALSE)
save(resList4,resList4Rows, resList4Cols, groupFactor4, file = "RCMsim/resList4.RData")
} else {load(file = "RCMsim/resList4.RData")}
```

```{r resList4 sampleBiplot, purl = FALSE}
#A sample biplot
randomScatter(resList4Rows, col = groupFactor4)
```

RCM comes out as best, CA as worst

```{r libSizeCorr4, purl = FALSE, fig.cap = "Correlations of row scores with library sizes, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. 'Control' is a negative control setting of an independent standard normal variable. \\label{fig:libCor4}"}
datList4 = lapply(resList4, function(x){x$RCM$X})
plotCor(resList4Rows, datList4, dataMat = TRUE)
```

```{r AbCorr4, purl = FALSE, fig.cap = "Correlations of column scores with abundances, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. 'Control' is a negative control setting of an independent standard normal variable. \\label{fig:abCor4}"}
plotCor(resList4Cols, datList4, dataMat = TRUE, scoreDim = "columns")
```

##### Simulate under H1, with correlated taxa

We know that taxa living in the same niche are dependent on each other, and as a result their abundances are correlated. This phenomenon can affect the differentialn abundance detection, so it could also affect our visualization procedures. I would also expect it to have an effect on the CoDa method, which relies on ratios between taxa of the same sample. We'll use the correlation networks previously estimated from the HMP and AGP datasets.

```{r NBrelatedDistrCorr, purl = FALSE}
p = 1000 #Number of taxa
n = 60 #Number of samples
nPop = 4 #Number of underlying populations
nPerPop = n/nPop #Number of samples per population
reps = 100 #replicates
nDim  = 3 #Number of dimensions
TPR = 0.1
FC = 5
load("/home/stijn/PhD/Simulations/data/covList.RData")
groupInd = rep(1:nPop, each = nPerPop) #Group indicator, as integers
groupFactor5 = factor(groupInd, levels = 1:nPop, labels =  paste("Population", 1:nPop), ordered = TRUE) #as factor
if(!file.exists(file = "RCMsim/resList5.RData")){
  if(!file.exists(file = "RCMsim/datList5.RData")){
datList5 = mclapply(mc.cores = 4, 1:reps, function(i){
## Sample parameters
rhosSampled0 = sample(rhoEst, p)
rhosSampled0 = rhosSampled0/sum(rhosSampled0)
#For each population, modify a fraction TPR of the abundances (compensation is not so badly needed here, we do not test individual species). Record the taxa that have been modified.
rhosIDSampled = lapply(1:nPop, function(x, rhos){
  id = sample(seq_len(p), round(TPR*p))
  rhos[id] = rhos[id]*FC
  cbind(rhos = rhos/sum(rhos), id = id) #renormalize
}, rhos = rhosSampled0)

rhosSampled00 = sapply(rhosIDSampled, function(x){x[, "rhos"]})
rhosSampled = rhosSampled00[,groupInd]
IDsampled = sapply(rhosIDSampled, function(x){x[, "id"]})

thetasSampled = sample(thetaEst, p) 
libSizesSampled = sample(libSizesEst, n) #Sampled libSizes, confounded with group membership
covMatName = sample(names(covList), 1)

## Create dataset
meanMat = libSizesSampled * t(rhosSampled)

dataMat = rmvnegbin(n = n,mu = meanMat, ks = thetasSampled, Sigma = as.matrix(covList[[covMatName]]))
colnames(dataMat) = rownames(rhosSampled) 
rownames(dataMat) = groupFactor5
dataMat = dataMat[rowSums(dataMat)>0,colSums(dataMat)>0]
list(dataMat = dataMat, libSizesSampled = libSizesSampled, thetasSampled = thetasSampled, rhosSampled = rhosSampled00, IDsampled = IDsampled, covMatName = covMatName)
})
save(datList5,groupFactor5,  file = "RCMsim/datList5.RData")
} else {load(file = "RCMsim/datList5.RData")}

# Results
  if(!file.exists("RCMsim/resList5RCM.RData")){
RCMresList5 = mclapply(mc.cores=4, datList5, function(y){
  RCM(y$dataMat, k = nDim, distribution = "NB")
  })
save(RCMresList5, file = "RCMsim/resList5RCM.RData")
} else {load("RCMsim/resList5RCM.RData")}
resList5 = otherAnalyses(RCMresList5)
IDlist5 = lapply(datList5, function(x){
   id = apply(x$IDsampled, 2, unique)
   taxa = apply(id, 2, function(y){rownames(x$rhosSampled)[y]})
   })
resList5Rows = lapply(resList5, extractRows)
resList5Cols = mapply(resList5, resList5Rows,FUN = extractCols, SIMPLIFY = FALSE)
save(resList5,resList5Rows, resList5Cols, IDlist5, groupFactor5, file = "RCMsim/resList5.RData")
} else {load(file = "RCMsim/resList5.RData")}
```

```{r resList5 sampleBiplot, purl = FALSE}
#A sample biplot
randomScatter(resList5Rows, col = groupFactor5)
```

```{r resList5Evalsil, purl = FALSE, fig.cap = "Mean silhouettes by method, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. A silhouette of 1 point to perfect classification, 0 to borderline cases, -1 to perfect wrong classification \\label{fig:silhouette5}"}
plotSil(resList5Rows, groupFactor5)
```

```{r resList5Evaldist, purl = FALSE, fig.cap = "Mean ratios of within to overal distance by method, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. \\label{fig:dist5}"}
distRatioPlot(resList5Rows, groupFactor5)
```

RCM comes out as best, CA as worst

```{r libSizeCorr5, purl = FALSE, fig.cap = "Correlations of row scores with library sizes, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. 'Control' is a negative control setting of an independent standard normal variable. \\label{fig:libCor5}"}
datList5 = lapply(resList5, function(x){x$RCM$X})
plotCor(resList5Rows, datList5, dataMat = TRUE)
```

```{r AbCorr5, purl = FALSE, fig.cap = "Correlations of column scores with abundances, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. 'Control' is a negative control setting of an independent standard normal variable. \\label{fig:abCor5}"}
plotCor(resList5Cols, datList5, dataMat = TRUE, scoreDim = "columns")
```

```{r taxRat5, purl = FALSE, fig.cap = "Mean ratio of signal taxa to top 50% non-signal taxa in terms of contribution to cluster separation, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. \\label{fig:taxRat5}"}
contr5 = contrTaxaWrap(resList5Rows, resList5Cols, IDlist5, groupFactor5)
contrPlot(contr5)
```

##### Negative binomial with phylogenetic information

In order to also include the very popular distance measures UniFrac and weighted UniFrac, we also generate data with the negative binomial and Dirichlet multinomial taking into account phylogenetic structure. We base our strategy on the second strategy described by Chen _et al._ ("Associating microbiome composition with environmental
covariates using generalized UniFrac distances", 2012). We generate random trees for the template vector of relative abundances, and modify abundances within one of the 20 clusters

```{r UniFrac, purl = FALSE}
p = 1000 #Number of taxa
n = 60 #Number of samples
nPop = 4 #Number of underlying populations
nPerPop = n/nPop #Number of samples per population
reps = 100 #replicates
nDim  = 3 #Number of dimensions
TPR = 0.1
FC = 5
k = round(1/TPR) #Number of clusters

groupInd = rep(1:nPop, each = nPerPop) #Group indicator, as integers
groupFactorPhy = factor(groupInd, levels = 1:nPop, labels =  paste("Population", 1:nPop), ordered = TRUE) #as factor

clusters = seq_len(k)

if(!file.exists(file = "RCMsim/resListPhy.RData")){
  if(!file.exists(file = "RCMsim/datListPhy.RData")){
datListPhy = mclapply(mc.cores = 4, 1:reps, function(i){
## Sample parameters
cluSampled = sample(clusters, nPop) #Cluster to be made DA

rhosSampled0 = sample(rhoEst, p)
rhosSampled0 = rhosSampled0/sum(rhosSampled0)

randomTree = rcoal(n = p, tip.label = names(rhosSampled0)) # The random tree, use rcoal to make it ultrametric
psclust = cutree(as.hclust(randomTree), k = k) #Split into clusters
names(psclust) = randomTree$tip.label #Assign names
psclust = psclust[names(rhosSampled0)] #Reorder according to the rhos

#For each population, modify the abundances of a cluster of related taxa. Record the clusters that have been modified.
rhosIDSampled = lapply(cluSampled, function(x, rhos){
  id = x == psclust
  rhos[id] = rhos[id]*FC
  cbind(rhos = rhos/sum(rhos), id = id) #renormalize
}, rhos = rhosSampled0)

rhosSampled00 = sapply(rhosIDSampled, function(x){x[, "rhos"]})
rhosSampled = rhosSampled00[,groupInd]
IDsampled = sapply(rhosIDSampled, function(x){x[, "id"]})

thetasSampled = sample(thetaEst, p) 
libSizesSampled = sample(libSizesEst, n) #Sampled libSizes

## Create dataset
meanMat = libSizesSampled * t(rhosSampled)
dataMat = matrix(rnbinom(prod(dim(meanMat)), mu = meanMat, size = matrix(thetasSampled,n,p,byrow = TRUE)),n,p)
colnames(dataMat) = rownames(rhosSampled) 
rownames(dataMat) = groupFactorPhy
dataMat = dataMat[rowSums(dataMat)>0,colSums(dataMat)>0]
list(dataMat = dataMat, libSizesSampled = libSizesSampled, thetasSampled = thetasSampled, rhosSampled = rhosSampled00, IDsampled = IDsampled, cluSampled = cluSampled, randomTree = randomTree)
})
save(datListPhy, groupFactorPhy, file = "RCMsim/datListPhy.RData")
} else {load(file = "RCMsim/datListPhy.RData")}

# Results
 if(!file.exists("RCMsim/resListPhyRCM.RData")){
RCMresListPhy = mclapply(mc.cores=4, datListPhy, function(y){
  RCM(y$dataMat, k = nDim, distribution = "NB")
  })
save(RCMresListPhy, file = "RCMsim/resListPhyRCM.RData")
  } else {load("RCMsim/resListPhyRCM.RData")}
  listID = sapply(RCMresListPhy, is.list)
resListPhy = otherAnalyses(RCMresListPhy[listID], unifrac = TRUE, treeList = lapply(datListPhy[listID], function(x){x$randomTree}) )
IDlistPhy = lapply(datListPhy[listID], function(x){
   taxa = lapply(seq_len(ncol(x$IDsampled)), function(y){rownames(x$rhosSampled)[as.logical(x$IDsampled[,y])]})
   })
resListPhyRows = lapply(resListPhy, extractRows)
resListPhyCols = mapply(resListPhy, resListPhyRows,FUN = extractCols, SIMPLIFY = FALSE)
save(resListPhy, resListPhyRows, resListPhyCols, groupFactorPhy, IDlistPhy, file = "RCMsim/resListPhy.RData")
} else {load(file = "RCMsim/resListPhy.RData")}
```

```{r resListPhy sampleBiplot, purl = FALSE}
#A sample biplot
randomScatter(resListPhyRows, col = groupFactorPhy)
```

```{r resListPhyEvalsil, purl = FALSE, fig.cap = "Mean silhouettes by method, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. A silhouette of 1 point to perfect classification, 0 to borderline cases, -1 to perfect wrong classification \\label{fig:silhouettePhy}"}
plotSil(resListPhyRows, groupFactorPhy, las = 2)
```

The Unifrac distances do not seem to get any advantage from their phylogenetic distances

```{r resListPhyEvaldist, purl = FALSE, fig.cap = "Mean ratios of within to overal distance by method, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. \\label{fig:distPhy}"}
distRatioPlot(resListPhyRows, groupFactorPhy, log = TRUE)
```

The distance ratio statistic does reveal an advantage for weighted UniFrac in this setting.

```{r libSizeCorrPhy, purl = FALSE, fig.cap = "Correlations of row scores with library sizes, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. 'Control' is a negative control setting of an independent standard normal variable. \\label{fig:libCorPhy}"}
datListPhy = lapply(resListPhy, function(x){x$RCM$X})
plotCor(resListPhyRows, datListPhy, dataMat = TRUE)
```

But weighted unifrac suffers from correlations with the library sizes

```{r resListPhy taxaBiplot, purl = FALSE}
#A taxon biplot
randomScatter(resListPhyCols)
```

```{r AbCorrPhy, purl = FALSE, fig.cap = "Correlations of column scores with abundances, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. 'Control' is a negative control setting of an independent standard normal variable. \\label{fig:abCorPhy}"}
plotCor(resListPhyCols, datListPhy, dataMat = TRUE, scoreDim = "columns")
```

```{r taxRatPhy, purl = FALSE, fig.cap = "Mean ratio of signal taxa to top 50% non-signal taxa in terms of contribution to cluster separation, results of 100 simulations shown for first two dimensions for simulation with the negative binomial distribution with 10% DA taxa. Red diamonds indicate means. \\label{fig:taxRatPhy}"}
contrPhy = contrTaxaWrap(resListPhyRows, resListPhyCols, IDlistPhy, groupFactorPhy)
contrPlot(contrPhy, ylim = c(-4,4)) #Excluding outliers
```

#### Other distributions

We also simulate datasets from other distributions than the negative binomial to see how the method behaves when its distributional assumptions are violated.

##### Zero-inflated negative binomial distribution

The zero-inflated negative binomial (ZINB) distribution was used by Zhang _et al._ (2016) for differential abundance testing. Our own goodness-of-fit tests indicate that for 14\% of the taxa, the ZINB provides a better fit than the regular NB

```{r simZINB, purl = FALSE}
p = 1000 #Number of taxa
n = 60 #Number of samples
nPop = 4 #Number of underlying populations
nPerPop = n/nPop #Number of samples per population
reps = 100 #replicates
nDim  = 3 #Number of dimensions
TPR = 0.1
FC = 5

groupInd = rep(1:nPop, each = nPerPop) #Group indicator, as integers
groupFactorZINB = factor(groupInd, levels = 1:nPop, labels =  paste("Population", 1:nPop), ordered = TRUE) #as factor

if(!file.exists(file = "RCMsim/resListZINB.RData")){
load(file="/home/stijn/PhD/Biplots/Zicoef.RData")

if(!file.exists(file = "RCMsim/datListZINB.RData")){
datListZINB = mclapply(mc.cores=1, 1:reps, function(i){
## Sample parameters
rhosSampled0 = sample(rhoEst, p)
rhosSampled0 = rhosSampled0/sum(rhosSampled0)
#For each population, modify a fraction TPR of the abundances (compensation is not so badly needed here, we do not test individual species). Record the taxa that have been modified.
rhosIDSampled = lapply(1:nPop, function(x, rhos){
  id = sample(seq_len(p), round(TPR*p))
  rhos[id] = rhos[id]*FC
  cbind(rhos = rhos/sum(rhos), id = id) #renormalize
}, rhos = rhosSampled0)

rhosSampled00 = sapply(rhosIDSampled, function(x){x[, "rhos"]})
rhosSampled = rhosSampled00[,groupInd]
IDsampled = sapply(rhosIDSampled, function(x){x[, "id"]})

thetasSampled = sample(thetaEst, p) 
propSampled = expit(sample(ZIcoef, p))
libSizesSampled = sample(libSizesEst, n) #Sampled libSizes

## Create dataset
meanMat = libSizesSampled * t(rhosSampled)
dataMat = matrix(rzinegbin(prod(dim(meanMat)), munb = meanMat, size = matrix(thetasSampled,n,p, byrow = TRUE), pstr0 = matrix(propSampled,n,p,byrow = TRUE)),n,p)
colnames(dataMat) = rownames(rhosSampled) 
rownames(dataMat) = groupFactorZINB
dataMat = dataMat[rowSums(dataMat)>0,colSums(dataMat)>0]
list(dataMat = dataMat, libSizesSampled = libSizesSampled, thetasSampled = thetasSampled, rhosSampled = rhosSampled00, IDsampled = IDsampled, propSampled = propSampled)
})
save(datListZINB,groupFactorZINB,  file = "RCMsim/datListZINB.RData")
} else {load(file = "RCMsim/datListZINB.RData")}

# Results

 if(!file.exists("RCMsim/resListZINBRCM.RData")){
RCMresListZINB = mclapply(mc.cores=4, datListZINB, function(y){
  RCM(y$dataMat, k = nDim, distribution = "NB")
  })
save(RCMresListZINB, file = "RCMsim/resListZINBRCM.RData")
} else {load("RCMsim/resListZINBRCM.RData")}
resListZINB = otherAnalyses(RCMresListZINB)
IDlistZINB = lapply(datListZINB, function(x){
  id = apply(x$IDsampled, 2, unique)
  taxa = apply(id, 2, function(y){rownames(x$rhosSampled)[y]})
  })
save(resListZINB,groupFactorZINB, IDlistZINB, file = "RCMsim/resListZINB.RData")
} else {load(file = "RCMsim/resListZINB.RData")}
```

```{r resListZINB sampleBiplot, purl = FALSE}
#A sample biplot
resListZINBRows = lapply(resListZINB, extractRows)
randomScatter(resListZINBRows, col = groupFactorZINB)
# In this setting, the RCM really appears to outperform the other methods

# par(mfrow = c(ZINB,4), mar = c(3,3,3,3))
# ID = sample(size=1, seq_along(resListZINB))
# obj = resListZINBRows[[ID]]
# lapply(names(obj),function(x){plot(obj[[x]][,c(1,3)], col = groupFactorZINB, main=x, xlab = "Dim 1",ylab = "Dim 3", pty = "s")})
# rm(ID, obj)
# par(mfrow = c(1,1))
```

```{r resListZINBEvalsil, purl = FALSE, fig.cap = "Mean silhouettes by method, results of 100 simulations shown for first two dimensions for simulation with the zero-inflated negative binomial distribution with 10% DA taxa. Red diamonds indicate means. A silhouette of 1 point to perfect classification, 0 to borderline cases, -1 to perfect wrong classification \\label{fig:silhouette1}"}
plotSil(resListZINBRows, groupFactorZINB)
```

The RC(M) method has the highest silhouettes, PCoA is second best but far below.

```{r resListZINBEvaldist, purl = FALSE, fig.cap = "Mean ratios of within to overal distance by method, results of 100 simulations shown for first two dimensions for simulation with the zero-inflated negative binomial distribution with 10% DA taxa. Red diamonds indicate means. \\label{fig:dist1}"}
distRatioPlot(resListZINBRows, groupFactorZINB)
```

RCM comes out as best, CA as worst

```{r libSizeCorrZINB, purl = FALSE, fig.cap = "Correlations of row scores with library sizes, results of 100 simulations shown for first two dimensions for simulation with the zero-inflated negative binomial distribution with 10% DA taxa. Red diamonds indicate means. \\label{fig:libCor1}"}
datListZINB = lapply(resListZINB, function(x){x$RCM$X})
plotCor(resListZINBRows, datListZINB)
```

```{r AbCorrZINB, purl = FALSE, fig.cap = "Correlations of column scores with abundances, results of 100 simulations shown for first two dimensions for simulation with the zero-inflated negative binomial distribution with 10% DA taxa. Red diamonds indicate means. 'Control' is a negative control setting of an independent standard normal variable. \\label{fig:abCorZINB}"}
resListZINBCols = mapply(resListZINB, resListZINBRows,FUN = extractCols, SIMPLIFY = FALSE)
plotCor(resListZINBCols, datListZINB, dataMat = TRUE, scoreDim = "columns")
```

CA suffers from correlation with relative abundances, but not on average.

```{r taxRatZINB, purl = FALSE, fig.cap = "Mean ratio of signal taxa to top 50% non-signal taxa in terms of contribution to cluster separation, results of 100 simulations shown for first two dimensions for simulation with the zero-inflated negative binomial distribution with 10% DA taxa. Red diamonds indicate means. \\label{fig:taxRatZINB}"}
contrZINB = contrTaxaWrap(resListZINBRows, resListZINBCols, IDlistZINB, groupFactorZINB)
contrPlot(contrZINB)
```

Also here RCM and CA perform best in identifying taxa thanks to their explicit taxon scores.

\clearpage

##### Dirichlet-multinomial

The use of the Dirichlet multinomial for microbiome data was proposed by La Rosa _et al._ (2012). Although it is very restrictive in terms of variance and covariance structures, we include it here because of its relative popularity. Also Chen _et al._ (2012) use it to simulate data for a visualization.

```{r simDM, purl = FALSE}
p = 1000 #Number of taxa
n = 60 #Number of samples
nPop = 4 #Number of underlying populations
nPerPop = n/nPop #Number of samples per population
reps = 100 #replicates
nDim  = 3 #Number of dimensions
TPR = 0.1
FC = 5

groupInd = rep(1:nPop, each = nPerPop) #Group indicator, as integers
groupFactorDM = factor(groupInd, levels = 1:nPop, labels =  paste("Population", 1:nPop), ordered = TRUE) #as factor
load("/home/stijn/PhD/American Gut/AGdm.RData")

if(!file.exists(file = "RCMsim/datListDM.RData")){
datListDM = mclapply(mc.cores = 1, 1:reps, function(i){
## Sample parameters
rhosSampled0 = sample(rhoEst, p)
rhosSampled0 = rhosSampled0/sum(rhosSampled0)
#For each population, modify a fraction TPR of the abundances (compensation is not so badly needed here, we do not test individual species). Record the taxa that have been modified.
rhosIDSampled = lapply(1:nPop, function(x, rhos){
  id = sample(seq_len(p), round(TPR*p))
  rhos[id] = rhos[id]*FC
  cbind(rhos = rhos/sum(rhos), id = id) #renormalize
}, rhos = rhosSampled0)

rhosSampled00 = sapply(rhosIDSampled, function(x){x[, "rhos"]})
rhosSampled = rhosSampled00[,groupInd]
IDsampled = sapply(rhosIDSampled, function(x){x[, "id"]})

thetaDM = AGdm$theta
libSizesSampled = sample(libSizesEst, n) #Sampled libSizes

## Create dataset
dataMat = t(sapply(seq_along(libSizesSampled),function(i){Dirichlet.multinomial(libSizesSampled[i], shape = rhosSampled[,i]*(1-thetaDM)/thetaDM)}))
colnames(dataMat) = rownames(rhosSampled) 
rownames(dataMat) = groupFactorDM
dataMat = dataMat[rowSums(dataMat)>0,colSums(dataMat)>0]
list(dataMat = dataMat, libSizesSampled = libSizesSampled, rhosSampled = rhosSampled00, IDsampled = IDsampled)
})
save(datListDM,groupFactorDM, file = "RCMsim/datListDM.RData")
} else {load(file = "RCMsim/datListDM.RData")}

# Results
if(!file.exists(file = "RCMsim/resListDM.RData")){
  if(!file.exists("RCMsim/resListDMRCM.RData")){
RCMresListDM = mclapply(mc.cores=4, datListDM, function(y){
  RCM(y$dataMat, k = nDim, distribution = "NB")
  })
save(RCMresListDM, file = "RCMsim/resListDMRCM.RData")
  } else {load("RCMsim/resListDMRCM.RData")}
  listID = sapply(RCMresListDM, class)=="RCM"
RCMresListDM = Filter(RCMresListDM, f = is.list)
resListDM = otherAnalyses(RCMresListDM)
IDlistDM = lapply( datListDM[listID], function(x){
  id = apply(x$IDsampled, 2, unique)
  taxa = apply(id, 2, function(y){rownames(x$rhosSampled)[y]})
  })
save(resListDM, groupFactorDM,IDlistDM, file = "RCMsim/resListDM.RData")
} else {load(file = "RCMsim/resListDM.RData")}
```

Only 68% fits

```{r resListDM sampleBiplot, purl = FALSE}
#A sample biplot
resListDMRows = lapply(resListDM, extractRows)
randomScatter(resListDMRows, col = groupFactorDM)
# Our method and CoDa show only white noise, CA and PCoA show artificial clusters. Confounding mean and dispersion effects?
#randomScatter(resListDMRows, Dim = c(1,3))
```

```{r libSizeCorrDM, purl = FALSE, fig.cap = "Correlations of row scores with library sizes, results of 1DMDM simulations shown for first two dimensions for simulation with the Dirichlet-multinomial distribution with 10% DA taxa. Red diamonds indicate means.\\label{fig:libCor1}"}
datListDM = lapply(resListDM, function(x){x$RCM$X})
plotCor(resListDMRows, datListDM, scoreDim = "rows")
```

Hellinger distance, CoDa ana Bray-Curtis on absolut abundances show correlations with library sizes.

The taxa

```{r resListDMcols sampleBiplot, purl = FALSE, include = FALSE}
#A sample biplot
resListDMCols = mapply(resListDM, resListDMRows, FUN=extractCols, SIMPLIFY=FALSE)
randomScatter(resListDMCols)
# CA has outliers, CoDa and Hellinger reveal weird patterns
#randomScatter(resListDMCols, Dim = c(1,3))
```

CoDa and Hellinger distances have unacceptable correlatios with the library sizes

```{r AbsCorrDM, purl = FALSE, fig.cap = "Correlations of column scores with average relative abundances, results of 1DMDM simulations shown for first two dimensions for simulation with the Dirichlet-multinomial distribution with 10% DA taxa. Red diamonds indicate means.\\label{fig:libCor1}"}
plotCor(resListDMCols, lapply(resListDM, function(x){x$RCM$X}), scoreDim = "columns", dataMat = TRUE)
```

```{r resListDMEvalsil, purl = FALSE, fig.cap = "Mean silhouettes by method, results of 100 simulations shown for first two dimensions for simulation with the Dirichlet-multinomial distribution with 10% DA taxa. Red diamonds indicate means. A silhouette of 1 point to perfect classification, 0 to borderline cases, -1 to perfect wrong classification \\label{fig:silhouette1}"}
plotSil(resListDMRows, groupFactorDM)
```

JSD and Bray-Curtis have the best performance in this setting in terms of silhouette

```{r resListDMEvaldist, purl = FALSE, fig.cap = "Mean ratios of within to overal distance by method, results of 100 simulations shown for first two dimensions for simulation with the Dirichlet-multinomial distribution with 10% DA taxa. Red diamonds indicate means.  \\label{fig:dist1}"}
distRatioPlot(resListDMRows, groupFactorDM)
```

RCM comes out as best, together with JSD and Bray-Curtis

```{r AbCorrDM, purl = FALSE, fig.cap = "Correlations of column scores with abundances, results of 100 simulations shown for first two dimensions for simulation with the zero-inflated negative binomial distribution with 10% DA taxa. Red diamonds indicate means. 'Control' is a negative control setting of an independent standard normal variable. \\label{fig:abCorDM}"}
resListDMCols = mapply(resListDM, resListDMRows,FUN = extractCols, SIMPLIFY = FALSE)
plotCor(resListDMCols, datListDM, dataMat = TRUE, scoreDim = "columns")
```

CA suffers from correlation with relative abundances, but not on average.

```{r taxRatDM, purl = FALSE, fig.cap = "Mean ratio of signal taxa to top 50% non-signal taxa in terms of contribution to cluster separation, results of 100 simulations shown for first two dimensions for simulation with the zero-inflated negative binomial distribution with 10% DA taxa. Red diamonds indicate means. \\label{fig:taxRatDM}"}
contrDM = contrTaxaWrap(resListDMRows, resListDMCols, IDlistDM, groupFactorDM)
contrPlot(contrDM)
```

In this setting none of the methods does really well at identifying causal taxa

\clearpage

##### Nonparametric simulation

An objective simulation approach would be to use non-parametric resampling from a true dataset, as in _SimSeq_. For this we need microbiome datasets with covariates known to be related to bacterial abundance, preferably with more than two groups. The Zeller data is one such dataset, with the cancer variable expected to be related to relative abundance and having three levels (Normal, small adenoma and cancer). For the American Gut data we can use IBD. For the Turnbaugh lean and obese twins dataset we can use health status (lean, overweight or obese), even though we have only 18 samples. For the Turnbaugh humanized mouse dataset we could use "Diet", with levels "BK" and "Western".

We generate data as follows

1. Select a covariate and test for differential abundance using WMW/Kruskal-Wallis
2. Calculate local false discovery rates (lfdr)
3. Sample non DA taxa with equal weights from all taxa
4. Sample DA taxa from all taxa with weights equal to 1-lfdr
5. Sample counts from non DA taxa from samples with the most frequent covariate level
6. Sample counts from DA taxa from the samples with other covariate levels, and correct for differences in library sizes. This maintains the same distribution of covariate levels and overal daat matrix size as the original dataset. 

```{r simSeqFunctions}
# A wrapper function for phyloseq objects for the testing, yields the lfdr's of the taxa. We extend the original code to allow for Kruskal-Wallis test
testSimSeq = function(physeq, variable, filterTaxa = FALSE){
  countMat = if(taxa_are_rows(physeq)){t(otu_table(physeq)@.Data)} else {otu_table(physeq)@.Data}
  treatment = get_variable(physeq, variable)
  if(filterTaxa){
  # Filter physeq to no all-zeroes in either of the groups
  physeq = prune_taxa(x = physeq, taxa = apply(countMat, 2, function(y){
    !any(tapply(y, treatment, function(z){
      all(z==0)
    }))
  }))
  }
  #Filter physeq for NA's in variables
  physeq = prune_samples(x = physeq, !is.na(treatment))
  treatment = factor(get_variable(physeq, variable))
  countMat = if(taxa_are_rows(physeq)){t(otu_table(physeq)@.Data)} else {otu_table(physeq)@.Data}
  pvals = apply(countMat, 2, function(x){kruskal.test(x, g=treatment)$p.value})
  result = apply(countMat, 2, function(x){
    avRank = length(x)/2 + 0.5
    rankX = rank(x)
    sapply(levels(treatment),avRank = avRank, function(y, avRank){
    ifelse(mean(rankX[treatment==y])>avRank, "up","down")
        })}) #Was taxon up or down regulated in group 1 (first level of treatment?
  lfdr = fdrtool(pvals, statistic = "pvalue", plot=FALSE, verbose=FALSE)$lfdr
  list(lfdr = lfdr, physeq = physeq, variable = variable, result = result)
}
genSimSeq = function(testres, Ntaxa = ntaxa(testres$physeq), fracTaxa = 0.2){
  physeq = testres$physeq
  weights = 1-testres$lfdr
  weights = weights/sum(weights) #Normalize sampling weights
  normFactors = sample_sums(physeq)
  treatment = get_variable(physeq, testres$variable)
  DEtaxa = sample(taxa_names(physeq), round(Ntaxa*fracTaxa), prob = weights) # The DE taxa
  whichTrt = names(sort(table(treatment),decreasing=TRUE)[1]) # The most frequent treatment group, frow which we will sample the EE taxa
  trtLeft = unique(treatment)[unique(treatment) != whichTrt]
  countMat = if(taxa_are_rows(physeq)){t(otu_table(physeq)@.Data)} else {otu_table(physeq)@.Data}
  samIDee = sample(which(treatment == whichTrt),nrow(countMat), replace = TRUE)
  dataSimSeq = countMat[samIDee,] #EE taxa
  for (i in trtLeft){
  rowInd = which(treatment==i)
  dataSimSeq[rowInd,DEtaxa] = round(countMat[rowInd, DEtaxa] * normFactors[samIDee][rowInd]/normFactors[rowInd])
  }
  keepID = colSums(dataSimSeq)>0
  list(data = dataSimSeq[, keepID], treatment = treatment, DEtaxa = DEtaxa[DEtaxa %in% colnames(dataSimSeq[, keepID])])
}
```

```{r SimSeqTests, purl = FALSE}
if(!file.exists(file = "SimSeqTests.RData")){
load(file = "/home/stijn/PhD/Datasets/Humanized mouse/HumMicePhy.RData")
testHum = testSimSeq(HumMicePhy, "Diet")
load(file = "/home/stijn/PhD/Datasets/TurnbaughObeseTwins/TurnTwins.RData")
testTwins = testSimSeq(TurnTwins, "Status_p") # No significance
testAG = testSimSeq(AGphylo, "IBD")
testZeller = testSimSeq(zellerSphy, "Diagnosis")
save(testHum, testTwins, testAG, testZeller, file = "SimSeqTests.RData")
} else {load(file = "SimSeqTests.RData")}
```

No DA for the lean and obese twins. Then again it is a small dataset.

```{r SimSeqGenerateData, purl = FALSE}
if(!file.exists(file = "SimSeqData.RData")){
NsimSimSeq = 100
HumSimSeqList = mclapply(mc.cores = 1, 1:NsimSimSeq, function(i){
  genSimSeq(testHum)
})
AGSimSeqList = mclapply(mc.cores = 1, 1:NsimSimSeq, function(i){
  genSimSeq(testAG)
})
ZellerSimSeqList = mclapply(mc.cores = 1, 1:NsimSimSeq, function(i){
  genSimSeq(testZeller)
})
save(HumSimSeqList, ZellerSimSeqList, AGSimSeqList, file = "SimSeqData")
} else {load(file = "SimSeqData.RData")}
```

```{r simSeqAnalysis, purl = FALSE}
if(!file.exists("RCMsim/resListSimSeq.RData")){
  if(!file.exists("RCMsim/resListSimSeqRCM.RData")){
RCMresListSimSeqHum = mclapply(mc.cores=4, HumSimSeqList, function(y){
  RCM(y$data, k = nDim, distribution = "NB")
  })
RCMresListSimSeqAG = mclapply(mc.cores=4, AGSimSeqList, function(y){
  RCM(y$data, k = nDim, distribution = "NB")
  })
RCMresListSimSeqZeller = mclapply(mc.cores=4, ZellerSimSeqList, function(y){
  RCM(y$data, k = nDim, distribution = "NB", round = TRUE)
  })
save(RCMresListSimSeqHum, RCMresListSimSeqAG, RCMresListSimSeqZeller, file = "RCMsim/resListSimSeqRCM.RData")
  } else {load("RCMsim/resListSimSeqRCM.RData")}
  resListSimSeqHum = otherAnalyses(RCMresListSimSeqHum)
  IDlistHum = mapply(HumSimSeqList,resListSimSeqHum, FUN =  function(y,x, test){
    taxa = y$DEtaxa[y$DEtaxa %in% colnames(x$RCM$X)]
    test[taxa,]
    }, SIMPLIFY = FALSE, MoreArgs = list(test = t(testHum$result)))
  groupFactorHum = HumSimSeqList[[1]]$treatment
  resListSimSeqHumRows = lapply(resListSimSeqHum, extractRows)
  resListSimSeqHumCols = mapply(resListSimSeqHum, resListSimSeqHumRows, FUN=extractCols, SIMPLIFY=FALSE)
  datListHum = lapply(resListSimSeqHum, function(x){x$RCM$X})
  # Zeller
  zellerID = sapply(RCMresListSimSeqZeller, class) == "RCM"
  resListSimSeqZeller = otherAnalyses(RCMresListSimSeqZeller[zellerID])
  IDlistZeller = mapply(ZellerSimSeqList[zellerID],resListSimSeqZeller, FUN =  function(y,x, test){
    taxa = y$DEtaxa[y$DEtaxa %in% colnames(x$RCM$X)]
    test[taxa,]
    }, SIMPLIFY = FALSE, MoreArgs = list(test = t(testZeller$result)))
  groupFactorZeller = ZellerSimSeqList[[1]]$treatment
  resListSimSeqZellerRows = lapply(resListSimSeqZeller, extractRows)
  resListSimSeqZellerCols = mapply(resListSimSeqZeller, resListSimSeqZellerRows, FUN=extractCols, SIMPLIFY=FALSE)
  datListZeller = lapply(resListSimSeqZeller, function(x){x$RCM$X})
save(datListHum, IDlistHum, groupFactorHum, resListSimSeqHumRows, resListSimSeqHumCols, IDlistZeller, resListSimSeqZellerRows, resListSimSeqZellerCols, groupFactorZeller, datListZeller, file = "RCMsim/resListSimSeq.RData")
} else {load("RCMsim/resListSimSeq.RData")}
```

```{r resListSimSeqHum sampleBiplot, purl = FALSE}
#A sample biplot

randomScatter(resListSimSeqHumRows, col = groupFactorHum)
# Our method and CoDa show only white noise, CA and PCoA show artificial clusters. Confounding mean and dispersion effects?
#randomScatter(resListSimSeqHumRows, Dim = c(1,3))
```

Except for PCoA with Bray-Curtis distances and CA, all methods seem to separate the clusters rather well

```{r libSizeCorrHum, purl = FALSE, fig.cap = "Correlations of row scores with library sizes, results of 1HumHum simulations shown for first two dimensions for simulation with the Dirichlet-multinomial distribution with 10% DA taxa. Red diamonds indicate means.\\label{fig:libCor1}"}
plotCor(resListSimSeqHumRows, datListHum, scoreDim = "rows")
```

CoDa struggles with the library sizes, but also RC(M) and Hellinger seems to suffer from it slightly!

The taxa

```{r resListSimSeqHumcols sampleBiplot, purl = FALSE, include = FALSE}
#A sample biplot
randomScatter(resListSimSeqHumCols)
```

```{r AbsCorrHum, purl = FALSE, fig.cap = "Correlations of column scores with average relative abundances, results of 1HumHum simulations shown for first two dimensions for simulation with the Dirichlet-multinomial distribution with 10% DA taxa. Red diamonds indicate means.\\label{fig:libCor1}"}
plotCor(resListSimSeqHumCols, datListHum, scoreDim = "columns", dataMat = TRUE)
```

Taxa scores of correspondence analysis tend to correlate with taxon abundances in some cases

```{r resListSimSeqHumEvalsil, purl = FALSE, fig.cap = "Mean silhouettes by method, results of 100 simulations shown for first two dimensions for simulation with the Dirichlet-multinomial distribution with 10% DA taxa. Red diamonds indicate means. A silhouette of 1 point to perfect classification, 0 to borderline cases, -1 to perfect wrong classification \\label{fig:silhouette1}"}
plotSil(resListSimSeqHumRows, groupFactorHum)
```

In the non-parametric setting RCM achieves by far the highest silhouette!

```{r resListSimSeqHumEvaldist, purl = FALSE, fig.cap = "Mean ratios of within to overal distance by method, results of 100 simulations shown for first two dimensions for simulation with the Dirichlet-multinomial distribution with 10% DA taxa. Red diamonds indicate means.  \\label{fig:dist1}"}
distRatioPlot(resListSimSeqHumRows, groupFactorHum)
```

In the non-parametric setting, RCM achieves the lowest distance ratios, CA the highest

```{r AbCorrHum, purl = FALSE, fig.cap = "Correlations of column scores with abundances, results of 100 simulations shown for first two dimensions for simulation with the zero-inflated negative binomial distribution with 10% DA taxa. Red diamonds indicate means. 'Control' is a negative control setting of an independent standard normal variable. \\label{fig:abCorHum}"}
plotCor(resListSimSeqHumCols, datListHum, dataMat = TRUE, scoreDim = "columns")
```

CA suffers from correlation with relative abundances, but not on average.

```{r taxRatHum, purl = FALSE, fig.cap = "Mean ratio of signal taxa to top 50% non-signal taxa in terms of contribution to cluster separation, results of 100 simulations shown for first two dimensions for simulation with the zero-inflated negative binomial distribution with 10% DA taxa. Red diamonds indicate means. \\label{fig:taxRatHum}"}
contrHum = contrTaxaWrap(resListSimSeqHumRows, resListSimSeqHumCols, IDlistHum, groupFactorHum, upDown = FALSE)
contrPlot(contrHum)
```

In the SimSeq setting neither method does really well, surprisingly here Hellinger distance and CoDa are the best performers

```{r resListSimSeqZeller sampleBiplot, purl = FALSE}
#A sample biplot
randomScatter(resListSimSeqZellerRows, col = groupFactorZeller)
# Clearly a tough job
```

Except for PCoA with Bray-Curtis distances and CA, all methods seem to separate the clusters rather well

```{r libSizeCorrZeller, purl = FALSE, fig.cap = "Correlations of row scores with library sizes, results of 1ZellerZeller simulations shown for first two dimensions for simulation with the Dirichlet-multinomial distribution with 10% DA taxa. Red diamonds indicate means.\\label{fig:libCor1}"}
plotCor(resListSimSeqZellerRows, datListZeller, scoreDim = "rows")
```

CoDa struggles with the library sizes, but also RC(M) and Hellinger seems to suffer from it slightly!

The taxa

```{r resListSimSeqZellercols sampleBiplot, purl = FALSE, include = FALSE}
#A sample biplot
randomScatter(resListSimSeqZellerCols)
```

```{r AbsCorrZeller, purl = FALSE, fig.cap = "Correlations of column scores with average relative abundances, results of 1ZellerZeller simulations shown for first two dimensions for simulation with the Dirichlet-multinomial distribution with 10% DA taxa. Red diamonds indicate means.\\label{fig:libCor1}"}
plotCor(resListSimSeqZellerCols, datListZeller, scoreDim = "columns", dataMat = TRUE)
```

Taxa scores of correspondence analysis tend to correlate with taxon abundances in some cases

```{r resListSimSeqZellerEvalsil, purl = FALSE, fig.cap = "Mean silhouettes by method, results of 100 simulations shown for first two dimensions for simulation with the Dirichlet-multinomial distribution with 10% DA taxa. Red diamonds indicate means. A silhouette of 1 point to perfect classification, 0 to borderline cases, -1 to perfect wrong classification \\label{fig:silhouette1}"}
plotSil(resListSimSeqZellerRows, groupFactorZeller)
```

In the non-parametric setting RCM achieves by far the highest silhouette!

```{r resListSimSeqZellerEvaldist, purl = FALSE, fig.cap = "Mean ratios of within to overal distance by method, results of 100 simulations shown for first two dimensions for simulation with the Dirichlet-multinomial distribution with 10% DA taxa. Red diamonds indicate means.  \\label{fig:dist1}"}
distRatioPlot(resListSimSeqZellerRows, groupFactorZeller)
```

In the non-parametric setting, RCM achieves the lowest distance ratios, CA the highest

```{r AbCorrZeller, purl = FALSE, fig.cap = "Correlations of column scores with abundances, results of 100 simulations shown for first two dimensions for simulation with the zero-inflated negative binomial distribution with 10% DA taxa. Red diamonds indicate means. 'Control' is a negative control setting of an independent standard normal variable. \\label{fig:abCorZeller}"}
plotCor(resListSimSeqZellerCols, datListZeller, dataMat = TRUE, scoreDim = "columns")
```

CA suffers from correlation with relative abundances, but not on average.

```{r taxRatZeller, purl = FALSE, fig.cap = "Mean ratio of signal taxa to top 50% non-signal taxa in terms of contribution to cluster separation, results of 100 simulations shown for first two dimensions for simulation with the zero-inflated negative binomial distribution with 10% DA taxa. Red diamonds indicate means. \\label{fig:taxRatZeller}"}
contrZeller = contrTaxaWrap(resListSimSeqZellerRows, resListSimSeqZellerCols, IDlistZeller, groupFactorZeller, upDown = TRUE)
contrPlot(contrZeller)
```

## Constrained RC(M)

\newpage

# Homogeneous linear equation and the singular value decomposition

A homogeneous linear equations has zero as its constant term. As a result, a system of homogeneous linear equations can be written as:

$$Ax = 0$$

with matrix __A__ known and vector __x__ unknown.

# Some comments on analysing microbiome count data

##### Do not log-transform count data

- log-transforms come with arbitrary pseudocounts for the many zeroes
- models based on transformed data are hard to interpret in funcion of the untransformed data
- log-transformation does not stabilize the variance
- Accept non-normality and heteroscedasticity instead of trying to transform them away!
- Log-transformation is more robust though and provides better type I error control (see papers O'Hara 2010, Ives 2015, Warton 2016 and this [post](http://stats.stackexchange.com/questions/114848/negative-binomial-glm-vs-log-transforming-for-count-data-increased-type-i-erro?rq=1) )

##### Do not rely on residuals-based approaches

- Most residual based approaches rely on normally behaved residuals
- Microbiome residuals are skewed to the right 
- The expectations should be larger than or equal to 5 for this approximation to hold, which is not the case for the majority of microbiome entries

##### Use a GLM

- Normalizing to relative or rarefied abundances throws away information on the variance and valuable counts
- Leave the data untransformed and use a proper GLM for counts! This will properly model mean and variance and allows incorporation of covariate information

