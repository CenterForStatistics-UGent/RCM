---
title: "RC(M)-method: Implementation, theory and examples"
author: "Stijn"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    includes:
      in_header: packagesRCM.sty
---

\setcounter{tocdepth}{3}
\tableofcontents


```{r setup, warning=FALSE, message=FALSE, echo=FALSE, purl=TRUE}
WD = "/home/stijn/PhD/Biplots"
knitr::opts_knit$set(root.dir = WD)
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE,  warning = FALSE, message=FALSE, echo=FALSE, eval=TRUE, tidy = TRUE, fig.width = 9, fig.height=6, purl=TRUE, fig.show = "hold", cache.lazy = FALSE)
```

```{r check-install-load-packages, warning=FALSE, message=FALSE, echo=FALSE, purl=TRUE}
# The required package list:
reqpkg <- c("phyloseq", "ggplot2", "reshape2", "parallel")
for(i in reqpkg)
{
  library(i, quietly=TRUE, verbose=FALSE, warn.conflicts=FALSE, character.only=TRUE)
} 
par(pty="s", mar = c(8,2,4,2), cex.main = 0.9) #Make sure the biplots are square!
palStore = palette()
funFiles = dir("R")
funFilesPub = dir("pubFun")
for (i in funFiles) {source(file.path("R",i))}
for (i in funFilesPub) {source(file.path("pubFun",i))};rm(i)
load("/home/stijn/PhD/American Gut/AGphylo.RData")
load("/home/stijn/PhD/Simulations/data/physeqListV13.RData")
```

# Introduction

This document provides an exact description of the algorithm used to fit the RC(M) model augmented with the negative binomial distribution for microbiome data. Existing implementations in R, such as the $rcim()$ function in the $VGAM$ package \cite{VGAM} and $rc()$ in the $logmult$ package \cite{logmult} fail to converge, likely due to numerical reasons. Further it embeds the method in the existing literature, and points out correspondences and differences with existing methods. Finally the method is demonstrated using real datasets.

# Fitting procedure \label{sec:FittingProcedure}

## Inputs

The algorithm requires the following inputs:

 - an n-by-p data matrix __X__, with samples $i$ in the rows and taxa (species, OTUs) $j$ in the columns. Thus $x_{ij}$ is the observed count of taxon $j$ in sample $i$.
 - a required dimension of the solution K. The dimensions are fitted sequentially, so the solutions for the lower dimensions are independent of the final required dimension.
 
 The algorithm allows to supply the following optional inputs
 
  - a n-by-q design matrix __E__ of confounding variables, containing $z-1$ dummies for categorical variables with $z$ levels, and an intercept. The entry $g_{il}$ then represents the value of confounder variable $l$ in sample $i$.
  - a n-by-g design matrix __C__ of constraining variables, containing $z$ dummies for categorical variables with $z$ levels, and no intercept. The entry $c_{iy}$ then represents the value of constraining variable $y$ in sample $i$.

## Trimming

Rows and columns of __X__ with zero counts are trimmed prior to model fitting.

To avoid numerical instability, also taxa below a certain prevalence threshold or with total count lower than a certain fraction of the number of samples $n$ are excluded prior to model fitting.

The default prevalence threshold is 5%, the default fraction of $n$ is 10%.

If a confounder matrix is provided with dummy variables, also discard the taxa that fall below the prevalence and total count fractions mentioned above within each level of the categorical confounding variables, again to avoid overflow.

## Independence model

The independence model is defined as:

$$log(E(x_{ij})) = u_i + u_j$$

The independence model is fitted as follows:

 1. Find starting values $u_{i, init} = log(\sum_{j=1}^p x_{ij})$ and $u_{j, init} = log(\sum_{i=1}^n x_{ij})$ for $u_{i}$  and $u_j$ respectively
 2. Estimate a mean-dispersion trend using the estimateGLMTrendedDisp() function of the _edgeR_ package (version `r packageVersion("edgeR")`), given $u_i$ and $u_i$. This estimate is very insensitive to slight changes in the mean structure and will never be reestimated throughout the whole fitting procedure to save computation time.
 3. Estimate the dispersion parameters $\theta_j$ based on the mean-dispersion trend using empirical Bayes with the estimateGLMTagwiseDispersion() function in the _edgeR_ package, given $u_i$ and $u_j$.
 4. Estimate $u_{i,new}$'s using maximum likelihood (ML), keeping the $\theta_j$'s and $u_j$'s constant.
5. Estimate the $u_{j,new}$'s using maximum likelihood, keeping the $\theta_j$'s and $u_i$'s constant
 6. Check for convergence. If no convergence is reached, repeat steps 3-5. Convergence is assumed when
 
 $$\Big(\frac{1}{n}\sum_{i=1}^n |1-\frac{u_{i,new}}{u_{i,old}}|^a\Big)^{\frac{1}{a}}$$
 
 and
 
 $$\sqrt{\Big(\frac{1}{p}\sum_{j=1}^p (1-\frac{u_{j,new}}{u_{j,old}})^2\Big)}$$
are below a tolerance of 0.001 as tolerance. Once the independence model has converged, the estimates $u_i$ and $u_j$ are kept constant throughout the remainder of the fitting process. All maximum likelihood estimation is under the negative binomial model.
 
## Conditioning on confounders

If a confounder matrix is provided, the effect of the confounders is filtered out by fitting the following model:

 $$log(E(X_{ij})) = u_i + u_j + \sum_{l=1}^q \zeta_{jl}g_{il}$$

with $\zeta_{jl}$ the interaction parameter between taxon $j$ and confounding variable $l$, which is fitted by maximum likelihood. Note that $\forall i: g_{i1}=1$, i.e. the model is fitted with an intercept.

Again this step is performed iteratively by alternating between estimating the $\zeta_{jl}$ parameters and re-estimating the overdispersion parameters as in step. of the independence model. Convergence is then assumed when $\sqrt{\frac{1}{pq} \sum_{l=1}^q \sum_{j=1}^p (\zeta_{jl,new} - \zeta_{jl,old})^2}$ drops below a tolerance level of 0.001.

## Capturing the signal

The steps undertaken so far to model $E(X_{ij})$ are merely fitting a "null" model and will not play a role in the final ordination. The next terms that will be added will capture the biological signal in the data __X__ and will be used for data visualization. This step differs between an _unconstrained_ RC(M) model, that merely uses the data __X__, and _constrained_ analysis, that also incorporates the covariate matrix __C__.

### Unconstrained RC(M)

The unconstrained RC(M) model looks like

$$log(E(X_{ij})) = u_i + u_j + \Big[\sum_{l=1}^q \zeta_{jl}e_{il}\Big] + \sum_{k=1}^K\psi_kr_{ik}s_{jk}$$

with the term between [] being optional.

The unconstrained RC(M) model is fitted as follows:

1. Obtain a singular value decomposition as $R^{-1}(X-E)C^{-1} = U\Sigma V$. This gives us initial values $[r_{i1}^{SVD}, r_{i2}^{SVD},..., r_{iK}^{SVD}] = U_{(1.)}$, the first row of $U$, and correspondingly $\Sigma$ and $V$ give initial values for $\mathbf{\psi}^{SVD}$ and $\mathbf{s}^{SVD}$. For these initial values we still need to ensure that the (weighted) variances equal 1, and transfer these weights to the importance parameters $\mathbf{\psi}^{SVD}$ so we set

$$\psi_k^{init} = \psi_k^{SVD}\sum_{i=1}^n\big({r^{SVD}_{ik}}\big)^2 \sum_{j=1}^p\big(z_js^{SVD}_{jk}\big)^2$$

$$r_{ik}^{init} = \big(\frac{r_{ik}^{SVD}}{\sum_{i=1}^n{\big({r^{SVD}_{ik}}\big)^2}}\big)^{1/2}$$

and

$$s_{jk}^{init} = \big(\frac{z_js_{jk}^{SVD}}{\sum_{j=1}^p{\big(s^{SVD}_{jk}}z_j\big)^2}\big)^{1/2}$$

with $z_j = exp(u_j)$, see below for an extended discussion on the weights.

2. For all dimensions $k$ starting from 1 to K :
a) Estimate the dispersions $\theta_j$ using empirical Bayes as before
b) Estimate the importance parameter $\psi_k$ by full ML, keeping the sample and taxon scores and overdispersions fixed.
c) Estimate the sample scores $r_ik$ by restricted ML, keeping the taxon scores, overdispersions and importance parameters fixed. Lagrangian multipliers are used to ensure that

$$ \sum_{i=1}^n r_{ik} = 0$$
and 

$$ \sum_{i=1}^n r_{ik}r_{ik'} = \delta_{kk'}$$
with $\delta$ the Kronecker delta.

c) Estimate the taxon scores $s_jk$ by restricted ML, keeping the sample scores, overdispersions and importance parameters fixed. Lagrange multipliers are used to ensure that

$$ \sum_{j=1}^p z_js_{jk} = 0$$

and 

$$ \sum_{j=1}^p z_js_{jk}s_{jk'} = \delta_{kk'}$$
d) Check for convergence of dimension $k$. If no convergence reached, repeat steps a-c, otherwise fit the next dimension conditional on the previous ones. Convergence for dimension $k$ is assumed when 

$$\Big|1-\frac{\psi_k^{new}}{\psi_k^{old}}\Big| < 0.001$$
 
 AND
 
 $$\Big(\sqrt{\sum_{i=1}^n (1-\frac{r^{new}_{ik}}{r^{old}_{ik}})^2} < 0.001 \Big) \cup \Big( \sqrt{\sum_{j=1}^p (1-\frac{s^{new}_{jk}}{s^{old}_{jk}})^2} < 0.001 \Big)$$

i.e. an infinity norm is used for the psis, and an L2-norm for the sample and taxon scores.

The Lagrangian parameters are stored to be used as starting values in the next iteration to speed up the computation.

### Constrained RC(M)

The constrained RC(M) model looks like

$$log(E(X_{ij})) = u_i + u_j + \Big[\sum_{l=1}^q \zeta_{jl}e_{il}\Big] + \sum_{k=1}^K\psi_kf_{jk}(\boldsymbol{\alpha}_k^t\mathbf{C}_i)$$

with the term between [] being optional. For this model four components need to be fitted iteratively:

 - $\theta_k$, the overdispersion parameter as before
 - $\psi_k$ the importance parameter as in the unconstrained model
 - $\boldsymbol{\alpha}_k$, the environmental gradient, under the restriction that
 $$\boldsymbol{\alpha}^t_k\boldsymbol{\alpha}_k' = \delta_{kk'}$$
 - $f_{jk}$, the species specific response function. This can be parametric (polynomial in practice) or non-parametric.
 
 The fitting of the constrained RC(M) model proceeds as follows:
 
 1. Standardize the covariate matrix. To render the values of the continuous variables in the environmental gradient comparable, it is clear that they need to be centered and scaled prior to model fitting, as in PCA. This means that their corresponding elements of $\boldsymbol{\alpha}$ represent the contribution to the environmental score of one standard deviation away from the mean of this variable (in case of linear response functions). A perfect quantitative comparison to the magnitude of the values of the paramaters of the dummies of the categorical variables will never be possible. In our case, with 0-1 dummy coding, equal parameters for a dummy and a continuous variable imply that this level of the categorical variable contributes as much to the environmental score as one standard deviation away from the overall mean of the continuous variable.
 
 2. Starting values for $\boldsymbol{\alpha}$ are obtained from a constrained correspondence analysis by the $cca()$ function in the $vegan$ package. Next they are normalized to fulfil the $\boldsymbol{\alpha}^t_k\boldsymbol{\alpha}_k = 1$ requirement. Starting values for $\boldsymbol{\psi}$ are the eigenvalues of the constrained correspondence analysis. For the response functions no starting values are calculated.
 3. For all dimensions $k$ starting from 1 to K :
 a) Estimate the overdispersions by empircal Bayes as before.
 b) If the response function is parametric, estimate the importance parameter $\psi_k$ by full ML
 c) Estimate the response functions $f_{jk}$ by full ML. 
 For parametric, polynomial response functions this entails estimating a vector of parameters $\Big(\beta_{0jk},..., \beta_{vjk} \Big)$ with v the degree of the polynomial. After fitting, normalize these parameters to
 
 $$\beta_{wjk}^{new} = \frac{\beta_{wjk}}{\sqrt{\sum_{j=1}^p\beta_{wjk}^2}}$$
 with $w$ a degree indicator. This assures the normalization $\boldsymbol{\beta}_{wk}^t\boldsymbol{\beta}_{wk} = 1$.
 
 For non-parametric response functions this relies on cubic splines as defined by the $bs()$ function of the $VGAM$ package. Also for non-parametric response functions the area between the response functions and the x-axis over the range of environmental scores $\boldsymbol{\alpha}_k^t\mathbf{C}_i$ is calculated as $a_{jk}$. The importance parameter psi is now defined to be 
 
 $$\psi_k = \sqrt{\sum_{j=1}^p a_{jk}^2z_j}$$
 Analogously, estimate general response functions $f_k$ ignoring species labels.
 
 d) Estimate the environmental gradient $\boldsymbol{\alpha}_k^t$ by maximizing the likelihood ratio
 
 $$LR(\boldsymbol{\alpha}_k) = log \frac{\prod_{i=1}^n \prod_{j=1}^p g_{NB}(x_{ij};\boldsymbol{\alpha}^T_k \mathbf{c}_i,f_{jk}, \theta_j, \psi_k) }{\prod_{i=1}^n \prod_{j=1}^p g_{NB}(x_{ij};\boldsymbol{\alpha}^T_k \mathbf{c}_i,f_k, \theta_j, \psi_k) }$$
 with $g_{NB}$ the density function of the negative binomial distribution. This approach encourages maximal niche separation between the species. The environmental gradient is restricted such that the coefficients of dummies of the same categorical variable sum to zero, again using Lagrange multipliers. This is convenient for later plotting and also assures avoids dependence of the solution on the choice of reference level, in the light of the normalization step in e).
 
However, the separated niche concept is not accepted by all ecologists. If niches are really maximally separated, how can species co-occur then? An alternative option is therefore to estimate $\alpha$ through full ML, maximizing only the numerator in the previous equation. Surprisingly, the solutions of both approaches are very similar, although not exactly identical. The full ML solution is also much faster.

 e) Normalize the environmental gradient to unit variance
 f) Check for convergence. If no convergence reached, repeat steps a-e, otherwise fit the next dimension conditional on the previous ones. Convergence is assumed when
 
 $$\Big|1-\frac{\psi_k^{new}}{\psi_k^{old}}\Big| < 0.001$$
 
 AND
 
 $$\sqrt{\sum_{l=1}^d (1-\frac{\alpha_{lk}^{new}}{\alpha_{lk}^{old}})^2} < 0.001$$
 For parametric response functions there is an additional requirement that 
 
 $$\forall w: \sqrt{\sum_{j=1}^p \Big(1-\frac{\beta_{jk}^{new}}{\beta_{jk}^{old}}\Big)^2} < 0.001$$
 
# Explanatory notes

In this section we give some explanation regarding the particularities of this fitting procedure with respect to the original method by Goodman \cite{Goodman1979}.

## Estimating the independence model

An independence model for a contingency table is basically a marginal model. Therefor the most evident, model free way to estimate the independence model may be simply through sample and taxon sums, namely $u_i = log(x_{i.})$ with $x_{i.} = \sum_{j=1}^p x_{ij}$ and $u_j = log(\frac{x_{.j}}{x_{..}})$ with $x_{.j} = \sum_{i=1}^n x_{ij}$ and $x_{..} = \sum_{j=1}^p \sum_{i=1}^n x_{ij}$.

However, the library sizes $x_{i.}$ do not correspond to the maximum likelihood estimate of $u_i$ under the negative binomial model. As a result the first dimensional row scores $r_{1i}$ tried to correct for this discrepancy and became related (linearly correlated) to the library sizes. This effect of sequencing depth on the sample ordination is something we want to avoid absolutely.

Therefor we estimate the $u_i$'s, $u_j$'s iteratively using maximum likelihood, which also implies dispersion estimation as outlined above.

In practice, the marginal sums differ more from the MLE for the library sizes than for the taxon abundances. We can think of the following mathematical explanation: when calculating library sizes or abundances by row and column sums, each observations receives the same weight. However, when we estimate the margins through ML, we solve the following score equations:

$$\frac{\partial L(\mathbf{X}_i|\mathbf{u}_{i}, \mathbf{u}_j, \boldsymbol{\theta})}{\partial u_{i}} = \sum_{j=1}^p \frac{x_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}}$$

$$\frac{\partial L(\mathbf{X}_i|\mathbf{u}_{i}, \mathbf{u}_j, \boldsymbol{\theta})}{\partial u_{j}} = \sum_{i=1}^n \frac{x_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}}.$$

Note that since we are estimating offsets the value of the regressor is 1. In this case the difference of $x_{ij}$ with the expected value $\mu_{ij}$ is weighted by a factor $\frac{1}{1+\frac{\mu_{ij}}{\theta_j}} = \frac{\theta_j}{\theta_j+\mu_{ij}}$. When estimating $u_j$, $\theta$ is a constant but when estimating $u_i$ it is different for every observation $y_{ij}$. Hence the weights put on every observation differ much more when estimating the sample offsets than when estimating the taxon offsets. That is the reason why the MLE differs more from the marginal sum for the library size than for the abundances.

Note also that when there is a very large overdispersion for a taxon _j_ ($\theta_j$ small), its observations carry little information and their weights are small in the calculation of the library sizes. However, when there is very little overdispersion ($\theta_j \rightarrow \infty$), the weights of the components of the score function equal 1, as with Poisson regression. It is thus not surprising that the MLEs of the Poisson regression are equal to the estimators based on the marginal sums. This means that the larger and the more diverse the overdispersion estimates are, the more the MLEs under the negative binomial model will depart from the marginal sums. Finally, we see that departures from the mean $\mu_{ij}$ are weighted down for large values of $\mu_{ij}$, acknowledging the fact that the variance increases faster than linear with the mean in the negative binomial model.

## The choice of normalization weights

Constraints are needed to render this model identifiable, since we have three unknown regressors ($r_{ik}, s_{jk} and psi_k$) instead of one. We do restrict the importance parameter $psi_k$ to be positive, and center the (weighted) row and column scores around 0, which is a useful property for the biplot and is also the case for correspondence analysis. In addition the row and column scores are restricted to have (weighted) variance 1. $psi_k$ is the only parameter that can grow in size without restriction. As a result it will automatically serve as a measure of importance of the departure from independence in that direction, since all scores of the dimensions are normalized. This is also the case for correspondence analysis. Thridly, the scores of the different dimensions are orthogonal, so the solutions in different dimensions are linearly independent.

Centering:

$$\sum_{i=1}^nw_ir_{ki} = 0$$

Normalization($k=k'$) and orthogonality ($k \neq k'$)

$$\sum_{i=1}^nw_ir_{ki}r_{k'i} = \delta_{kk'}$$

$$\sum_{j=1}^pz_js_{kj} = 0$$

$$\sum_{j=1}^pz_js_{kj}s_{k'j} = \delta_{kk'}$$
Hereby $w_i$ and $z_j$ are row and column weigths. But which weights $w_i$ and $z_j$ should we use? Goodman proposes to use $w_i = x_{i.}$ and $z_j = x_{.j}$ thus uses _weighted_ constraints, to retain the relationship with correspondence analysis \cite{Goodman1979}. Others recommend using uniform weights not to let the marginal distribution affect the model fit \cite{Becker1989}.

To make the correct choice one should remember that the weights can be regarded as probability density functions, they represent the likelihood of sampling a certain sample or taxon from the population. On the population level we could say that

$$E(w_iX_{ik}) = 0$$,

i.e. the average row score on the population level is zero. This is a useful restriction to make sure that the biplot is centered around zero. Analogously we want that

$$E(w_iX_{ik}X_{ik'}) = \delta_{kk'}$$

and accordingly for the column scores:

$$E(z_j S_{jk}) = 0$$

$$E(z_j S_{jk} S_{jk'}) = \delta_{kk'}$$

For the microbiome case, every subject comes from the same population under the null-hypothesis and all subjects are thus eqaully likely to be sampled and have the same importance. The library sizes as a purely technical artefact, unrelated to the biological importance of the subject. Consequently we use uniform row weights $w_i = 1/n$ (or $w_i = 1$, the magnitude is of no importance since the associated $\psi_k$ will grow or shrink accordingly). However, some taxa are more prevalent in the population than others. We want the average column scores on the population level to be centered around zero, have variance one and be orthogonal. That is why we set $z_j = exp(u_j)$; because the more abundant species are in fact more abundant in the population as a whole (as opposed to samples with a large library size), it makes sense to use a marginal weighting scheme for the column scores. The weights $z_j$ are derived from the independence model, as explained in the previous paragraph.

For the parameters of the parametric response functions we use uniform weights for normalization, because the use of $exp(u_j)$ as weights for the normalization leads to very extreme solutions. Likely this is because the parameters are not centered

## Shape of the response function

Note that our definition of the response function differs from the response function most common one \cite{terBraak1986, Zhu205, Yee2006}: it no longer models mean abundance but mean departure from independence.

A linear response function may be most appropriate for problems with __short gradients__ i.e. whereby the difference in observed environmental variables is too short to devine more than an increase or decrease in departure from independence. Also in this case it is easy to interpret the effect of each of the environmental variables on the departure from independence. As so often in statistics, the linearity assumption may not be realistic, but renders models that are easy to interpret ("All models are wrong, but some models are useful"). 

For problems with __long gradients__, i.e. whereby species' departures of independence rise and drop (or drop and rise) within the score of the observed environmental scores, quadratic response functions may be more appropriate. This corresponds e.g. with the scenario whereby a species' abundance does bot depart a lot from independence for extreme values of the environmental score, and but does depart the independence model for an intermediate value of the environmental score, e.g. it thrives in this environment and its abundance is higher than under independence. In essence this is the same as the approach \cite{Zhu2005}, only now the baseline is the independence model rather than 0. Every taxon thereby has its own baseline, the response function models departures from this baseline. 

Note that we usually do not choose the ranges of the environmental variables or scores, so that we cannot guarantee a range long enough for the quadratic response function to be appropriate. Note also that what is a long gradient for one species, may be a short one for another.

Even though the quadratic response function can be fitted, it may still be pointless if the maximum lies outside the range of the observed values for the environmental score. The peak location would then merely be an extrapolation, and a linear response function may be preferable. Even though the fit is worse than for the parabolic curve, it represents more truthfully the way the species reacts to the given values of the environmental gradient. Therefore we also provide a "dynamic"-option for the response function, whereby initially a quadratic model is fitted but discarded in favour of a linear one if the optimum lies outside of the range of observed environmental scores. For plotting it is not very attractive to have different shapes of the response function for the same taxon in different dimensions though.

If the user is unsure and has enough data, he may use non-parametric response functions. This may improve the sample and covariate ordination, although the result for the species becomes harder to interpret. This approach makes no assumptions on the shape of the response function (apart from a certain smoothness) and may be interesting if one wants to study individual species' response functions.

 All in all we see a trade-off between quality of the ordination of the samples versus interpretability of the role of the taxa.
 
## Relationship between unconstrained and constrained RC(M)

If we fit the constrained method with only one covariate, namely a factor with a unique level for every sample, and all $\beta_{0j}$ (and $\beta_{2j}$) equal to zero , we expect to find the same solution as the unconstrained case. This means we do not constrain the ordination at all in this case and $\beta_{1jk} = s_{jk}$ and $\boldsymbol{\alpha}_{k}^t\mathbf{C}_i = r_{ik}$.

## Comparison with existing methods



# Plotting the RC(M) ordination

## Unconstrained RC(M)

To plot the unconstrained sample ordination, e.g. in the first two dimensions, plot $\psi_1r_{1i}$ vs $\psi_2r_{2i}$, preferably as dots. All weight of the importance parameters $\psi_k$ is alloted to the samples, which means that the distances between sample points can be intepreted as optimal representations of between-sample distances in lower dimension. More weight is added to differences in scores in important dimensions.

To show the role of the taxa in the ordination, add taxon scores $s_{1j}$ vs $s_{2j}$ as arrows to make a biplot. This assures that the orthogonal projection of the vector ($\psi_1r_{1i}$, $\psi_2r_{2i}$) on ($s_{1j}$, $s_{2j}$) equals $cos (\eta) ||(\psi_1r_{1i}, \psi_2r_{2i})|| = \frac{(s_{1j}, s_{2j})^t(\psi_1r_{1i}, \psi_2r_{2i})}{||(s_{1j}, s_{2j})||}$ with $\eta$ the angle between the two vectors. This projection is thus proportional to the departure from independence in the first two dimensions combined, for taxon $j$ in sample $i$. The larger the entries of the species and sample scores (the scaling between these two sets is arbitrary, we usually choose them in the same order of magnitude) and the smaller the angle, the larger the departure of this taxon in this sample.

Distances between taxon arrows are meaningless in this representation.

## Constrained RC(M)

### Linear response functions

For a constrained ordination with linear response functions, plot e.g. in the first two dimensions the sample scores $(\psi_1\boldsymbol{\alpha}_1^t\mathbf{C}_i, \psi_2\boldsymbol{\alpha}_2^t\mathbf{C}_i)$, preferably as dots. Again this ordination optimally represents distances between samples in low dimension that can be explained by the environmental variables.

Taxon arrows can be added to make a biplot with their origin in $(-\frac{\beta_{0j1}}{\beta_{1j1}}, -\frac{\beta_{0j2}}{\beta_{1j2}})$. This represents the combination of values of the environmental scores in the first two dimensions were a sample would have no expected departure from independence for this taxon $j$. The arrow then extends in the direction of $(\beta_{1j1}, \beta_{1j2})$. The orthogonal projection of this taxon vector onto the sample scores (which depart from the origin), is then equal to $\frac{(\beta_{1j1}, \beta_{1j2})^t(\psi_1\boldsymbol{\alpha}_1^t\mathbf{C}_i, \psi_2\boldsymbol{\alpha}_2^t\mathbf{C}_i)}{||(\beta_{1j1}, \beta_{1j2})||}$, i.e. the departure from uniformity of taxon $j$ that is due to the environmental score from sample $i$.

In order to make a triplot, labels for the environmental variables are then added according to the loadings of $\boldsymbol{\alpha}_k$. The projection of $\boldsymbol{\alpha}_{y}$ onto $\boldsymbol{\beta}_{1j}$ then reflects the sensitivity of the expected abundance of taxon $j$ to changes in variable $y$. For the categorical variables all levels are shown on the plot, there are no hidden reference level. The continuous variables represent changes of the magnitude of one standard deviation. Comparison of the magnitude of the loadings of continuous and categorical variables is inherently difficult. 

There is no interpretation available for the relative position of sample and variable vectors. This is because the environmental gradient $\boldsymbol{\alpha}_k$ projects the environmental variables of a sample $i$, $\mathbf{C}_i$ onto a single scalar $h_{ik}$, the environmental score. Many combinations of variables $\mathbf{C}_i$ can lead to the same environmental score. Also Distances between taxon arrows are meaningless in this representation.

### Quadratic response function

For a quadratic response function the samples are ordered as for the linear one.

The taxa are plotted as dots at the locations $(-\frac{\beta_{2j1}}{2\beta_{3j1}}, -\frac{\beta_{2j2}}{2\beta_{3j2}})$ of maximal departure from independence. The convexity $\beta_{3jk}<0$ or concavity $\beta_{3jk}>0$ in each dimension can be shown e.g. by a colour code. Note that cases like $\beta_{3j1}<0<\beta_{3j1}$ can occur, which greatly complicates the interpretation. Further, ellipses can be drawn around the taxon points to indicate the steepness of the response functions. we choose to draw ellipses connecting the values of the envrionmental score at which the response functions are at 95% of their peaks.

The variables can be added as in the linear case to show how they contribute to the environmental gradient.

### Non-parametric response functions

For non-parametric response functions the samples and variables can be plotted as before, but there is no clear cut way to add the role of the species. The interpretation of the plot will thus mainly focus on the sample ordination.

If the researcher is interested in the response functions of the species, they can be plotted in one dimension in function of the environmental score for many species combined. The environmental gradient of this dimension can be added as a reference to show which variables constitute the gradient.

# Assessing the model quality

Even though it is only an explorative visualization and conclusions may still be valid in the face of slight violation of its assumptions, we need tools to evaluate the goodness of fit of the RC(M) model and the validity of its assumptions.

## Parsimony

An unconstrained RC(M) model of dimension $k$ on a n-by-p data matrix requires estimation of p (abundances) + n (library sizes) + p (dispersions) + kxp (column scores) + kxn (row scores) + k (importance parameters) = (k+2)p + (k+1)n + k parameters out of np entries. 4k + k(k-1) restrictions have been imposed, so the final model is still very parsimonious for n and p in the hundreds.

An constrained RC(M) model with linear response functions of dimension $k$ on a n-by-p data matrix and with a n-by-d covariate matrix requires estimation of p (abundances) + n (library sizes) + p (dispersions) + 2kp (response function parameters) + kd (environmental gradient loadings) + k (importance parameters) = (2k+1)p + kd + k parameters out of np entries. 3k + k(k-1) restrictions have been imposed.

## Importance of the dimension

A very natural question is to know how much more important the higher dimensions are in explaining the present variability then the lower dimensions. Also we would want a measure of how much of the variability has been explained in lower dimensions. In principal components analysis (PCA) there is the concept of "percentage variance explained", in correspondence analysis (CA) the total inertia is known and thus also the percentage of variance captured by the lower dimensions. Still the value of these expression is questionable, since they only yield a fraction of _total_ variability. However, part of the total variability is noise, and one does not know which percentage of the _signal_ the higher dimensions explain.

Since for the RC(M) model for computational reasons only a couple of dimensions are fitted, there is no measure of total variability. One could compare the likelihood of the model to that of the saturated model, but this is not very informative on the count scale.

The best measure of differences in importance between the dimensions are the importance parameters $\psi_k$. Since all other parameters in both the unconstrained and constrained variables are normalized, these are the only ones that can grow in magnitude to give more weight to the departures of independence in their dimension. This is very similar to the eigenvalues in PCA or the singular values in correspondence analysis, whose size is proportional to the importance of the corresponding dimension. In both the unconstrained and the constrained case it may occur that the magnitude of the $\psi_k$'s is not always monotonically decreasing with the dimensions. However, for skewed distributions as the negative binomial this need not be surprising: the strongest improvement in likelihood is not always achieved by the greatest change in the mean, especially in the presence of nuisance parameters.

The plotting procedure described above will alot all weight of the importance parameter to the samples, thus automatically weighting for the importance of the dimensions in the sample ordination.

## Detecting lack of fit

In our 2D or 3D representation, some samples and taxa may be very well represented, but others not. This may be because of a lack of fit of the negative binomial distribution _tout court_, or because its departure from independence cannot be represented in lower dimension. Anyhow, we provide tools to detect lack of fit.

### Deviance residuals

The deviance residuals $d_{ij}$ of the negative binomial distribution are defined as (see M. L. Zwilling, “Negative Binomial Regression,” The Mathematica Journal, 2013):

\[
d_{ij} = 
\begin{cases}
sgn(X_{ij}-\mu_{ij})\sqrt{2\Big(X_{ij}ln(\frac{X_{ij}}{\mu_{ij}})-(X_{ij}+\frac{1}{\phi_j})ln(\frac{1+X_{ij}\phi_j}{1+\mu_{ij}\phi_j})\Big)} \text{ if } X_{ij} > 0\\
sgn(X_{ij}-\mu_{ij})\sqrt{\frac{2}{\phi_j}ln(1+\phi_j\mu_{ij})} \text{ if } X_{ij} = 0
\end{cases}
\]

Their sum of squares equals the total deviance per sample or taxon. For the unconstrained we can visually represent this by colour codes. In the constrained case with parametric response functions, plotting the deviances in function of the environmental gradient can reveal patterns and thus lack of fit to the linearity assumption.

We can do this for the taxa that respond strongest to the environmental gradient, and make residual plots with deviance or Pearson residuals. An alternative is to try to detect systematic trends through series of positive or negative residuals using Ward and Wolfowitz' runs test, and plot the taxa with the largest test statistic.

## Identifying influential observations

Since we have explicitly expressed all score functions, we can easily identify influential observations using _influence functions_. They represent the influence a certain observation has on parameter, keeping the other sorts of parameters fixed. Because of the iterative algorithm this latter assumption is incorrect, but the influence functions might still harbour interesting information.

For maximum likelihood estimation the influence function $\psi(\theta| f, x)$ of a parameter $\theta$ for a distribution $f$ and data x is defined as:

$$\psi(\theta| f, x) = -S_f(\theta|x) E\big(I(\theta, \theta|x)\big)^{-1}  $$
 with $S_f(\theta|x)$ the score function and $E\big(I(\theta, \theta|x)\big)$ the expected Fisher information matrix.

For the unconstrained case in a scenario without outliers the influence functions may not yield very surprising results on the level of the plot, observations mainly have influence on their own row and column scores. Coupling through the constraints is rather weak. It may however help to identify outlying abundances in case of outlying row- or column scores.

For the constrained case it may be enlightning to see which samples (and taxa) affect the estimation of the environmental gradient most.

# Real data examples

We apply the RC(M) method to a number of real datasets to illustrate its functionality and prove that it yields biologically valid results.

## Human microbiome project

The Human Microbiome Project (HMP) aimed to characterize the healthy human microbiome.

```{r RC(M)_NB real datasets, purl = FALSE}
realNames = c("AGP", "Hard.palate", "Buccal.mucosa", "Anterior.nares", "Left.Antecubital.fossa", "Posterior.fornix",  "Mid.vagina", "Left.Retroauricular.crease", "Vaginal.introitus", "Stool", "Hard.palate")
if(!file.exists(file = "resListRCMagphmp.RData")){
resListRCM = lapply(c(AGphylo, physeqListV13[realNames[-1]]),RCM, k = 3)
names(resListRCM) = realNames
save(resListRCM, file = "resListRCMagphmp.RData")
} else {load(file = "resListRCMagphmp.RData")}

SClevels  = unique(c(unlist(sapply(physeqListV13, function(x){
  levels(get_variable("RUNCENTER",physeq=x))
}))))
#Give similar colours for the same sequencing center
cols = c("cadetblue","cyan", "darkblue","blue","magenta","orange","yellow","brown","black",  "grey80","grey70","pink") 
palette(cols)
```

```{r HMPplot, purl = FALSE, fig.cap="Biplot of the RC(M) ordination of the Anterior nares dataset of the HMP. Colours indicate sequencing center. Sequencing center clearly affects the obtained microbiome compositions. \\label{fig:samHMP}"}
plot(resListRCM[["Anterior.nares"]], samColour ="RUNCENTER", colLegend = "Sequencing center")
```

Since we know that sequencing facility affects the outcome of the sequencing assay and one is not interested in visualizing this technical variability we can condition out this variable. We condition only on the primary sequencing center.

```{r HMP filter out variable effect, purl=FALSE, fig.cap="Biplot of the RC(M) ordination of the HMP Anterior nares dataset after conditioning. Colours indicate sequencing center. The effect of sequencing center has been largely filtered out. \\label{fig:samHMP}"}
if(!file.exists(file="/home/stijn/PhD/Biplots/filtResHMP.RData")){
  K=2
nleqslv.control = list()
#Group the runcenters together

RCMhmpFilt = lapply(physeqListV13, function(x){
  runcenters = data.frame(sample_data(x))[ "RUNCENTER"]
runcenters[["RUNCENTER"]][runcenters[["RUNCENTER"]] %in% c("BCM","BI,BCM","BCM,BI","BCM,WUGC", "BCM,JCVI")] = "BCM"
runcenters[["RUNCENTER"]][runcenters[["RUNCENTER"]] %in% c("JCVI","JCVI,BI","JCVI,WUGC")] = "JCVI"
runcenters[["RUNCENTER"]][runcenters[["RUNCENTER"]] %in% c("WUGC,JCVI","WUGC","WUGC,BCM")] = "WUGC"
sample_data(x) = data.frame(sample_data(x) ,RUNCENTER2 = runcenters[[1]])
x})
filtRes = lapply(RCMhmpFilt, function(x){
  RCM(x, distribution="NB", k = K, nleqslv.control= nleqslv.control, maxItOut=5e3, prevCutOff=0.01, colWeights="marginal", rowWeights = "uniform", marginEst = "MLE", round=TRUE, confounders = "RUNCENTER2")
  })
save(filtRes, file="/home/stijn/PhD/Biplots/filtResHMP.RData")
} else {load(file="/home/stijn/PhD/Biplots/filtResHMP.RData")}

plot(filtRes[["Anterior.nares"]], samColour ="RUNCENTER", colLegend = "Sequencing center")
```

\clearpage

## The American gut project

The American gut project consits of stool samples sampled by volunteers at home, together with their answers to a questionnaire. Since they are sampled at home the variability is expected to be large.

```{r AGPplots, purl = FALSE}
plot(resListRCM[["AGP"]])
```

The AGP dataset is indeed too noisy to find much of a signal

\clearpage

## Zeller et al. (2014)

#### Unconstrained RC(M)

The Zeller data are obtained from a study on colorectal cancer in cancer patients and healthy controls \cite{Zeller2014}. Patient covariates recorded were age, gender, BMI, cancer diagnosis(healthy, small adenoma or cancer) and country(France or Germany).

```{r load zeller RC(M) NB results, purl = FALSE, include = FALSE}
load("/home/stijn/PhD/Simulations/data/zellerData.RData")
zellers = c("zellerMeta","zeller16S")
if(!file.exists("zellerSphyRCM.RData")){
zellerSphyRCM = RCM(zellerSphy, k=3, round = TRUE)
save(zellerSphyRCM, file="zellerSphyRCM.RData")
} else {load("zellerSphyRCM.RData")}

if(!file.exists("zellerMphyRCM.RData")){
zellerMphyRCM = RCM(zellerMphy, k=3, round = TRUE, prevCutOff = 0.05, minFraction = 0.15) # Less than one minute! More trimming needed here
save(zellerMphyRCM, file="zellerMphyRCM.RData")
} else {load("zellerMphyRCM.RData")}

zellerRCM  =list(zellerMphyRCM, zellerSphyRCM)
names(zellerRCM) = zellers
# sapply(zellerRCM,function(x){x$converged}) # All converged
# sapply(zellerRCM,function(x){x$runtime})
# sapply(zellerRCM,function(x){x$iter})
```

```{r plotZellerSphyDiagnosis, purl = FALSE}
plot.RCM(zellerSphyRCM, samColour = "Diagnosis", samShape = "Country")
```

We see some gradient in function of cancer diagnosis, but there is still a lot of remaining variability.

```{r plotZellerMphy, purl = FALSE }
plot.RCM(zellerMphyRCM, samColour = "Diagnosis", samShape = "Country")
```

For the metagenomics data we do not see a trend in function of the diagnosis.

\clearpage

#### Constrained RC(M)

In the constrained analysis we use all the available covariates to construct the environmental gradient.

```{r zellerConstrained, purl =FALSE}
if(!file.exists("zellerSphyRCMconstr.RData")){
zellerSphyRCMconstrLin = RCM(zellerSphy, k=2, round = TRUE, covariates = c("Age","Gender","BMI","Country", "Diagnosis"), responseFun = "linear") # Use smallest possible cndtol!
zellerSphyRCMconstrQuad = RCM(zellerSphy, k=2, round = TRUE, covariates = c("Age","Gender","BMI","Country", "Diagnosis"), responseFun = "quadratic")
zellerSphyRCMconstrNonParam = RCM(zellerSphy, k=2, round = TRUE, covariates = c("Age","Gender","BMI","Country", "Diagnosis"), responseFun = "nonparametric")
save(zellerSphyRCMconstrLin, zellerSphyRCMconstrQuad, zellerSphyRCMconstrNonParam, file="zellerSphyRCMconstr.RData")
} else {load("zellerSphyRCMconstr.RData")}
```

```{r ZellerConstrainedBiplot, purl=FALSE, eval = FALSE, fig.cap="Triplot of constrained RC(M) analysis on zeller data with linear response functions. Cancer diagnosis and country appear to be the most important variables. \\label{fig:triZellerDiagnosis}"}
plot.RCM(zellerSphyRCMconstrLin, samColour = "Diagnosis", samShape = "Country")
```

```{r ZellerConstrainedBiplotNonParam, purl=FALSE, eval = FALSE, fig.cap="Triplot of constrained RC(M) analysis on Zeller data with non-parametric response functions. The environmental gradient is similar as to the linear case, nly age is more important according to this model. \\label{fig:triZellerDiagnosisNP}"}
plot.RCM(zellerSphyRCMconstrNonParam, samColour = "Diagnosis", samShape = "Country")
```

\clearpage

## Turnbaugh et al. (2009)

In this study the fecal microbiome of monozygotic and dizygotic twins was characterized \cite{Turnbaugh2009}. On of the aims was to find a link with obesity.

```{r turnTwins, eval = FALSE, purl = FALSE}
if(!file.exists("turntwinsRCm.RData")){
  load(file = "/home/stijn/PhD/Datasets/TurnbaughObeseTwins/TurnTwins.RData")
turntwinsRCm = RCM(TurnTwins, k=3)
save(turntwinsRCm, file = "turntwinsRCm.RData")
} else {load("turntwinsRCm.RData")}
plot.RCM(turntwinsRCm, samColour = "Status_p", colLegend = "Weight status")
rm(turntwinsRCm)
```

With such small datasets it is hard to draw conclusions.

\clearpage

## Turnbaugh et al. (2009b)

In this group, 15 gnotobiotic mice were inoculated with human feces, and for one group the diet was switched to a Western diet after one month \cite{Turnbaugh200b9}. Then a second generation of mice was inoculated with cecal samples from the previous groups, and here also diet was varied. The variables recorded were:

- Diet: current diet
- Generation_p: "Recipient1" if second generation, otherwise diet of cecal samples with which they were inoculated
- Time: age of mouse at sampling
- DGS: Diet and previous diet
- DTG: combines sampling diet, time, and previous generation (if any) 

```{r turnmice, purl = FALSE}
if(!file.exists("turnMiceRCM.RData")){
  load(file = "/home/stijn/PhD/Datasets/Humanized mouse/HumMicePhy.RData")
HumMiceRCM = RCM(HumMicePhy, k=3)
HumMiceRCMlin = RCM(HumMicePhy, k=3, covariates = c("Diet","Generation_p","Time"))
gommsHum = gommsPhy(HumMicePhy)
save(HumMiceRCM, gommsHum, file = "turnMiceRCM.RData")
} else {load("turnMiceRCM.RData")}
```

```{r humMicePlot, purl = FALSE}
plot.RCM(HumMiceRCM) # Two or three distinct groups
```

This dataset appears to have a very strong signal, we can distinguish three or four clusters.

```{r humMicePlotSecondLook, purl = FALSE}
plot.RCM(HumMiceRCM, samColour = "Diet", samShape = "Generation_p")
```

It is clear that current diet is the dominant variable in the first dimension.

```{r humMicePlotSecondLookTime, purl = FALSE}
plot.RCM(HumMiceRCM, samColour = "Time", samShape = "Generation_p")
```

Also the older group of generation 1 mice is different, it separates in the second dimension.

\clearpage

## Kostic et al. (2014)

This is a study on the microbiome of colorectal cancer in humans \cite{Kostic2012}. Nine tumor patients were matched with 9 healthy patients, samples were taken repeatedly. The researchers find a enrichment of Fusobacteria in the tumor and a depletion of Bacteroidetes and Firmicutes.

```{r load Kostic, purl=FALSE}
if(!file.exists("Kostic.RData")){
filepath = system.file("extdata", "study_1457_split_library_seqs_and_mapping.zip", package="phyloseq")
kostic = microbio_me_qiime(filepath)
save(kostic, file = "Kostic.RData")
} else {load("Kostic.RData")}
```

For the constrained analysis we use the variables

- NECROSIS_PERCENT
- AGE
- NORMAL_EQUIVALENT_PERCENT
- FIBROBLAST_AND_VESSEL_PERCENT
- TREATMENT
- CEA (Carcinoembryonic antigen)
- SEX
- COUNTRY
- CHEMOTHERAPY
- HISTOLOGIC_GRADE
- TUMOR_PERCENT
- RADIATION_THERAPY
- INFLAMMATION_PERCENT
- PC3 (a prostate cancer cell line)

For radiation therapy and chemotherapy, "None" and "No" were pooled. For Necrosis percent, normal equivalent percent, tumor percent, inflammation percent and CEA, "None" was set to 0, for age to the average age. Necrosis percent, age, normal equivalent, fibroblast and vessel percent, and cea were treated as continuous variables

```{r KosticRCM}, purl = FALSE}
kostVars = c("NECROSIS_PERCENT_CONT","AGE_CONT","NORMAL_EQUIVALENT_PERCENT_CONT", "FIBROBLAST_AND_VESSEL_PERCENT_CONT", "TREATMENT",  "CEA_CONT", "SEX", "COUNTRY", "CHEMOTHERAPY_POOLED",  "HISTOLOGIC_GRADE", "TUMOR_PERCENT_CONT", "RADIATION_THERAPY_POOLED", "INFLAMMATION_PERCENT_CONT", "PC3")
if(!file.exists("kosticRCM.RData")){
sample_data(kostic) = within(sample_data(kostic),{
  RADIATION_THERAPY_POOLED = RADIATION_THERAPY
  RADIATION_THERAPY_POOLED[RADIATION_THERAPY_POOLED=="None"] = "No"
  RADIATION_THERAPY_POOLED = droplevels(RADIATION_THERAPY_POOLED)
  
  CHEMOTHERAPY_POOLED = CHEMOTHERAPY
  CHEMOTHERAPY_POOLED[CHEMOTHERAPY_POOLED=="None"] = "No"
  CHEMOTHERAPY_POOLED = droplevels(CHEMOTHERAPY_POOLED) 
  
  NECROSIS_PERCENT_CONT = NECROSIS_PERCENT
  NECROSIS_PERCENT_CONT[NECROSIS_PERCENT_CONT=="None"] = 0
  NECROSIS_PERCENT_CONT = as.numeric(as.character(NECROSIS_PERCENT_CONT))
  
  TUMOR_PERCENT_CONT = TUMOR_PERCENT
  TUMOR_PERCENT_CONT[TUMOR_PERCENT_CONT=="None"] = 0
  TUMOR_PERCENT_CONT = as.numeric(as.character(TUMOR_PERCENT_CONT))
  
  NORMAL_EQUIVALENT_PERCENT_CONT = NORMAL_EQUIVALENT_PERCENT
  NORMAL_EQUIVALENT_PERCENT_CONT[NORMAL_EQUIVALENT_PERCENT_CONT=="None"] = 0
  NORMAL_EQUIVALENT_PERCENT_CONT = as.numeric(as.character(NORMAL_EQUIVALENT_PERCENT_CONT))
  
  CEA_CONT = factor(CEA, levels = c(0, levels(CEA)))
  CEA_CONT[CEA_CONT=="None"] = 0
  CEA_CONT = as.numeric(as.character(CEA_CONT))
  
  INFLAMMATION_PERCENT_CONT = factor(INFLAMMATION_PERCENT, levels = c(0, levels(INFLAMMATION_PERCENT)))
  INFLAMMATION_PERCENT_CONT[INFLAMMATION_PERCENT_CONT=="None"] = 0
  INFLAMMATION_PERCENT_CONT = as.numeric(as.character(INFLAMMATION_PERCENT_CONT))
  
  FIBROBLAST_AND_VESSEL_PERCENT_CONT = factor(FIBROBLAST_AND_VESSEL_PERCENT, levels = c(0, levels(FIBROBLAST_AND_VESSEL_PERCENT)))
  FIBROBLAST_AND_VESSEL_PERCENT_CONT[FIBROBLAST_AND_VESSEL_PERCENT_CONT=="None"] = 0
  FIBROBLAST_AND_VESSEL_PERCENT_CONT = as.numeric(as.character(FIBROBLAST_AND_VESSEL_PERCENT_CONT))
  
  AGE_CONT = as.character(AGE)
  AGE_CONT[AGE_CONT=="None"] = mean(as.numeric(AGE_CONT[AGE_CONT!="None"]))
  AGE_CONT = as.numeric(AGE_CONT)
})
kosticRCM = RCM(kostic, k = 3)
kosticRCMlin = RCM(kostic, k = 3, covariates = kostVars, responseFun = "linear")
kosticRCMnp = RCM(kostic, k = 3, covariates = kostVars, responseFun = "nonparametric")
save(kosticRCM, kosticRCMlin, file = "kosticRCM.RData")
} else {load("kosticRCM.RData")}
```

```{r KosticPlots}
plot(kosticRCM, samColour = "TREATMENT")
plot(kosticRCM, samColour = "COUNTRY")
plot(kosticRCM, samColour = "CHEMOTHERAPY_POOLED")
plot(kosticRCM, samColour = "RADIATION_THERAPY_POOLED")
```

Country, chemotherapy and radiotherapy seem to be related to microbiome composition

```{r KosticPlots Constrained}
plot(kosticRCMlin)
```

Four sample clusters are visible. Important drivers of variability are the cancer status, country, tumor percentage and radiation therapy

```{r KosticPlotsConstrainedCol}
plot(kosticRCMlin, samColour = "TREATMENT", samShape = "COUNTRY")
```

The first dimension is dominated by country, the second by treatment, so these are the two most important variables. This is a bit surprising since the treatment does not seem to play a role in the unconstrained ordination.

\clearpage

## Props et al. (2016)

This is a longitudinal dataset on microbial growth in a water cooling system of a nuclear power plant \cite{Props2016}.

```{r readInCMET, purl = FALSE, eval = FALSE}
if(!file.exists(file = "CMETdata.RData")){
  library(openxlsx)
  samData = read.xlsx("/home/stijn/PhD/Datasets/CMET/Metadata.xlsx")
  rownames(samData) = samData$sample_title
  samData$Reactor.cycle = as.character(samData$Reactor.cycle)
  samData$cellDensity = samData$`Cell.density.(cells/mL)`
  samData$`Cell.density.(cells/mL)` = NULL
  otuTab = read.csv("/home/stijn/PhD/Datasets/CMET/OTU_table_raw.csv", sep = ";")
  rownames(otuTab) = otuTab$Sample_ID
  otuTab$Sample_ID = NULL
  #Select only samples present in samData
  otuTab = otuTab[as.character(samData$sample_title),]
  #Multiply relative abundances by cell counts in samData. Not a good approach!
  # otuTab = round(as.matrix(otuTab)/rowSums(as.matrix(otuTab))*samData$cellDensity)
  taxTab = read.csv("/home/stijn/PhD/Datasets/CMET/tax_table_raw.csv", sep = ";")
  rownames(taxTab) = taxTab$X
  taxTab$X = NULL
  samData = samData[rowSums(otuTab)>0,]
  taxTab = taxTab[ colSums(otuTab)>0,]
  otuTab = otuTab[rowSums(otuTab)>0, colSums(otuTab)>0] #Number of zeroes is not related to rounding
  CMETwater = phyloseq(otu_table(otuTab, taxa_are_rows = FALSE), sample_data(samData), tax_table(as.matrix(taxTab)))
  save(CMETwater, file = "CMETdata.RData")
}else {load(file = "CMETdata.RData")}
```

```{r analysCMET, purl = FALSE}
if(!file.exists(file = "CMETRCM.RData")){
  CMETRCM = RCM(CMETwater, k = 3, prevCutOff = 0.05)
  envVarsCMET = c("ph","temp","Conductivity","cellDensity", "Reactor.phase", "Reactor.cycle")
  CMETRCMlin = RCM(CMETwater, k = 3, responseFun = "linear", covariates = envVarsCMET, prevCutOff = 0.05)
  CMETRCMquad = RCM(CMETwater, k = 3, responseFun = "quadratic", covariates = envVarsCMET, prevCutOff = 0.05)
  CMETRCMnonParam= RCM(CMETwater, k = 3, responseFun = "nonparametric", covariates = envVarsCMET, prevCutOff = 0.05)
  #Condition on time variable
  CMETRCMcond = RCM(CMETwater, k = 3, prevCutOff = 0.05, confounders = "Timepoint")
  save(CMETRCM, CMETRCMlin, CMETRCMnonParam, CMETRCMcond, file = "CMETRCM.RData")
} else {load(file = "CMETRCM.RData")}
```

```{r plotCMET, purl = FALSE}
plot(CMETRCM, samColour = "Reactor.phase", samShape = "Reactor.cycle")
```

The first dimension mainly tracks the change of the cooling water throughout the different phases, the second dimension is related to the reactor cylce

```{r plotCMETlin, purl = FALSE}
plot(CMETRCMlin, samColour = "Reactor.phase", samShape = "Reactor.cycle")
```

The constrained analysis confirms phase and cycle as being the most important drivers. Also, the most important taxa are seen to react to the gradient dominated by reactor cycle.

```{r plotCMETnp, purl = FALSE}
plot(CMETRCMnonParam, samColour = "Reactor.phase", samShape = "Reactor.cycle")
```

The sample ordination is similar when non-parametric response functions are used, but the role of reactor phase is assessed differently.
