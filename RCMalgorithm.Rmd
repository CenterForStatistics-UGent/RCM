---
title: "RC(M)"
author: "Stijn"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    includes:
      in_header: packagesRCM.sty
---

\setcounter{tocdepth}{2}
\tableofcontents

# Introduction

This document provides an exact description of the algorithm used to fit the RC(M) model augmented with the negative binomial distribution for microbiome data.

# Inputs

The algorithm requires the following inputs:

 - an n-by-p data matrix __X__, with samples $i$ in the rows and taxa (species, OTUs) $j$ in the columns. Thus $x_{ij}$ is the observed count of taxon $j$ in sample $i$.
 - a required dimension of the solution K. The dimensions are fitted sequentially, so the solutions for the lower dimensions are independent of the final required dimension.
 
 The algorithm allows to supply the following optional inputs
 
  - a n-by-q design matrix __E__ of confounding variables, containing $z-1$ dummies for categorical variables with $z$ levels, and an intercept. The entry $g_{il}$ then represents the value of confounder variable $l$ in sample $i$.
  - a n-by-g design matrix __C__ of constraining variables, containing $z$ dummies for categorical variables with $z$ levels, and no intercept. The entry $c_{iy}$ then represents the value of constraining variable $y$ in sample $i$.

# Trimming

Rows and columns of __X__ with zero counts are trimmed prior to model fitting.

To avoid numerical instability, also taxa below a certain prevalence threshold or with total count lower than a certain fraction of the number of samples $n$ are excluded prior to model fitting.

The default prevalence threshold is 5%, the default fraction of $n$ is 10%.

If a confounder matrix is provided with dummy variables, also discard the taxa that fall below the prevalence and total count fractions mentioned above within each level of the categorical confounding variables, again to avoid overflow.

# Independence model

The independence model is defined as:

$$log(E(x_{ij})) = u_i + u_j$$

The independence model is fitted as follows:

 1. Find starting values $u_{i, init} = log(\sum_{j=1}^p x_{ij})$ and $u_{j, init} = log(\sum_{i=1}^n x_{ij})$ for $u_{i}$  and $u_j$ respectively
 2. Estimate a mean-dispersion trend using the estimateGLMTrendedDisp() function of the _edgeR_ package, given $u_i$ and $u_i$. This estimate is very insensitive to slight changes in the mean structure and will never be reestimated throughout the whole fitting procedure to save computation time.
 3. Estimate the dispersion parameters $\theta_j$ based on the mean-dispersion trend using empirical Bayes with the estimateGLMTagwiseDispersion() function in the _edgeR_ package, given $u_i$ and $u_j$.
 4. Estimate $u_{i,new}$'s using maximum likelihood (ML), keeping the $\theta_j$'s and $u_j$'s constant.
5. Estimate the $u_{j,new}$'s using maximum likelihood, keeping the $\theta_j$'s and $u_i$'s constant
 6. Check for convergence. If no convergence is reached, repeat steps 3-5. Convergence is assumed when
 
 $$\Big(\frac{1}{n}\sum_{i=1}^n |1-\frac{u_{i,new}}{u_{i,old}}|^a\Big)^{\frac{1}{a}}$$
 
 and
 
 $$\Big(\frac{1}{p}\sum_{j=1}^p |1-\frac{u_{j,new}}{u_{j,old}}|^a\Big)^{\frac{1}{a}}$$
, with $a$ a convergence norm, are below a certain tolerance. The default is to use 0.001 as tolerance and $a = 2$ as convergence norm. Once the independence model has converged, the estimates $u_i$ and $u_j$ are kept constant throughout the remainder of the fitting process. All maximum likelihood estimation is under the negative binomial model.
 
# Conditioning on confounders

If a confounder matrix is provided, the effect of the confounders is filtered out by fitting the following model:

 $$log(E(X_{ij})) = u_i + u_j + \sum_{l=1}^q \zeta_{jl}g_{il}$$

with $\zeta_{jl}$ the interaction parameter between taxon $j$ and confounding variable $l$, which is fitted by maximum likelihood. Note that $\forall i: g_{i1}=1$, i.e. the model is fitted with an intercept.

Again this step is performed iteratively by alternating between estimating the $\zeta_{jl}$ parameters and re-estimating the overdispersion parameters as in step. of the independence model. Convergence is then assumed when $\sqrt{\frac{1}{pq} \sum_{l=1}^q \sum_{j=1}^p (\zeta_{jl,new} - \zeta_{jl,old})^2}$ drops below a tolerance level of 0.001.

# Capturing the signal

The steps undertaken so far to model $E(X_{ij})$ are merely fitting a "null" model and will not play a role in the final ordination. The next terms that will be added will capture the biological signal in the data __X__ and will be used for data visualization. This step differs between an _unconstrained_ RC(M) model, that merely uses the data __X__, and _constrained_ analysis, that also incorporates the covariate matrix __C__.

## Unconstrained RC(M)

The unconstrained RC(M) model looks like

$$log(E(X_{ij})) = u_i + u_j + \Big[\sum_{l=1}^q \zeta_{jl}e_{il}\Big] + \sum_{k=1}^K\psi_kr_{ik}s_{jk}$$

with the term between [] being optional.

The unconstrained RC(M) model is fitted as follows:

1. Obtain a singular value decomposition as $R^{-1}(X-E)C^{-1} = U\Sigma V$. This gives us initial values $[r_{i1}^{SVD}, r_{i2}^{SVD},..., r_{iK}^{SVD}] = U_{(1.)}$, the first row of $U$, and correspondingly $\Sigma$ and $V$ give initial values for $\mathbf{\psi}^{SVD}$ and $\mathbf{s}^{SVD}$. For these initial values we still need to ensure that the (weighted) variances equal 1, and transfer these weights to the importance parameters $\mathbf{\psi}^{SVD}$ so we set

$$\psi_k^{init} = \psi_k^{SVD}\sum_{i=1}^n\big({r^{SVD}_{ik}}\big)^2 \sum_{j=1}^p\big(z_js^{SVD}_{jk}\big)^2$$

$$r_{ik}^{init} = \big(\frac{r_{ik}^{SVD}}{\sum_{i=1}^n{\big({r^{SVD}_{ik}}\big)^2}}\big)^{1/2}$$

and

$$s_{jk}^{init} = \big(\frac{z_js_{jk}^{SVD}}{\sum_{j=1}^p{\big(s^{SVD}_{jk}}z_j\big)^2}\big)^{1/2}$$

with $z_j = exp(u_j)$

2. For all dimensions $k$ starting from 1 to K :
a) Estimate the dispersions $\theta_j$ using empirical Bayes as before
b) Estimate the importance parameter $\psi_k$ by full ML, keeping the sample and taxon scores and overdispersions fixed.
c) Estimate the sample scores $r_ik$ by restricted ML, keeping the taxon scores, overdispersions and importance parameters fixed. Lagrangian multipliers are used to ensure that

$$ \sum_{i=1}^n r_{ik} = 0$$
and 

$$ \sum_{i=1}^n r_{ik}r_{ik'} = \delta_{kk'}$$
with $\delta$ the Kronecker delta.

c) Estimate the taxon scores $s_jk$ by restricted ML, keeping the sample scores, overdispersions and importance parameters fixed. Lagrange multipliers are used to ensure that

$$ \sum_{j=1}^p z_js_{jk} = 0$$

and 

$$ \sum_{j=1}^p z_js_{jk}s_{jk'} = \delta_{kk'}$$
d) Check for convergence of dimension $k$. If no convergence reached, repeat steps a-c. Convergence is assumed when 

$$|1-\frac{\psi_k^{new}}{\psi_k^{old}}| < 0.001$$
 
 AND
 
 $$\forall \mathbf{r_k}, \mathbf{s_k} \Big(\sqrt{\sum_{i=1}^n (1-\frac{r^{new}_{ik}}{r^{old}_{ik}})^2} < 0.001 \Big) \cup \Big( \sqrt{\sum_{j=1}^p (1-\frac{s^{new}_{jk}}{s^{old}_{jk}})^2} < 0.001 \Big)$$

i.e. an infinity norm is used for the psis, and an L2-norm for the sample and taxon scores.

### Log-linear analysis

Another modelling approach is to use log-linear modelling an thereby introduce the negative binomial as error term. We know from previous goodness of fit testing that this is an appropriate error model for microbiome data.

In log-linear analysis the logged expected count $l_{ij}$ is modelled as

$$log(E(X_{ij})) = l_{ij} = u + u_i + u_j + u_{ij}$$.

For $u_{ij} \neq 0$ this is the saturated model, which provides a perfect fit by setting all expected values equal to the observed ones. If $u_{ij} = 0$ this is the independence model presented above.

### The RC(M)-association model

For the purpose of making biplots, we might consider the following model (proposed by Leo Goodman, 1985. See also VanDerHeijden 1994)

$$log(E(x_{ij})) = l_{ij} = u + u_i + u_j + \psi_1r_{1i}s_{1j} + \psi_2r_{2i}s_{2j}$$

or in matrix form:

$$\mathbf{lE} = \mathbf{u_{sam,MLE}}\mathbf{u_{tax,MLE}}^t + \mathbf{r}_k \boldsymbol{\psi}_k \mathbf{s}_k^t$$

with $\mathbf{lE}$ the n-by-p matrix of logged expectations, $\mathbf{u_{sam,MLE}}$ the vector with length $n$ with the row intercepts estimated by ML, $\mathbf{u_{tax,MLE}}$ the vector of length $p$ with the column intercepts estimated by ML, $\mathbf{r}_k$ the n-by-k matrix of row scores, $\boldsymbol{\psi}_k$ a diagonal matrix with the $k$ $\psi$'s on the diagonal and $\mathbf{s}_k$ the p-by-k matrix of column scores

Constraints are needed to render this model identifiable, since we have three unknown regressors ($r_{ik}, s_{jk} and psi_k$) instead of one. We do restrict the importance parameter $psi_k$ to be positive, and center the (weighted) row and column scores around 0, which is a useful property for the biplot and is also the case for correspondence analysis. In addition the row and column scores are restricted to have (weighted) variance 1. $psi_k$ is the only parameter that can grow in size without restriction. As a result it will automatically figure as a measure of importance of the departure from independence in that direction, since all scores of the dimensions are normalized. This is also the case for correspondence analysis. Thridly, the scores of the different dimensions are orthogonal, so the solutions in different dimensions are linearly independent.

Centering:

$$\sum_{i=1}^nw_ir_{ki} = 0$$

with k=1,2 and

Normalization($k=k'$) and orthogonality ($k \neq k'$)

$$\sum_{i=1}^nw_ir_{ki}r_{k'i} = I(k=k')$$

$$\sum_{j=1}^pz_js_{kj} = 0$$

$$\sum_{j=1}^pz_js_{kj}s_{k'j} = I(k=k')$$
Hereby $w_i$ and $z_j$ are row and column weigths.

#### RC(M)-weights

The weights can be regarded as probability density functions, they represent the likelihood of sampling a certain sample or taxon from the population. On the population level we could say that

$$E(w_iX_{ik}) = 0$$,

i.e. the average row score on the population level is zero. This is a useful restriction to make sure that the biplot is centered around zero. Analogously we want that

$$E(w_iX_{ik}X_{ik'}) = I(k=k')$$

and accordingly for the column scores:
$$E(z_j S_{jk}) = 0$$

$$E(z_j S_{jk} S_{jk'}) = I(k=k')$$. 

But which weights $w_i$ and $z_j$ should we use?

Goodman proposes to use $w_i = x_{i.}$ and $z_j = x_{.j}$ thus uses _weighted_ constraints, to retain the relationship with correspondence analysis (see below). Becker _et al._ (1989) recommend using uniform weights not to let the marginal distribution affect the model fit. 

For the microbiome case, every subject comes from the same population under the null-hypothesis and all subjects are thus eqaully likely to be sampled and have the same importance. The library sizes as a purely technical artefact, unrelated to the biological importance of the subject. Consequently we use uniform row weights $w_i = 1/n$ (or $w_i = 1$, the magnitude is of no importance since the associated $\psi_k$ will grow or shrink accordingly). However, some taxa are more prevalent in the population than others. We want the average column scores on the population level to be centered around zero, have variance one and be orthogonal. That is why we set $z_j = exp(u_j)$; because the more abundant species are in fact more abundant in the population as a whole (as opposed to samples with a large library size), it makes sense to use a marginal weighting scheme for the column scores. The weights $z_j$ will be estimated by maximum likelihood, as explained in the next paragraph.

#### RC(M) margins

The most logical choices for the margins and the independence model have long seemed to use $u=-log(x_{..})$, $u_i = log(x_{i.})$ and $u_j = log(x_{.j})$.

However, the library sizes do not correspond to the maximum likelihood estimate of $u_i$ under the Negative Binomial model. As a result the first dimensional row scores $r_{1i}$ tried to correct for this effect and became related (linearly correlated) to the library sizes, which we want to avoid absolutley. 

To avoid this we set $u=0$ and estimate the $u_i$'s, $u_j$'s and overdispersions iteratively using maximum likelihood in a first step (this converges very quickly). The main disadvantage I see of this approach is that the independence model becomes model dependent. Next we'll have to iterate between fitting the overdispersions, the imporance parameters $\psi$, the $r's$ and the $s's$. Hereby we use marginal weights for the taxa ($z_j = exp(u_j)$) and uniform weights for the rows. 

In practice, the marginal sums differ more from the MLE for the library sizes than for the taxon abundances. We can think of the following mathematical explanation: when calculating library sizes or abundances by row and column sums, each observations receives the same weight. However, when we estimate the margins through ML, we solve the following score equations:

$$\frac{\partial L(\mathbf{X}_i|\mathbf{u}_{i}, \mathbf{u}_j, \boldsymbol{\theta})}{\partial u_{i}} = \sum_{j=1}^p \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}}$$

$$\frac{\partial L(\mathbf{X}_i|\mathbf{u}_{i}, \mathbf{u}_j, \boldsymbol{\theta})}{\partial u_{j}} = \sum_{i=1}^n \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}}.$$

Note that since we are estimating offsets the value of the regressor is 1. In this case the difference of $y_{ij}$ with the expected value $\mu_{ij}$ is weighted by a factor $\frac{1}{1+\frac{\mu_{ij}}{\theta_j}} = \frac{\theta_j}{\theta_j+\mu_{ij}}$. When estimating $u_j$, $\theta$ is a constant but when estimating $u_i$ it is different for every observation $y_{ij}$. Hence the weights put on every observation differ much more when estimating the sample offsets than when estimating the taxon offsets. That is the reason why the MLE differs more from the marginal sum for the library size than for the abundances.

Note also that when there is a very large overdispersion for a taxon _j_ ($\theta_j$ small), its observations carry little information and their weights are small in the calculation of the library sizes. However, when there is very little overdispersion ($\theta_j \rightarrow \infty$), the weights of the components of the score function equal 1, as with Poisson regression. These MLEs of the Poisson regression are also equal to the estimators based on the marginal sums. This means that the larger and the more diverse the overdispersion estimates are, the more the MLEs under the negative binomial model will depart from the marginal sums. Finally, we see that departures from the mean $\mu_{ij}$ are weighted down for large values of $\mu_{ij}$, acknowledging the fact that the variance increases faster than linear with the mean in the negative binomial model.

#### Imposing the restrictions

The centering, normalization and orthogonality of the row and column scores are enforced through Lagrange multipliers. This makes the systems of equations harder to solve but assures independence of the dimensions.

#### A note on gradient analysis

Note that this is a case of __indirect gradient__ analysis: the dimensions are estimated without incorporating gradient information (measured covariates) to maximally represent variability in the data. In a next step these obtained gradients can be compared with measured covariates (often graphically through a colour code) to see how well these covariates explain the gradients. 

#### Relationship between CA and log-linear analysis

According to Escoufier, 1982 if $a =\sum_{k=1}^K \omega_k v_{ki} w_{jk}$ is small (i.e. the deviation from independence is small) then $log(1+a) \approx a$ and

$$log(E(x_{ij})) = log(x_{i.}) + log(x_{.j}) - log(x_{..}) + log\big(1 + \sum_{k=1}^K \omega_k v_{ki} w_{jk}\big) \approx log(x_{i.}) + log(x_{.j}) - log(x_{..}) + \sum_{k=1}^K \omega_k v_{ki} w_{jk}$$

i.e. an equivalence between the RC(M)-model and correspondence analysis.

Since the same restricitions apply to the scores $v_{ki}$ and $w_{jk}$ as to U and V, we can state that  $\psi_k \approx \omega_k$. The assumption that the departure from independence is small seems unlikely in our case, but it does provide us with some starting values.

#### The RC(M) model and time series

Often we have data from experiments with repeated measurements, e.g. the evolution of the infant microbiome over time. Should we take this into account while fitting the RC(M) model? I do not think so, since it is an exploratroy method and we just want to visualize the variation in the data. It would be interesting to see if the individuals form clusters or not. In the constrained version of this method (see below), we can always add labels for the individuals and  time variable.

### Fitting the RC(M) model

The models are usually fitted by ML. See page 55 of Goodman 1985 for details. Goodman assumed a Poisson distribution, but it might be possible to use a negative binomial model with unique dispersions for each taxon $j$ as well.

See Goodman 1985 (appendix), Becker 1988 for information on estimation. The code below is generic but we take k=2 for simplicity.

Parameters of interest to estimate are $\psi$, $r_1$, $r_2$, $c_1$, $c_2$. In the process we'll also need the nuisance parameters $\mathbf{\theta}$ and the offsets of the independence model $u_i$ and $u_j$. Assume $E(X_{ij}) = \mu_{ij}$ and  $Var(X_{ij})=\mu_{ij} + \frac{\mu_{ij}^2}{\theta}$

#### The negative binomial density function

For the sake of completeness we give the density function of the negative binomial distribution (see Lawless, 1987) in the ($\mu$, $\theta$) parametrization whereby $E(X_{ij}) = \mu_{ij}$ and $Var(X_{ij}) = \mu_{ij} + \frac{\mu_{ij}^2}{\theta_j}$ .

$$f_{NB}(X_{ij}) = \frac{\Gamma(X_{ij}+\theta_j)}{\Gamma(\theta_j)} \big(\frac{\mu_{ij}}{\theta_j+\mu_{ij}}\big)^{X_{ij}} \big(\frac{\theta_j}{\theta_j+\mu_{ij}}\big)^{\theta_j}$$

#### Starting values



#### Iteration

2. If the independence model is estimated by MLE, estimate the $u_i$'s, $u_j$'s and $\theta_j$'s iteratively until convergence (set $u=0$). The estimating equeations are independent (Jacobian is diagonal) so this converges very quickly. Otherwise set $u_i = log(x_{i.})$ , $u_j = log(x_{.j})$ and $u = log(x_{..})$.

3. Estimate overdispersions

Theoretically we might estimate the dispersions by ML, but the estimates are very variable, which dramatically affects the rest of the fitting procedure.

 - Maximum likelihood estimation (not used) 
 
$$log(E(x_{ij})) = l_{ij} = offset\big(log(\frac{x_{i.}x_{.j}}{x_{..}}  \big) + \psi_1 (s_{1i}^{init}r_{1j}^{init}) + \psi_2 (s_{2i}^{init}r_{2j}^{init})\big)$$

to obtain estimates for the overdispersions $\theta_j$, whereby the means $\mu_{ij}$ are assumed known

 $$\sum_{i=1}^n \sum^{y_i^*}_{t=0} \big( \frac{t}{1+\frac{t}{\theta_j} } \big) + \theta_j^2 log(1+\frac{\mu_{ij}}{\theta_j})- \mu_{ij}\frac{y_{ij}+\theta_j}{1+\frac{\mu_{ij}}{\theta_j} }$$
 
 For this we can use the theta.ml() function in the _MASS_ package
 
 - edgeR robust
 
To get more stable estimates of the dispersions, RNA-seq packages such as _DESeq_ and _edgeR_ use an empirical Bayes approach to shrink dispersion estimates towards a fitted mean-dispersion trend. Here I use the machinery of edgeR robust (version `r packageVersion("edgeR")`) to estimate the dispersions
 
To reduce the computational cost and because the estimates do not change a lot anyway the estimation of the overdispersions is not repeated in every iteration

4. Estimate the psi parameters
 
 Solve the system of score equations (see Lawless 1987) for $\psi_1$ and $\psi_2$, assuming $\theta_j$'s given
 
 $$\sum_{i=1}^n \sum_{j=1}^p r_{im}s_{mj} \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}} = 0$$ for all m=1,...,k
 
 with
 
 $$E(X_{ij}) = \mu_{ij} = exp\big(log(\frac{x_{i.}x{.j}}{x_{..}}) + \psi_1^{MLE} s_{1j}^{init}r_{1i}^{init} + \psi_2^{MLE} s_{2j}^{init}r_{2i}^{init} \big)$$.
 
 These are non-linear equations for which we use the _nleqslv_ package, with its default the Broyden method. This is a variation of the Newton-Raphson method (or Fisher scoring in case of ML) whereby the Jacobian is recycled and not recalculated at every iteration. The Jacobian is a square matrix of dimension k*(k-1)/2. The diagonal entry at position $kk'$ is:
 
 $$\frac{\delta^2 L_{NB}(\mu, \theta)}{\delta \psi_k \delta \psi_{k'}} = -\sum_{i=1}^n \sum_{j=1}^p r_{ik}r_{ik'}s_{kj}s_{k'j}\frac{\mu_{ij}(1+\frac{x_{ij}}{\theta_j})}{(1+\frac{\mu_{ij}}{\theta_j})^2}$$
 
5. Estimate the row scores. 

To estimate the $r_{i}$'s we would like to use weighted least squares to minimize

$$\sum_{j=1}^p\Big(log(x_{ij}) - \big(u + u_i + u_j +\sum_{k=1}^M r_{ik} \big( \hat{\psi_ks_{jk}} \big)\Big)^2$$

for every $i$. Unfortunately we cannot calculate $log(x_{ij})$ because of the zeroes.

We'll have to use another way to estimate them, maximum likelihood in our case. I don't like using weighted least squares on the non-transformed counts,

$$\sum_{j=1}^p\Big(x_{ij} - exp\big(u + u_i + u_j +\sum_{k=1}^M r_{ik} \big( \hat{\psi_ks_{jk}} \big)\Big)^2,$$

because of the skewness of the residuals as explained above.

To enforce the constraints on the row scores one option would be to estimate all scores through full maximum likelihood and then modify a few of them to make them satisfy the constraints and repeat until convergence. This runs into numerical problems though, because the modified scores are usually very large initially which leads to overflow when exponenttiated. Instead we use the methods of the Lagrange multipliers to implement the constraints. We thus seek to maximize the Lagrangian function

$$Lag(r_{ik}, \mathbf{\lambda}) = L_{NB}(\mathbf{r}) + \sum_{k=1}^M \big( \lambda_{1k} \sum_{i=1}^n w_i r_{ik} \big) + \sum_{k=1}^M \big( \lambda_{2k} ( \sum_{i=1}^n w_i r_{ik}^2 ) - 1 \big) + \sum_{k \neq k'} \big(\lambda_{3kk'} \sum_{i=1}^n (w_ir_{ik}r_{ik'}) \big)$$

with $L_{NB}(\mathbf{r})$ the log-likelihood function of the negative binomial regression. The derivatives of this function for the following system of equations

$$\frac{\partial Lag(r_{ik}, \mathbf{\lambda})}{\partial r_{ik}} = \sum_{j=1}^p \hat{s}_{jk} \hat{\psi}_k \frac{y_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}} + \sum_{k=1}^M w_i \lambda_{1k}  + \sum_{k=1}^M \big(2 \lambda_{2k} w_ir_{ik}\big) +  \sum_{k' \neq k} w_ir_{ki} \lambda_{kk'} = 0$$

and 

$$\frac{\partial Lag(r_{ik}, \mathbf{\lambda})}{\partial \lambda_{1k}} = \sum_{i=1}^n w_i r_{ik} = 0$$

and 

$$\frac{\partial Lag(r_{ik}, \mathbf{\lambda})}{\partial \lambda_{2k}} = (\sum_{i=1}^n w_i r_{ik}^2) - 1 = 0$$

and 

$$\frac{\partial Lag(r_{ik}, \mathbf{\lambda}))}{\partial \lambda_{3kk'}} = (\sum_{i=1}^n w_i r_{ik}r_{ik'}) - 1 = 0$$

. The latter three groups of equations represent the original centering, normalization and orthogonality constraints. 

There are additional equations in the system now, but also as many extra $\lambda$ parameter to optimize. These $\lambda$ parameters are also stored and used as initial values in the next iteration run. There size represent the extent to which the constraints pull away the solution from the optimal, unconstrained solution, but I don't see how we can use that in a biologically or statistically meaningful way.

We now truly have a system of equations to solve, which we do with the _nleqslv_ package. It is straightforward but a bit cumbersome to also provide the analytical Jacobian, which greatly speeds up the iterations (otherwise it has to be approximated numerically). The Jacobian is sparse and symmetric. with following non-zero entries:

 $$\frac{\delta^2 Lag(\mu, \theta)}{ \delta r_{ik} \delta r_{ik'}} = -\sum_{i=1}^n \sum_{j=1}^p \psi_{k}\psi_{k'}s_{kj}s_{k'j}\frac{\mu_{ij}(1+\frac{x_{ij}}{\theta_j})}{(1+\frac{\mu_{ij}}{\theta_j})^2}=0$$
 
  $$\frac{\delta^2 Lag(\mu, \theta)}{\delta r_{ik} \delta \lambda_{1k}} = w_i$$
  
  $$\frac{\delta^2 Lag(\mu, \theta)}{\delta r_{ik} \delta \lambda_{2k}} = 2w_ir_{ik}$$
  
  $$\frac{\delta^2 Lag(\mu, \theta)}{\delta r_{ik} \delta \lambda_{3kk'}} = w_ir_{ik'}$$
  
  All other entries are zero.
  
6. Estimate the column scores

Repeat step 4 but now estimate $s_{jk}$ column scores in the restrictions, e.g. the Lagrangian becomes:

$$Lag(s_{jk}, \mathbf{\lambda}) = L_{NB}(s_{jk}) + \sum_{k=1}^M \big( \lambda_{1k} \sum_{j=1}^p z_j s_{jk} \big) + \sum_{k=1}^M \big( \lambda_{2k}  ( \sum_{j=1}^p z_js_{jk}^2 ) - 1 \big) + \sum_{k \neq k'} \big(\lambda_{3kk'} \sum_{j=1}^p (z_js_{jk}s_{jk'}) - 1\big)$$

#### Termination

7. Repeat steps 3-6 until convergence. Convergence is assumed if between two iterations

 - None of the $\psi$ parameters change less than $0.01\%$ (infinity norm)
 
 AND
 
 - None of the square roots of the L2-norms of the relative changes of row and column scores exceeds 0.1%, or
 
 $$\forall \mathbf{r_k}, \mathbf{s_k} \Big(\sqrt{\sum_{i=1}^n (1-\frac{r^{(iter-1)}_{ik}}{r^{(iter)}_{ik}})^2} < 0.001 \Big) \cup \Big( \sqrt{\sum_{j=1}^p (1-\frac{s^{(iter-1)}_{kj}}{s^{(iter)}_{kj}})^2} < 0.001 \Big)$$
 
 and analogously for all the scores. Other norms, such as L1 or infinity norms can also be used. A norm between 1 and 2 will lead to fast convergence.
 
 Overdispersions are not taken into account in determining the convergence since they're merely nuisance parameters.

#### Biplot

8. Plot $\psi_1r_{1i}$ vs $\psi_2r_{2i}$ as sample points and add $s_{1j}$ vs $s_{2j}$ as arrows to make a biplot. This assures that the orthogonal projection of the vector ($\psi_1r_{1i}$, $\psi_2r_{2i}$) on ($s_{1j}$, $s_{2j}$) equals $cos (\eta) ||(\psi_1r_{1i}, \psi_2r_{2i})|| = \frac{(s_{1j}, s_{2j})^t(\psi_1r_{1i}, \psi_2r_{2i})}{||(s_{1j}, s_{2j})||}$ with $\eta$ the angle between the two vectors. This projection is thus proportional to the departure from independence in the first two dimensions combined. The larger the entries of the species and sample scores (the scaling between these two sets is arbitrary, we usually choose them in the same order of magnitude) and the smaller the angle, the larger the departure of this taxon in this sample. The choice of $\psi_1r_{1i}$ vs $\psi_2r_{2i}$ assures the distances between samples are well represented, since more weight is added to differences in scores in important dimensions.

In the end we'll have estimated  p (abundances) + n (library sizes) + p (dispersions) + kxp (column scores) + kxn (row scores) + k (importance parameters) = (k+2)p + (k+1)xn + k parameters out of np entries. We have imposed 4k + k(k-1) restrictions, so the final model is still very parsimonious for n and p in the hundreds.

##### Model quality

9. Assess the goodness of fit

It would be interesting to assess the goodness of fit of our models. Questions that come to mind are:

 - How do the different dimensions of the fit relate to one another in terms of importance? How well does our lower dimensional solution represent high-dimensional departures from independence?
 - How much better are quadratic or nonparametric response functions compared to the linear response functions?
 - Which samples and/or taxa are poorly represented in the low-dimensional solution? Which ones exhibit lack of fit to the negative binomial distribution?
 - Which samples/taxa have a strong influence on the final model fit?
 
###### Relative importance of dimensions

Mean

The first measure of differences in importance between the dimensions are of course the importance parameters $\psi_k$. Since all other parameters in both the unconstrained and constrained variables are normalized, these are the only ones that can grow in magnitude to give more weight to the departures of independence in their dimension. The same concept exists for correspondence analysis, where the size of the diagonal elements of $\Sigma$ is proportional to the importance of the corresponding dimension. In the unconstrained case this works very well, but in the constrained case the magnitude of the $\psi_k$'s is not always monotonically decreasing with the dimensions. This is because the order of the dimensions is determined by the way the response functions are separated, not just by the mean. Also we do not dispose of the "full" solution of importance parameters as in singular value or eigenvalue approaches, only the dimensions we fitted. This complicates the comparison with the total variance.

Likelihood

Another approach than to look at the mean is to look at likelihoods. After all our model is fitted largely with maximum likelihood. Since the model is overparametrized, classical ML theory (such as asymptotic behavious of maximum likelihood statistics) does not apply to our solution. Still we can compare the likelihoods of the independence, the RC(M) and the intermediate models to get an idea of the relative importance of the dimensions.

We can decompose the difference in log-likelihood between the independence and the full RC(K) model as

$$(LL_K - LL_0) = (LL_1 - LL_0) + (LL_2 - LL_1) + ... + (LL_K - LL_{K-1})$$

with $LL_k$ the log-likelihoods of RC(k) model, $LL_0$ being the independence model. Scaling by $(LL_{sat} - LL_0)$ with $LL-sat$ the log-likelihood of the saturated model, will provide interpretable fractions of the log-likelihood explained. The trouble is how to estimate the log likelihood of a saturated model, especially the overdispersion. We just use the likelihood of the Poisson distribution here, since the dispersions will be zero.

This approach leads to weird results for the constrained case as well, because the environmental gradients are not estimated to maximize the likelihood. For the the order of the dimensions is dictated by optimal separation of the species' niches, for good and for ill.

This criterion can also be used to assess the differences in likelihood between constrained methods with different shapes of the response functions (see later).

All in all the psis are the most interpretable measure of importance of each dimension. Means are more interpretable than likelihoods.

###### Comparison of response functions

In case the models are fitted on exactly the same dataset, we can compare the likelihoods of these models. Unconstrained models are freer and generally have more parameters, hence their likelihood is expected to be higher. On the other hand, the constrained models get more information, which they might exploit to beat the constrained ones. Within the constrained models, the likelihood is expected to increase from linear over quadratic to non-parametric response functions. However, if we estimate the environmental gradient $\alpha$ through the likelihood ratio criterion, this criterion does not necessarily maximize the likelihood or guarantee an increasing likelihood with the dimensions.

###### Detecting lack of fit

In our 2D or 3D representation, some samples and taxa may be very well represented, but others not. This may be because of a lack of fit of the negative binomial distribution _tout court_, or because its departure from independence cannot be represented in lower dimension. Anyhow, our plots will not be a truthful representation for this sample or taxon, and the end user should be warned of this.

####### Deviance

Deviance (in relation to the saturated model) would be one option. The deviance is proportional to the difference in log-likelihoods of a certain model and the saturated model. The taxon- and samplewise deviances could be calculated and plotted using a colour code. In the end all samples and taxa will be "poorly" fitted, we just need a relative measure of which ones are worst.

####### Deviance residuals

The deviance residuals $d_{ij}$ of the negative binomial distribution are defined as (see M. L. Zwilling, “Negative Binomial Regression,” The Mathematica Journal, 2013):

\[
d_{ij} = 
\begin{cases}
sgn(X_{ij}-\mu_{ij})\sqrt{2\Big(X_{ij}ln(\frac{X_{ij}}{\mu_{ij}})-(X_{ij}+\frac{1}{\phi_j})ln(\frac{1+X_{ij}\phi_j}{1+\mu_{ij}\phi_j})\Big)} \text{ if } X_{ij} > 0\\
sgn(X_{ij}-\mu_{ij})\sqrt{\frac{2}{\phi_j}ln(1+\phi_j\mu_{ij})} \text{ if } X_{ij} = 0
\end{cases}
\]

Their sum of squares equals the total deviance per sample or taxon. Plotting the deviances in function of the environmental gradient can reveal patterns and thus lack of fit to the linearity assumption.

We can do this for the taxa that respond strongest to the environmental gradient, and make residual plots with deviance or Pearson residuals. An alternative is to try to detect systematic trends through series of positive or negative residuals using Ward and Wolfowitz' runs test, and plot the taxa with the largest test statistic.

###### Identifying influential observations

Since we have explicitly expressed all score functions, we can easily identify influential observations using _influence functions_. They represent the influence a certain observation has on parameter, keeping the other sorts of parameters fixed. Because of the iterative algorithm this assumption is incorrect, maybe the influence functions cannot always be trusted. The long fitting times also exclude (leave-one-out) refitting procedures.

For the unconstrained case in a scenario without outliers the influence functions may not yield very surprising results on the level of the plot, observations mainly have influence on their own row and column scores. Coupling through the constraints is rather weak. It may however help to identify outlying abundances in case of outlying row- or column scores.

For the constrained case it may be enlightning to see which samples (and taxa) affect the estimation of the environmental gradient most.

For maximum likelihood estimation the influence function $\psi(\theta| f, x)$ of a parameter $\theta$ for a distribution $f$ and data x is defined as:

$$\psi(\theta| f, x) = -S_f(\theta|x) E\big(I(\theta, \theta|x)\big)^{-1}  $$
 with $S_f(\theta|x)$ the score function and $E\big(I(\theta, \theta|x)\big)$ the expected Fisher information matrix.
 
### One-by-one (1B1)

Initially all dimension were estimated jointly, but for reasons of speed we moved to a one-by-one approach estimating first the first dimension, then the second given the first, then the third given the first two and so on. By splitting the estimation procedures into smaller parts like this the estimation speeds up considerably, presumably because the systems are easier to solve.

### Confounders (Conditioning)

Sometimes some covariates are known to affect the abundances but one is not interested in their effect. We think first and foremost of technical parameters such as sequencing center or technology. We would like to condition on these parameters and visualize remainign variability.

#### Conditioning in correspondence analysis

Conditioning in correspondence analysis occurs by performing CCA with a confounder matrix Z as covariates, and then perform regular CA on the matrix of residuals.

#### Conditioning in the RCM approach

Thanks to our log-linear approach we can filter out the effect of these parameters by including interaction terms between the taxa and the confounding covariate to be filtered out. This willl occcur after fitting the independence model but before fitting the RC(M) component. We thus fit the following model in case of $c$ confounding variables:

 $$log(E(X_{ij})) = \lambda + \lambda_i + \lambda_j + \textcolor{red}{\sum_{l=1}^c \zeta_{jl}e_{il}} + \sum_{k=1}^M \psi_k r_{ik} s_{jk}$$
 
with $e_{il}$ the value of covariate $l$ in sample $i$ and $\zeta_{jl}$ the interaction parameter between taxon $j$ and confounding variable $l$. Note that we assume that the categorical variables have already been converted to multiple dummy variables.

#### Confounders and zero counts

Filtering out the effect of confounders brings the problems of the zeroes to the fore again. If a taxon has only zero counts in one of the subgroups defined by the confounding variables, the model will fail to fit, since the parameters $\zeta_{jl}$ would need to equal minus infinity. For continuous confounders this is of course not an issue. Maybe there exists a more efficient solution, but for now we will have to trim the taxa for which this occurs from further analysis in order to render the model identifiable. This trimming occurs already before fitting the independence model.

#### Weaknesses

The main drawback of our method is of course its strong reliance on the assumption of counts following the negative binomial distribution. The robustness to the violation of this assumption will need to be investigated. 

Also our method does not take taxon-interaction networks into account. It assumes independence between taxon counts. However, this is something we could still add to the method.

Finally also phylogenetic relation ships between taxa are not used, but this need not be a problem. Our method is a real compositional method: it reveals differences in taxon compositions between samples, and need not be influenced by "prior" information such as evolutionary relationships. After all taxa in the same body site are not guaranteed to be evolutionarily related, and vice versa distinct body sites can harbour closely related taxa.

#### Implementation

For the filtering based on the prevalence and abundance, we need a confounder matrix

 1. Without intercept (Overall filtering has happened already)
 2. With __all__ levels of the categorical variables, so set $contrasts=FALSE$ to avoid hidden reference levels of the factors
 
 When actually filtering on the confounders, we can simply use a confounder matrix
 
 1. With intercept
 2. With treatment coding
 
 In this case all that matters is to change the offset formed by the independence model. We will still return the confoundes' parameters though.

### Zero-inflated poisson

We could also augment our model with a zero-inflated Poisson rather than a negative binomial distribution. This opens additional modeling perspectives. We can model the chance on a structural zeroes:

$$logit(P(X_{ij}=0)) = f + f_i + f_j + t_{i1}v_{j1}\chi_1 + t_{i2}v_{j2}\chi_2$$.

The probability of a strucutral zero does not depend on the sample: strucutural zeroes are assumed to be truly absent species and not due to undersampling. All samples come from the same population under $H_0$, and there the structural zeroes are only dependent on the columns. Therefor we set all $f_i=0$. In practice $f$ is not estimated either, instead $f_j$ is estimated without restrictions. The terms $t_{i1}v_{j1}\chi_1 + t_{i2}v_{j2}\chi_2$ indicate in which two independent directions the observed number of zeroes deviates from this base level. I think we need the term $f_i$ because otherwise the first dimension $t_{i1}$ scores will start correcting for it. The independence moel in this case is that the chance on a structural zero only depends on the marginal probabilities. A logit model without an intercept does not make sense, it assumes a baseline probability of structural zero of 0.5.

Fitting the mean part of this models encounters severe numerical problems, this part of the project is not retained and is put in the fridge.

#### Restrictions

The same restrictions apply to the scores $t_i$ and $v_j$ as to $r_i$ and $s_j$

#### Estimation (see Lambert 1992 for details)

The mean and zero probability are modelled independently, absence of a species is independent of its abundance when it is present. Say $Z=0$ when $X$ is from the Poisson state and $Z=1$ when X is a structural zero. Evidently, when $X=0$, Z is unknown.

EM algorithm: iterate between 

 - E: estimate Z, assuming  $\mathbf{r_i}$, $\mathbf{s_j}$ and $\mathbf{\Psi}$ known, through its expectation
 - M: Maximize the log-likelihood given Z
 
Unlike the RC(2)NB model, $f_j$ has to be estimated as well, there is no obvious candidate here. We estimate it marginally and then keep it fixed during the iterations. This way the score visualize departures in zero inflation from this independence of zero-inflation between rows and columns.

Newton-Raphson would be incredibly complicated here, the derivatives of the log-likelihood functions barely fit on a page!

### Gomms

Our method bears some resemblance to the GOMMS-method proposed by Sohn and Li (2017). The aim of their method is to identify latent factors that optimally represent the microbiome count data in a low dimensional subspace. The advantage is that you do not need to model the covariance between the true underlying covariates. A number of factors $k$ is prespecified, and then the expected count is modelled as:

$$log(\mu_{ij}) = u_i + \sum_{k=1}^Kr_{ik}s_{jk} $$

whereby $r_{ik}$ represents the latent factor of sample $i$. For identifiability, they set

$$\mathbf{r}^t_k\mathbf{r}_k' = \delta_{kk'} $$

The GLM is augmented with a Zero-inflated quasi-Poisson. For numerical reasons they could not estimate taxon-wise dispersion parameters, pretty much like the numerical problems we encounterd with the ZIP and ZINB. Therefor they do with an
Still they only use the sample factors estimated this way, and do not go into making. A Hotelling's $T^2$ statistic can be used to test for significant division of clusters.

## Constrained analysis

So far we have been doing unconstrained or unsupervised explorative visualization, or _indirect_ gradient analysis. We just visualized the strongest signals in the data, independent of sample covariates. Only then did we use the covariate information to try to correlate the covariates with the latent variables formed by the row and column scores. We usually do this by adding a colour code for the samples in our biplots.

A logical next step is to develop a __constrained__ method in the same framework, i.e. with the same error distribution. We argued that PCoA with Bray-Curtis distance or regular correspondence analysis did not perform well, and we expect the same for canonical correspondence analysis since it relies on the same distributional assumptions. We will visualize those departures from the independence model that can be explained by the sample covariates. Each dimension will thus consist of a linear combination of sample covariates, the weight of each of these covariates in the dimensions reflecting how much they contribute to explaining the variation. For background information see also Zhu _et al._ , 2005, and Zhang and Thas, 2012.

We call our n-by-d covariate matrix $C$, for every sample $i$, $d$ baseline covariates are measured. These covariates can be either discrete or continuous. $c_{iy}$ is the observed value of covariate $y$ in sample $i$, $\mathbf{c_{i.}}$ the whole covariate vector of length $d$ of sample $i$, and $\mathbf{c_{.y}}$ all recorded values of variable $y$.

We could of course just perform our classical, unconstrained methods and then regress the obtained sample scores on the envrionmental variables. However, it is to be expected that this is a less efficient use of the data, which has been shown for the case of correspondence analysis by Prodon and Lebreton (1994).

### Canonical correlation analysis

Canonical correlation analysis seeks to compare multivariate sets of random variables __X__ (n-by-p) and __Y__ (n-by-m) with variables in the columns. This way one discovers which variables correlate best between both data sets. Usually both data sets represent different kinds of measurements, e.g. dietary habits vs. health condition, but all measured on the same individual. It looks for linear transformations __Xa__ and __Yb__ that maximize the correlation $\rho = cor(\mathbf{Xa}, \mathbf{Yb})$. We call these transformations the _canonical covariates_ $\mathbf{Xa_i} = \mathbf{U}_i$ and $\mathbf{Yb_i} = \mathbf{V}_i$ and the corresponding correlation $\rho_i$ the _canonical correlation_. The consecutive canonical covariates are linearly independent, the canonical correlation coefficients decrease over the dimensions of the solution.

__X__ and __Y__ are column centered to obtain __A__ and __B__, and then we look for projecting vectors __x__ and __y__ that maximize correlation between $\mathbf{z}_x = \mathbf{Ax}$ and $\mathbf{z}_y = \mathbf{Ay}$. Note that there is no notion of percentage of variance explained, and that this method is symmetric with respect to datasets __X__ and __Y__.

### Redundancy analysis (RDA)

Redundancy analysis extracts variation between variables of a n-by-p dataset __X__ that can be explained by variables from another n-by-d dataset __Y__ (column centered). It is a constrained version of principal components analysis (PCA), whereby principal components are restricted to be linear combinations of the set of constraining variables.

In a first step the rows of __X__ are fitted on those of __Y__ in multiple linear regression:

$$\widehat{X} = X (Y^tY)^{-1}Y^t X$$

Next the covariance matrix $\widehat{X}^t\widehat{X}$ is subjected to regular PCA:

$$\widehat{X}^t\widehat{X} = W \Lambda W^t$$
We plot the first columns of $\widehat{X} W \Lambda$ to represent the variability of __X__ explained by __Y__. Components of $B = (Y^tY)^{-1}Y^t X$ may be added to show which columns of __Y__ explain most variance of __X__.

#### Partial redundancy analysis

In partial redundancy analysis, the effect of a second covariate matrix Z is conditioned on prior to PCA or RDA. __X__ is regressed on __Z__, PCA or RDA proceeds on the residual matrix of this regression.

### Constrained/canonical correspondence analysis (CCA)

The idea behind canonical or constrained correspondence analysis is to have unimodal response functions as with regular CA, but to restrict the row scores to be linear functions of the environmental variables. Also, it is Gaussian ordination with the scores equally restricted under the same conditions as the unrestricted Gaussian ordination. CCA relates to CA as RDA relates to PCA.

Of course one could perform regular correspondence analysis, and then relate the obtained gradients to known envrionmental variables in a two-step approach. This is again an instance of _indirect_ gradient analysis (see Whittaker 1967).

There are many ways to introduce and explain CCA, with many parallels to other (forgotten) methods, but we will stick to the approach based on the singular value decomposition.

In contrast to redundancy analysis, constrained correspondence analysis (CCA) assumes bell-shaped, unimodal response curves with respect to environmental variables. A __response curve__ describes the expected abundance of a species in function of an environmental score, which is a function fo environmental variables. Redundancy analysis asssumes linear responses to the environmental variables, which is less useful in the ecological context. Classical ecological theory assumes species have an optimal combination of values of the environmental variables, and their expected abundances decrease as the environment departs from these optimal values.

Ter Braak (1987) proposes a method of direct gradient analysis based on CA: canonical correspondence analysis (CCA). It basically assumes that species have a Gaussian response curve in function of an environmental score which is a _linear_ conbination of environmental variables. The linear projection vector $\boldsymbol{\alpha}$ is called the __environmental gradient__. The row scores of the ordination are restricted to be linear combinations of the environmental variables. Prior to the analysis the environmental variables are centered and scaled to avoid arbitrariness in the choice of the units and render the elements of the envrionmental gradient comparable. The estimating equations are:

1) Species scores $\mathbf{s} = \mathbf{J}^{-1}\mathbf{X}^t\mathbf{r}$
2) Sample scores (weighted average taxon scores) $\mathbf{r*} = \mathbf{R}^{-1}\mathbf{Xs}$
3) Environmental gradient $\boldsymbol{\alpha} = (\mathbf{C}^t\mathbf{RC})^{-1}\mathbf{C}^t\mathbf{Rr*}$: weighted regression on environmental variables
4) Sample scores (environmental) $\lambda\mathbf{r} = \mathbf{C}\boldsymbol{\alpha}$

whereby $\lambda$ is a measure of the importance of that dimension. We can write this also as the following estimating equation:

$$(C^TXJ^{-1}X^TC - \boldsymbol{\lambda}C^TRC)\boldsymbol{\alpha} = \mathbf{0}$$

or as 

$$(C^TX(C^TRC)^{-1}X^TC - \boldsymbol{\lambda}J)\mathbf{s} = \mathbf{0}$$

The first equation represents a generalized eigenvalue problem, the second a regular eigenvalue problem since __J__ is a diagonal matrix.

Another approximation to CCA follows more the path we have before with CA. Hereby the rows of the chi-square transformed matrix $\mathbf{R^{-1/2}(X-E)C^{-1/2}}$ are subjected to weighted least squares regression on the environmental variables, with the sample totals $\mathbf{R}$ as weights. The matrix of environmental variables undergoes weighted centering prior to model fitting, with the library sizes as weights. The row-weight-centered matrix $C'$ is thus calculated as $C' = C(I - \frac{1}{c_{..}d}R\mathds{1}$. Then singular value decomposition is applied to the matrix of fitted values $F = Y (C^T R C)^{-1} C' R R^{-1/2}(X-E)C^{-1/2}$ of this regression, such that $F = U'\Sigma'V'$. Using the fitted values assures that only the systematic part explained by the covariates plays a role. 

A third approach by ter Braak in _"The history of canonical correspondence analysis"_ (2014) uses a direct SVD. Here he also references Goodman's work. The singular value approach he proposes is:

$$(Y^tRY)^{-1/2}Y^tXC^{-1/2} = U\Sigma V^t$$

The environmental gradients are then $(Y^tRY)^{-1/2}U \Sigma$, the taxon scores $C^{-1/2}V$.

CCA is related to weighted Redundancy analysis (RDA) of the Chi-squared distances, just like regular CA is related to PCoA of Chi-squared distances.

Legendre and Legendre (2012) describe an even different way of fitting the CCA. Because of all these algorithms and the use of weights, CCA is a very difficult method to interpret and compare, since it is not always clear which version is being used in which case.

#### CCA triplots

In a triplot of a CCA ordination, the samples and species are plotted as points, and the elements of the environmental gradient as arrows. The species scores are the right singular vectors, the sample scores can be calculated as linear combinations of environmental variables (environmental scores), or as the weighted average of their species scores (weighted by species abundances). In the latter case, the closer the species points lie to to the sample point, the higher the expected abundance. The orthogonal projection of the species point onto the environmental arrow represents how much the expected abundance increases (decreases) with changes in this environmental variable.

#### Conditioning in CCA

Conditioning occurs in the same fashion as with regular CA: the residuals of the CCA on the conditioning matrix Z are the input for the CCA on the environmental variables of interest.

### Redundancy analysis (RDA)

Redundancy analysis (RDA) is a constrained version of PCA, whereby the loadings are constrained to be linear combinations of a set of exploratory variables. This result can be compared to unconstrained PCA to see how much of the total variance is accounted for by the constrained variance. This type of analysis is indicated in case of linear relationships between the response variables (species counts) and explanatory variables. In our case this would e.g. be a environmental gradient in dimension $k$ that is dominated by the concentration of a toxic substance, which only decreases the expected abundance as its concentration increases. Another option is that the gradient is dominated by a variable with a short gradient.

### Constrained Analysis of Principal Coordinates

Constrained Analysis of Principal Coordinates (CAP) can be seen as a constrained version of PCoA. It is similar to RDA but extends to dissimilarity measures other than the Euclidean distance. See Anderson and Willis (2003).

### Constrained RC(M) model

The following approaches to constrained analysis of ecological data exist, or have been considered

#### The classical approach for gradient analysis

This model comes from ecology and is the one used by Zhu _et al._ (2005) and Zhang and Thas (2012 and 2016). This assumes each taxon has an optimal combination of covariates, and its expected abundance decreases as the covariates depart from this optimal combination. The expected abundance follows a Gaussian distribution in function of this departure.

The expected counts are modelled as 

$$log(E(X_{ij})) = \sum_{k=1}^K \big(a_{jk} - \frac{(h_{ik} - q_{jk})^2}{2m_{jk}^2}\big)$$

whereby 

 - $h_{ik} = \boldsymbol{\alpha}_k^t \mathbf{c}_{i.}$ the optimal linear combination of $\mathbf{c}_{i.}$ in dimension $k$, also called the _environmental score_
 - $\boldsymbol{\alpha}_k$ is called the _environmental gradient_
 - $\alpha_{ky}$ is element corresponding to covariate $y$ of the environmental gradient in dimension $k$
 - $a_{jk}$ is the maximal logged expected abundance achieved when the linear combination of covariates $\boldsymbol{\alpha}_k^t \mathbf{c}_{i.}$ equals the optimum $q_{jk}$
 - $m_{jk}$ is the tolerance, it determines how fast the expected value decreases as $\mathbf{c}_{i.}$ moves away from its optimal values $\boldsymbol{\alpha}_k^{-t}h_{ik}$
 
 The scales of $h_{ik}$, $q_{jk}$ and $m_{jk}$ are undefined, so we need an additional restriction to render the model identifiable. Zhu _et al._, 2005 propose setting $\sum_{j=1}^p m_{jk}^2/p=1$. Note that $a_{jk}$, $q_{jk}$ and $m_{jk}$ are species specific parameters whereas $\boldsymbol{\alpha}_k$ is specific for the dimension but common to all species.
 
 Does the environmental score really need to be a result of a _linear_ projection vector? From the modelling perspective perhaps not, but it does really ease the interpretation of course, which is hard enough already.
 
##### Triplot

- $q_{jk}$ is plotted as points as optimum per species
- The components of $\boldsymbol{\alpha}$ are plotted as arrows for the environmental variables.
- The samples are plotted as their site scores $\boldsymbol{\alpha}_k^t \mathbf{c}_{i.}$
 
##### Estimation

The estimation iterates between estimating $\boldsymbol{\alpha}_k$ and $\boldsymbol{\beta}_{jk} = \big(a_{jk}, q_{jk}, m_{jk})$.

Note that this model is fitted without an intercept

#### RC(M) approach to gradient analysis

#### Constrained RC(M), overparametrized version

If we want to retain analogy with the unconstrained version and model only the departure from independence, we could fit the following model:

$$log(E(X_{ij})) = u + u_i + u_j  + \sum_{k=1}^K \psi_k s_{jk} f_{jk}(\boldsymbol{\alpha}_k^t \mathbf{c}_{i.}) $$

with $f_j$ the species-specific response function, which can be linear, quadratic (Gaussian on count scale), non-parametric, ... . The usual restrictions apply to the column scores $\mathbf{s}$:

$$\mathbf{s^tu_{tax}} = \mathbf{0}$$
$$\mathbf{s_k^tUs_{k'}} = I(k = k')$$

with $u_{tax}$ the vector of $exp(u_j)$'s and  $\mathbf{U}$ a diagonal p-by-p matrix with $u_{tax}$ on the diagonal.

For $\alpha$ (a d-by-k matrix) we enforce the following restrictions:

- Normalization restricts the size, and dimensions are orthogonal (if $k \geq 3$)

$$\boldsymbol{\alpha}^t\boldsymbol{\alpha} = \mathbb{1}_d$$

with $\mathbb{1}_d$ the identity matrix of dimension $d$.

However, this model is overparametrized: $s_{jk}$ and $f_{jk}$ both model the response of a taxon $j$ in the $k$-th dimension.

### Constrained RC(M), identifiable version

A more parsimonious version of this model can be obtained by dropping the column scores.

For any response function with $q$ parameters $\boldsymbol{\beta}_j$ we model the mean as:

$$log(E(X_{ij})) = log(\mu_{ij}) = u_i + u_j + \sum_{k=1}^K \psi_k f_j \big( \boldsymbol{\alpha}_k^t \mathbf{C}_{i}|\boldsymbol{\beta}_j \big) .$$

Basically this represents a marriage of the RC(M) models of Goodman and the gradient models of Zhu and Hastie (2005). For the response function previously a quadratic function (Gaussian on count scale) has been proposed but this may not be realistic. Also the setting is different: we are looking for response function that maximize the __departure from independence__, no longer the abundances themselves! In a first stage we will try fit the linear model with $f(x) = \beta_0 + \beta_1 x$ but later we might estimate $f$ non-parametrically as in Zhu _et al._ (2005). 

Note that the concept of a "row score" is lost here: since every taxon reacts differently to changes in the environment $C_{i.}$. Still this will not yield a perfect fit since $\boldsymbol{\alpha}_k^t$ is only unique to the dimension and the number of parameters $q$ estimated per response function $f_j$ is much smaller than the number of samples $n$.

#### Shape of the response function

Note that our definition of the response function differs from the response function defined by Zhu and Hastie (2005): it no longer models mean abundance but mean departure from independence.

A linear response function may be most appropriate for problems with __short gradients__ i.e. whereby the difference in observed environmental variables is too short to devine more than an increase or decrease in departure from independence. Also in this case it is easy to interpret the effect of each of the environmental variables on the departure from independence. As so often in statistics, the linearity assumption may not be realistic, but renders models that are easy to interpret ("All models are wrong, but some models are useful"). 

For problems with __long gradients__, i.e. whereby species' departures of independence rise and drop (or drop and rise) within the score of the observed environmental scores, quadratic response functions may be more appropriate. This corresponds e.g. with the scenario whereby a species' abundance does bot depart a lot from independence for extreme values of the environmental score, and but does depart the independence model for an intermediate value of the environmental score, e.g. it thrives in this environment and its abundance is higher than under independence. In essence this is the same as the approach of Zhu and Hastie, only now the baseline is the independence model rather than 0. Every taxon thereby has its own baseline, the response function models departures from this baseline. 

Note that we usually do not choose the ranges of the environmental variables or scores, so that we cannot guarantee a range long enough for the quadratic response function to be appropriate. Note also that what is a long gradient for one species, may be a short one for another.

Even though the quadratic response function can be fitted, and/or has a "significantly" (quotation marks due to non-likelihood framework) higher likelihood than the linear response function, it may still be pointless if the maximum lies outside the range of the observed values for the environmental score. The peak location would then merely be an extrapolation, and a linear response function may be preferable. Even though the fit is worse than for the parabolic curve, it represents more truthfully the way the species reacts to the given values of the environmental gradient. Therefore we also provide a "dynamic"-option for the response function, whereby initially a quadratic model is fitted but discarded in favour of a linear one if the optimum lies outside of the range of observed environmental scores. Does this invalidate the likelihood-ratio criterion used to estimate the environmental gradient? I do not think this should be a problem. And should we use the same criterion then for the overall response function ignoring taxon labels? Probably yes, although this scenario seems highly unlikely. For plotting it is not very attractive to have different shapes of the response function for the same taxon in different dimensions.

If the user is unsure and has enough data, he may use non-parametric response funcion. This may improve the sample and covariate ordination, although the result for the species becomes harder to interpret.

 All in all we see a trade-off between quality of the ordination of the samples versus interpretability of the role of the taxa. Personally I tend to give the second factor most weight, to maximise the amount of information we visualize.

##### Relationship to constrained RC(M)

If we fit this method with only one covariate, namely a factor with a unique level for every sample, and all $\beta_{0j}$ (and $\beta_{2j}$) equal to zero , we expect to find the same solution as the unconstrained case. This means we do not constrain the ordination at all in this case and $\beta_{1jk} = s_{jk}$ and $\boldsymbol{\alpha}_{k}^t\mathbf{C}_i = r_{ik}$.

#### Restricitions

##### Restrictions on the envrionmental gradient

The $\boldsymbol{\alpha}$'s of different dimensions will be constrained as follows:

$$\boldsymbol{\alpha}_k^t\boldsymbol{\alpha}_k = \mathbf{1}$$

$$\boldsymbol{\alpha_k}^t\boldsymbol{\alpha_{k'}} = 0$$

or in shorter notation

$$\boldsymbol{\alpha}^t \boldsymbol{\alpha} = \mathds{1}$$

One restriction I intended to include but which turns out to be a __very bad idea__ is to center the $\alpha$'s:

$$\boldsymbol{\alpha}^t\mathbf{1} = \mathbf{0}$$

This would force the parameter of a single continuous covariate to be 0, the parameters of two continuous variables to be each other's opposite. In short it is not very desirable and is not implemented.

##### Restrictions on parameters of categorical variables in the environmental gradient

For continuous variables the previous discussion suffises, but for categorical variables special attention is required. In a regular, unrestricted setting one would use treatment coding, setting one of the levels of the variable to be the reference level and estimate no parameter for it. However, the above normalization and orthogonality constraints render the result dependent on the choice of reference level, which is of course highly undesirable. Also for quick understanding of the biplot it would be convenient to plot all the values of the categorical variable, and save the reader the trouble of looking up the reference value. Because of these two reasons we will need a coding scheme that includes all parameters.

Say we have $d_{cont}$ continuous covariates and $d_{cat}$ categorical variables with $d = d_{cont} + d_{cat}$, and $m_l$ levels for every categorical variable $l$. Then $\boldsymbol{\alpha}_k$ compromises

$$ q = d_{cont} + \sum_{l=1}^{d_{cat}} (m_l-1) $$
rows, or $q$ parameters to be estimated. Note that there is no intercept needed in this case, intercepts have already be included in the $f()$ function, such that samples with $\alpha_k = \mathbf{0}$ can still have departures from independence.  We can write $\boldsymbol{\alpha}_k$ then as:

$$\boldsymbol{\alpha}_k = (\beta_{cont_1}, ... , \beta_{cont_{d_{cont}}}, \beta_{cat_{11}}, \beta_{cat_{12}}, ..., \beta_{cat_{1(m_1-1)}}, ...,  \beta_{cat_{d_{cat}(m_{d_{cat}}-1)}})_k$$
, i.e. one paramater less than the number of levels. For imposing the restrictions and for plotting we want a representation whereby all covariate levels are represented. We therefore include all parameters, and impose that the parameters sum to 0 within each covariate.

$$\boldsymbol{\alpha}_k' = (\beta_{cont_1}', ... , \beta_{cont_{d_{cont}}}', \beta_{cat_{11}}', \beta_{cat_{12}}', ..., \beta_{cat_{1m_1}}', ...,  \beta_{cat_{d_{cat}m_{d_{cat}}}}')_k$$
Whereby $q' = d_{cont} + \sum_{l=1}^{d_{cat}}(m_l)$, but under the restriction that for all categorical variables $l$

$$\sum_{m=1}^{m_l} \beta_{cat_{lm}k}' = 0$$ 

We again have $g$ degrees of freedom. It is this $\boldsymbol{\alpha}_k'$ that will need to be centered, normalized and orthogonalized _in order to treat each variable level equally_ and _avoid dependence of the outcome on the choice of reference level_. Also this representation will come in handy for plotting, centering the levels of the same covariate around zero. We will call this the __zero-sum__ representation.

##### Restricitions on the parameters of the parametric response functions

The parameters of the response functions are restricted as follows (for linear and quadratic response functions):

$$\boldsymbol{\beta}_1^t\mathbf{Z}\boldsymbol{\beta}_1 = \boldsymbol{\beta}_0^t\mathbf{Z}\boldsymbol{\beta}_0 = 1$$
and for quadratic response functions:

$$\boldsymbol{\beta}_2^t\mathbf{Z}\boldsymbol{\beta}_2 = 1$$

with $\mathbf{Z}$ a diagonal matrix with the column weights $\mathbf{z}$ on hte diagonal, i.e. $Z_{ab} = \delta_{ab}z_a$. Since the parameters of the response functions are species specific, initially we thought they should undergo weighted normalization, following the same argumant as for the species scores in the unconstrained case. However, this leads to very extreme values for the parameters of the response functions for lowly abundant taxa, disfiguring the plots. That's why we decided to set all $z_a=\frac{1}{p}$.

Again, only the $\psi$ parameters can grow in size to reflect the importance of the dimension, other paramaters are normalized.

Since $\lambda_j$ already represents the offset for taxon $j$, should we fit the response functions without offsets (all $\beta_{0j} = 0$)? This means that if the environmental scores equals zero, there is no departure from independence. However, this does not give species fully the chance to react in a different way to the environmental gradient. For a certain value, some species might depart from independence while others might not. Therefore we leave $\beta_{0j}$ to be freely estimated, under the weighted variance 1 restriction.

#### Scaling

To render the values of the continuous variables in the environmental gradient comparable, it is clear that they need to be centered and scaled prior to model fitting, as in PCA. This means that their corresponding elements of $\boldsymbol{\alpha}$ represent the contribution to the environmental score of one standard deviation away from the mean of this variable (in case of linear response functions). A perfect quantitative comparison to the magnitude of the values of the paramaters of the dummies of the categorical variables will never be possible. In our case, with 0-1 dummy coding, equal parameters for a dummy and a continuous variable imply that this level of the categorical variable contributes as much to the environmental score as one standard deviation away from the overall mean of the continuous variable.

#### A note on the importance parameters

For the constrained model we have noted that the ordering of the importance parameters $\psi_k$ is not always decreasing. This is because of the way the $\alpha$'s are estimated: by maximizing the separation of the niches of the species. The first dimension is thus the one that maximally separates these niches, regardless of the total impact on the modelled mean. Therefore I think it is appropriate to keep the ordering of the dimensions as our algorithm spits it out.

#### Estimation of the constrained RC(M) model

##### Environmental gradient

###### Log-likelihood criterion

Another aspect from niche theory is that species have eveolved to occupy maximally distinct niches. $\boldsymbol{\alpha}$ could be estimated by ML as before, but in order to ensure maximum separation of the response functions of all the species we will consider a version of the log-likelihood ratio approach from Zhu _et al._ (2005):

$$LR(\boldsymbol{\alpha}) = log \frac{\prod_{i=1}^n \prod_{j=1}^p \big(p_j^{(\alpha)}(x_{ij};\boldsymbol{\alpha}^T \mathbf{c}_i,\boldsymbol{\beta}_j) \big)}{\prod_{i=1}^n \prod_{j=1}^p \big(p^{(\alpha)}(x_{ij};\boldsymbol{\alpha}^T \mathbf{c}_i,\boldsymbol{\beta}) \big)}$$

with $p^{(\alpha)}$ and $p^{(\alpha)}_j$ estimated probability density functions without and with taxon labels and $\boldsymbol{\beta}_l$ and $\boldsymbol{\beta}$ the parameters of the response functions. $p^{(\alpha)}$ and $p^{(\alpha)}_j$ are calculated under a certain error distribution (e.g. negative binomial). $LR(\boldsymbol{\alpha})$ actually compares two models. In the restricted model each species reacts in the same way to its environment, relative to its own baseline defined by $u_j$, and all do not depart from independence for a certain combination of environmental variables. In the extended model each species is left to react to the environment in its own way. In case of a linear response function it means that it gets a unique intercept and slope.

###### Full ML

However, the niche concept is not accepted by all ecologists. If niches are really maximally separated, how can species co-occur then?
An alternative option would be to estimate $\alpha$ through full ML. Surprisingly, the solutions of both approaches are very similar, although not exactly identical. The full ML solution is also much faster.

Naive approach

The naive approach to optimize $LR(\boldsymbol{\alpha})$ would be to use constrained optimization (constraining the $\boldsymbol{\alpha}$'s to have variance one etc.) and estimate $p_j^{(\boldsymbol{\alpha})}$ and $p^{(\boldsymbol{\alpha})}$ in every evaluation of this function. This implies a analytical jacobian cannot be estimated and requires a huge amount of re-estimation of the response functions. This naive approach is computationally unfeasible

Iterative approach

A smarter approach (also the one proposed by Zhu and Hastie, 2005) is to iterate between estimating $\boldsymbol{\alpha}$ by optimizing $LR(\boldsymbol{\alpha})$ given $p_j^{(\boldsymbol{\alpha})}$ and $p^{(\boldsymbol{\alpha})}$, and estimating $p_j^{(\boldsymbol{\alpha})}$ and $p^{(\boldsymbol{\alpha})}$ given $\boldsymbol{\alpha}$. This results in much fewer estimations of $p_j^{(\alpha)}$ and $p^{(\alpha)}$ and allows to optimize $LR(\boldsymbol{\alpha})$ by finding the roots of $\frac{\partial LR(\boldsymbol{\alpha})}{\partial \boldsymbol{\alpha}} = \mathbf{0}$ (and analytically specifying the second order derivative).

Convergence is assumed when every single change in $\alpha$ is below a tolerance level (infinity norm), or when the L2-norm of alpha is below a tolerance level.

Estimating $p^{(\alpha)}_k$ non-parametrically in this iterative algorithm will be computationally more demanding.

###### Lagrangian

Taking into account the restrictions mentioned above, the associated Lagrangian becomes

$$ Lag_{LR}(\boldsymbol{\alpha}_k| \boldsymbol{\beta}_{jk}, \boldsymbol{\beta}_{k}) =  \sum_{i=1}^n  \sum_{j=1}^p \Big(log \big(\frac{f_{NB}(x_{ij}|\psi_k, \mathbf{C}_i, \boldsymbol{\alpha}_k, \phi_j, f_j)}{f_{NB}(x_{ij}| \psi_k, \mathbf{C}_i, \boldsymbol{\alpha}_k, \phi_j, f_0)} \big) \Big) + \lambda_{1k} \boldsymbol{\alpha}_k^t\mathbf{M} + \lambda_{2k} (\boldsymbol{\alpha}_k^t \boldsymbol{\alpha}_k - 1) +  \boldsymbol{\alpha}_k^t \boldsymbol{\alpha}_{-k} \boldsymbol{\lambda}_{orth,k}$$

with $\boldsymbol{\alpha}_{-k}$ the $\boldsymbol{\alpha}$-matrix from the first to the (k-1)th column, and $f_{NB}$ the density function of the negative binomial distribution. $\boldsymbol{\lambda}_{orth,k}$ is a diagonal matrix with largangian multipliers for orthogonality on the diagonal.

###### Score equations

####### Linear response function

The first order derivatives becomes (in case of a linear response function, and remember we set $s_{jk} = 1$)

$$\frac{\partial Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k} = \psi_k \Big( \sum_{i=1}^n  \sum_{j=1}^p  \mathbf{C}_i \beta_{1j} \frac{x_{ij} - \mu_{ij}}{1 + \frac{\mu_{ij}}{\theta_j}} - \sum_{i=1}^n  \sum_{j=1}^p \mathbf{C}_i \beta_{1} \frac{x_{ij} - \mu_{ij0}}{1 + \frac{\mu_{ij0}}{\theta_j}} \Big) + \mathbf{M}\boldsymbol{\lambda}_{1k} + 2\lambda_{2k} \boldsymbol{\alpha}_k +  \boldsymbol{\alpha}_{-k}^t \boldsymbol{\lambda}_{orth,k} = \mathbf{0}$$

with $\mu_{ij0}$ the mean under the null model of equal response functions and $\mathbf{M}$ a q-by-(q-$d_cont$+1) centering matrix. M has ones in the first column, the next columns are all zero except for the entries corresponding to the positons of the dummies of this categorical variable in $\boldsymbol{\alpha}_k$, which are 1. As such, M forces the coefficients of each categorical variable to sum to zero.  In theory we should perhaps reestimate $\theta_j$ on every iteration between the estimation of $\boldsymbol{\alpha}_k$ and $\boldsymbol{\beta}_j$ and $\boldsymbol{\beta}$, and also estimate different overdispersion for the models with equal and species-wise response function, but we think it will complicate the estimation and not make a big difference anyway, so we ignore this for now.

####### Quadratic response function

For a quadratic response function ($f_j(z_i = \mathbf{C}_i\boldsymbol{\alpha}|\boldsymbol{\beta}) = \beta_0 + \beta_1 z_i + \beta_2 z_i^2$) this derivative becomes

$$\frac{\partial Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k} = \psi_k \Big( \sum_{i=1}^n  \sum_{j=1}^p\mathbf{C}_i (\beta_{1j}+2 \beta_{2j}\boldsymbol{\alpha}_k^t\mathbf{C}_i) \frac{x_{ij} - \mu_{ij}}{1 + \frac{\mu_{ij}}{\theta_j}} - \sum_{i=1}^n  \sum_{j=1}^p \mathbf{C}_i (\beta_{1}+2 \beta_{2}\boldsymbol{\alpha}_k^t\mathbf{C}_i) \frac{x_{ij} - \mu_{ij0}}{1 + \frac{\mu_{ij0}}{\theta_j}} \Big) + \mathbf{M}\boldsymbol{\lambda}_{1k} + \\ 2\lambda_{2k} \boldsymbol{\alpha}_k + \boldsymbol{\alpha}_{-k}^t \boldsymbol{\lambda}_{orth,k} = \mathbf{0}$$

For a non-parametric density function this derivative is not available, which which greatly increase the optimization time.

The other derivatives are (independent of the shape of the response function):

$$\frac{\partial Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\lambda_{1k}}} = \boldsymbol{\alpha}_k^t \mathbf{M} = \mathbf{0}$$
$$\frac{\partial Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \lambda_{2k}} = \boldsymbol{\alpha}_k^t \boldsymbol{\alpha}_k - 1 = 0$$
$$\frac{\partial Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\lambda_{3k}}} = \boldsymbol{\alpha}_k^t \boldsymbol{\alpha}_{-k} = \mathbf{0}$$

###### Jacobians

####### Linear response function

The second order derivatives are for the linear response function

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k^2} = - {\psi_k}^2 \Big( \sum_{i=1}^n  \sum_{j=1}^p (\mathbf{C}_i \beta_{1j})^2 \frac{\mu_{ij}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij}}{\theta_j})^2} - \sum_{i=1}^n  \sum_{j=1}^p (\mathbf{C}_i \beta_{1})^2 \frac{\mu_{ij0}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij0}}{\theta_j})^2} \Big) + 2\lambda_{2k} $$

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial {\alpha}_{yk}{\alpha}_{y'k}} = - {\psi_k}^2 \Big( \sum_{i=1}^n  \sum_{j=1}^p  \beta_{1j}^2 c_{iy} c_{iy'}\frac{\mu_{ij}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij}}{\theta_j})^2} - \sum_{i=1}^n  \sum_{j=1}^p \beta_{1}^2 c_{iy} c_{iy'} \frac{\mu_{ij0}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij0}}{\theta_j})^2} \Big) $$

####### Quadratic response function

For the quadratic response function these second order derivatives are:

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial {\alpha}_{yk}{\alpha}_{y'k}} = - {\psi_k}^2 \Big[ \sum_{i=1}^n \sum_{j=1}^p \Big((\beta_{1j} + 2\beta_{2j}\boldsymbol{\alpha}_k^t\mathbf{C}_i)(\beta_{1j} + 2\beta_{2j}\boldsymbol{\alpha}_k^t\mathbf{C}_i) c_{iy}^2\frac{\mu_{ij}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij}}{\theta_j})^2}   \\ 
- (\beta_{1} + 2\beta_{2}\boldsymbol{\alpha}_k^t\mathbf{C}_i)(\beta_{1} + 2\beta_{2}\boldsymbol{\alpha}_k^t\mathbf{C}_i) c_{iy}^2\frac{\mu_{ij}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij}}{\theta_j})^2}  \Big) \Big] - 2\psi_k \Big[\sum_{i=1}^n \sum_{j=1}^p c_{iy}^2 \big(\beta_{2j} \frac{x_{ij} - \mu_{ij}}{1 + \frac{\mu_{ij}}{\theta_j}}-\beta_{2} \frac{x_{ij} - \mu_{ij0}}{1 + \frac{\mu_{ij0}}{\theta_j}}\big)\Big] + 2\lambda_{2k} $$
$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial {\alpha}_{yk}{\alpha}_{y'k}} = - {\psi_k}^2 \Big[ \sum_{i=1}^n \sum_{j=1}^p \Big((\beta_{1j} + 2\beta_{2j}\boldsymbol{\alpha}_k^t\mathbf{C}_i)(\beta_{1j} + 2\beta_{2j}\boldsymbol{\alpha}_k^t\mathbf{C}_i) c_{iy} c_{iy'}\frac{\mu_{ij}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij}}{\theta_j})^2}   \\ 
- (\beta_{1} + 2\beta_{2}\boldsymbol{\alpha}_k^t\mathbf{C}_i)(\beta_{1} + 2\beta_{2}\boldsymbol{\alpha}_k^t\mathbf{C}_i) c_{iy} c_{iy'}\frac{\mu_{ij}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij}}{\theta_j})^2}  \Big) \Big] - 2\psi_k \Big[\sum_{i=1}^n \sum_{j=1}^p c_{iy}c_{iy'} \big(\beta_{2j} \frac{x_{ij} - \mu_{ij}}{1 + \frac{\mu_{ij}}{\theta_j}}-\beta_{2} \frac{x_{ij} - \mu_{ij0}}{1 + \frac{\mu_{ij0}}{\theta_j}}\big)\Big] $$

<!---+ \\ 2\beta_{2j} c_{iy} c_{iy'}\frac{x_{ij} - \mu_{ij}}{1 + \frac{\mu_{ij}}{\theta_j}}--->

The other derivatives are again independent of the response function:

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k \partial \boldsymbol{\lambda_{1k}}} =  \mathbf{M}$$

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_k \partial \lambda_{2k}} =  2\boldsymbol{\alpha}_k$$

$$\frac{\partial^2 Lag_{LR}(\boldsymbol{\alpha}_k)}{\partial \boldsymbol{\alpha}_{k} \partial \boldsymbol{\lambda}_{orth,k}} =  \boldsymbol{\alpha}_{-k}$$
All other second order derivatives equal zero. 

###### Non-parametric response function

The response function $f(\boldsymbol{\alpha}^t\mathbf{C})$ need not have a parametric form. We can also estimate it non-parametrically as in Zhu and Hastie (2005). These authors use the locfit library, which does not result in a ML solution.

Instead we use cubic smoothing splines in a generalized additive model (GAM) to obtain flexible response functions without yielding to a perfect fit. Cubic splines are piecewise polynomials in certain intervals that are continuous and have continuous first and second order derivatives. The knots that define the intervals are normally simply the observations x, although this may differ in the softwares when there are a large number of points. This makes them look very smooth. Their shape is a trade-off between goodness of fit and cumulative value of the second order derivative on the whole interval. The cubic smoothing spline minimizes

$$\sum_{i=1}^n(y_i - \hat{f}(x_i))^2 + \lambda \int \hat{f}''(x)^2 dx $$

This second term can be seen as a penalty for bumpiness of the fit (with smoothing parameter $\lambda$), and ensures smoothness.

The constrained model then looks like:

$$log(E(X_{ij})) = u_i + u_j + \sum_{k=1}^K\psi_k\hat{f}_j(\boldsymbol{\alpha}_k^tC_i)$$

In practice we fit the following model

$$log(E(X_{ij})) = u_i + u_j + \sum_{k=1}^K\beta_{jk}\hat{f}_j(\boldsymbol{\alpha}_k^tC_i)$$

and calculate the $\psi_k$'s afterwards as $\psi_k = \sqrt{\sum_{j=1}^p z_j \beta_{jk}^2}$, so that only the importance parameter $\psi$ can differ in size between the different dimensions.

##### Estimation of the parameters of the parametric response functions

We fit all parameters of the different taxa jointly, and enforce normalization through Lagrange multipliers again.

The derivative of the Lagrangian for $\beta_{jv}$ under the negative binomial model with $v$ a parameter index running along $\boldsymbol{\beta}_j$ (for k=1 and omitting dimension indices)

$$ \sum_{i=1}^n \frac{\partial Lag_{NB}(X_{.j}|\boldsymbol{\beta}_j, \theta_j)}{\partial \beta_{jv}} = 2\beta_{jv}\lambda_{2v} +  \sum_{i=1}^n \psi \frac{\partial f_j \big( \boldsymbol{\alpha}^t \mathbf{C}_{i.}|\boldsymbol{\beta}_j \big)}{\partial \beta_{jv}} \frac{x_{ij}-\mu_{ij}}{1+\frac{\mu_{ij}}{\theta_j}} = 0$$

The second order derivative for parameters from the same taxon is:

$$\sum_{i=1}^n \frac{\partial^2 Lag_{NB}(X_{.j}|\boldsymbol{\beta}_j, \theta_j)}{\partial \beta_{jv} \partial \beta_{jv'}} = -\psi^2 \sum_{i=1}^n \Big( \frac{\mu_{ij}(1+ \frac{x_{ij}}{\theta_j})}{(1 + \frac{\mu_{ij}}{\theta_j})^2} \frac{\partial f_j \big( \boldsymbol{\alpha}^t \mathbf{C}_{i.}|\boldsymbol{\beta}_j \big)}{\partial \beta_{jv}} \frac{\partial f_j \big( \boldsymbol{\alpha}^t \mathbf{C}_{i.}|\boldsymbol{\beta}_j \big)}{\partial \beta_{jv'}} \Big) + 2\lambda_{2v} \mathds{I}(v=v')$$

$$\sum_{i=1}^n \frac{\partial^2 Lag_{NB}(X|\boldsymbol{\beta}_j, \theta_j)}{\partial \beta_{jv} \partial \lambda_{2v'}} = 2\beta_{2v} \mathds{I}(v=v')$$

all other second order derivatives equal zero.

#### Algorithm

The whole estimation of the constrained RC(M)-model would then proceed as follows:

 1. Estimate the independence model ($u_i$'s, and $u_j$'s)
 2. (Optional): Filter out confounders by modifying offset
 3. Find starting values for $\alpha$, based on CCA
 4. Iterate between
  - Estimate $f_j$'s and $f_0$ given $\boldsymbol{\alpha}$
  - Find $\boldsymbol{\alpha}$ by optimizing $LR(\boldsymbol{\alpha})$
 5. Estimate overdispersions using empirical Bayes
 6. Estimate importance parameter $\psi_k$
 7. Repeat steps 4-6 for every dimension $k$ until convergence

Filtering of rare taxa is not needed in this case, as all covariates are summarized in one "continuous" row score. to check for convergencen, for $\psi_k$  we apply an infinity norm, for $\boldsymbol{\alpha}_k$ the default is an L2-norm.

#### Triplot

The interpretation of the triplot is different from the one from Zhu _et al._ (2005), and also different from that of the unconstrained RC(M).

 - The components of $\boldsymbol{\alpha}_k$ are plotted as arrows and represent the extent to which the covariates contribute to environmental score in dimension $k$. Categorical variables are centered, all their levels are shown. The continuous variables have been centered and scaled, so the magnitude of their compponents in $\boldsymbol{\alpha}_k$ can be compared to one another.
 - For the samples, the environmental scores are plotted as points and represent environmental scores of each sample. Samples close together in one dimension come from similar environments, considering that environmental gradient. This does _NOT_ mean that they have similar values of the environmental variables. 
 - The plot for the taxa depends on the response function used. 
    * In case of a linear response function we can plot arrows originating from $-\frac{\beta_{0j}}{\beta_{1j}}$, which represents the point of no departure from independence explained by the covariates. The direction and size of the arrow then depends purely on $\beta_{1j}\psi$. We can make two sorts of projections:
      - The projection of ($-\frac{\beta_{0j}}{\beta_{1j}}$, $-\frac{\beta_{0j}}{\beta_{1j}} + \beta_{1j}\psi$) onto $\boldsymbol{\alpha}^t\mathbf{C}_i$ (the environmental scores), then corresponds to the departure from independence for taxon $j$ in sample $i$ explained by its environmental variables $\mathbf{C}_i$. The arrow is appropriate since a larger environmental score always results in a larger departure from indepedence under this model. As often in statistics, linearity may not be a very credible assumption, but it leads to models that are easy to interpret.
      - The projection of ($-\frac{\beta_{0j}}{\beta_{1j}}$, $-\frac{\beta_{0j}}{\beta_{1j}} + \beta_{1j}\psi$) onto $(0,\alpha_y)$ reflects the sensitivity of taxon $j$ to changes in covariate $y$.
      We cannot make a direct connection between components of the environmental gradient $\boldsymbol{\alpha}$ and the observed environmental scores $\boldsymbol{\alpha}^t\mathbf{C}_i$, even though this may seem so intuitively. Often it is so that a large $\alpha_y$ points in the direction of samples with a large $c_{iy}$, but this is not guaranteed. Many different combinations of covariates $\mathbf{C}_{i}$ can lead to the same environmental score for a given $\boldsymbol{\alpha}$. This is because of the dimension reduction from $d$ to 1 resulting from the projection by $\boldsymbol{\alpha}$. We will have to warn the user for this pitfall, and facilitate the depiction of the environmental variables of each samples through colour codes.
    * In case of quadratic response function we should plot the extremum of the response functions in each dimension as a point (taking the importance parameter $\psi_k$ into account, which is multiplied by each of the parameters $\beta$ of the response functions). If this is a minimum, this represents the value of the environmental score with the largest drop in expected abundance compared to the independence model, in case of a maximum the largest rise. With colour and shape codes we can show which kind of extremum it is, and which kind of departure from indepedence (negative or positive effect on the expectation). To represent the tolerances, i.e. how steep these valleys and mountains of expected abundance are, we can use ellipses to designate e.g. points with 50% the departure from independence from the top. For CCA this feature is not needed since there all tolerances are assumed equal.
    * In case of non-parametric response function I do not see I clear way to plot the species. The benefit of this method will have to be the improved ordination of the environmental gradient and the samples. One can always look at the individual species response functions if one is interested.
  
It is clear that the triplot is not trivial to interpret, but then again: neither is a triplot based on CCA.

## Some general thoughts on the RC(M)

Contrary to most other methods to make biplots for microbiome data, the RC(M)-method is a truly statistical method that explicitly models the mean. This renders it very easy to interpret. It's main weaknesses are the reliance on a parametric distribution (which may be incorrect), and it's relatively long running times. However, many other biplot methods, such as CA ot PCoA with Hellinger distances, also make distributional assumptions, mostly without stating them explicitly.

\newpage

#### Test the software

Some preliminary test to make sure everything runs without errors

```{r tests, purl = FALSE, eval = FALSE}
n = 2000; p = 10000
counts = matrix(rnbinom(n*p, mu = 5, size = 3), n, p)
covars = data.frame(age = rnorm(n, 35,5), gender = sample(c(TRUE, FALSE), n , replace = TRUE), country = factor(sample(c("France","Belgium","Germany"),n, replace=TRUE)), BMI = rnorm(n , 27,6), status = sample(c("Treatment","Control"),n, replace=TRUE), siblings = rpois(n, lambda = 0.9))
physeq = phyloseq(otu_table(counts, taxa_are_rows = FALSE), sample_data(covars))
confounderVec = get_variable(physeq, "country")
d = 30
v = 2
X = matrix(rpois(n*p,2), n, p)
thetaMat = abs( matrix(rnorm(p), n, p, byrow = TRUE))
psi = 3
muMarg = outer(rowSums(X), colSums(X))/sum(X)
NB_params = betas = matrix(rnorm(p*v),v,p)
reg = model.matrix(~rnorm(n))

#Unconstrained one dimension matrix
un1Dmat = RCM(counts, k=1)
# Add two dimensions
un3Dmat = RCM(dat = counts, prevFit = un1Dmat, k=2)

#Unconstrained one dimension phyloseq
un1DmatPhy = RCM(physeq, k=1)
# Add two dimensions (even accepts a matrix!)
un3DmatPhy = RCM(dat = counts, prevFit = un1DmatPhy, k=3)

#Unconstrained one dimension matrix with confounder filtering
un1DmatConf = RCM(counts, k=1, confounders = data.frame(confounderVec))
# Add two dimensions
un3DmatConf = RCM(dat = counts, prevFit = un1DmatConf, k=3)

#Check some requirements
str(un3DmatConf$confParams)
colSums(un3Dmat$rMat^2)
crossprod(un3Dmat$rMat)
colSums(t(un3Dmat$cMat) * un3Dmat$abunds)
rowSums(un3Dmat$cMat^2)
sum(un3Dmat$cMat[1,]* un3Dmat$cMat[2,] * un3Dmat$abunds)
# All is well!

#Unconstrained one dimension physeq with confounder filtering
un1DmatConfPhy = RCM(physeq, k=1, confounders = "country")
# Add two dimensions
un3DmatConfPhy = RCM(dat = counts, prevFit = un1DmatConf, k=3)

#Constrained one dimension matrix without confounder filtering
con1Dmat = RCM(physeq, k=1, covariates =  sample_variables(physeq))
con1DmatML = RCM(dat = physeq, k=1, covariates =  sample_variables(physeq), envGradEst = "ML")
# Add two dimensions
con3Dmat = RCM(dat = physeq, prevFit = con1Dmat, k=3, covariates =  sample_variables(physeq))


con2DmatLin = RCM(physeq, k = 2, covariates =  sample_variables(physeq),  responseFun = "linear")

con2DmatDyn = RCM(physeq, k = 2, covariates =  sample_variables(physeq),  responseFun = "dynamic")
apply(con2DmatDyn$NB_params,3, function(x){mean(x[3,]==0)}) #19 and 16% of the taxa prefer linear response functions
con2DmatDyn$NB_params_noLab

con2DmatQuad = RCM(physeq, k = 2, covariates =  sample_variables(physeq),  responseFun = "quadratic")
# The zeller data?
con2DmatNon = RCM(physeq, k = 2, covariates =  sample_variables(physeq),  responseFun = "nonparametric")

plot.RCM(con2DmatLin, colour = "BMI")
plot.RCM(con2DmatQuad, colour = "BMI")
plot.RCM(con2DmatNon, colour = "BMI")

con2DmatLin$alpha
con2DmatQuad$alpha
con2DmatNon$alpha

#Comparsion with Julia
relAbs = 10^runif(p,-6,-2);
relAbs = relAbs/sum(relAbs);
thetas = abs(rnorm(p,3,1));
libSizes = 10^runif(n,3,5);
thetaMat = matrix(thetas, n, p, byrow =TRUE)
X = matrix(rnbinom(n*p, mu = outer(libSizes, relAbs), size = thetaMat), n, p)
system.time(dNBlibSizes(log(libSizes), X, log(relAbs),thetas))
```

# Competing methods

## Unconstrained RC(M)

### PCoA/MDS

Principal coordinates analysis (PCoA) or multidimensional scaling (MDS) are dimension reduction techniques specifically for samples. The consist in calculating dissimilarity matrices with dissimilarities between samples based on some chosen dissimilarity measure. These dissimilarity matrices are than decomposed through eigenvalue decomposition, and the first two dimensions are plotted. More formally, for a n-by-p data matrix $\mathbf{X}$, all pairwise distances between samples $i$ and $i'$ $D_{ii'} = d(\mathbf{X_{i.}}, \mathbf{X_{i'.}})$ are calculated and summarized in a n-by-n dissimilarity matrix $\mathbf{D}$. If the dissimilarity measure is a true distance, then $d_{ii'} = d_{i'i}$ and $\mathbf{D}$ is symmetric. Next $\mathbf{D}$ is squared and undergoes double centering:

$$B = -1/2 J D^2 J$$

with $\mathbf{J} = \mathbb{I} - \frac{1}{n}\mathbf{11'}$ a centering matrix. This transformed matrix __B__ then undergoes eigendecomposition into

$$ \mathbf{B} = \mathbf{V}^t\boldsymbol{\Lambda}\mathbf{V} $$

  with the eigenvectors of $\mathbf{D}$ in the columns of $\mathbf{V}$ and $\boldsymbol{\Lambda}$ a diagonal matrix with eigenvalues of $\mathbf{D}$ on the diagonal. $\mathbf{v}_1\lambda_1$ can then be plotted _vs_ $\mathbf{v}_2\lambda_2$ in a 2D scatterplot to represent distances between the samples optimally in 2D. In the calculation of $\mathbf{D}$, the information on the individual taxa is lost. Taxa scores can be calculated by weighted averaging of the sample scores for each taxon, weighted by the taxon abundances in these samples. The taxon score of taxon _j_ in dimension 1 then becomes
   
   $$t_{j1} = \sum_{i=1}^n v_{i1}*\frac{x_{ij}}{x_{.j}}$$
   
  This is a two step approach: the taxon scores are only calculated post-hoc and not as part of the eigendecomposition.
   
  In the following paragraphs we discuss a number of popular distance measures for microbiome data that are commonly used for PCoA.
   
#### Bray-Curtis dissimilarity

This distance measure was invented in 1957 in the context of forest ecology. It is calculated as

$$BC_{12} = 1-\frac{2*S_{12}}{S_1 + S_2}$$

with $S_{12}$ the counts or relative abundances of all species observed in \textbf{both samples}, and $S_{i}$ the counts or relative abundances of species only observed in sample $i$. This dissimilarity measure varies between 1 when no species are shared (maximal dissimilarity), and 0 when all species shared, and the community composition is identical. It is not a distance measure since it does not fulfill the triangle inequality $d(a,b) \leq d(a,c) + d(b,c)$. For use in microbiome research see e.g. Barfod _et al._, 2017 and many others. You can apply it to eihter absolute or relative counts, although the latter appears to be the most popular. In Callahan _et al._ (2016) they also apply it to log-transformed counts, even though they note that this leads to correlations with the library sizes.

#### UniFrac distance

The UniFrac distance was developed specifically for the microbiome by Lozupone and coworkers (2005), and includes phylogenetic information in the distance calculation. It is defined as the fraction of the phylogenetic tree length that leads to species found in only one of the two samples. As opposed to Bray-Curtis dissimilarity, it is a true distance metric.

##### Weighted UniFrac

To account for differences in sequencing depth, in 2007 weighted UniFrac was proposed. This weights every branch by the absolute difference in relative abundances the species of both samples as follows:

$$d_{UniFrac}(1,2) = \sum_{i=1}^b l_i |\frac{x_{1b}}{x_{1.}} - \frac{x_{2b}}{x_{2.}}|$$
with $x_{zb}$ the total abundance of species in sample _z_ pertaining to branch _b_. This means that also branches with species from both samples can contribute to the distance between the samples, if their compositions are uneven.

#### Jenssen-Shannon distance
   
We include the Jenssen-Shannon distance because of its role in the enterotypes publication (Arumugam _et al._, 2011). It is the square root of the Jenssen-Shannon _divergence_, a symmetrized version of the Kullback-Leibler divergence.

$$d_{Jenssen-Shannon}(a,b) = \sqrt{0.5 \sum_{j=1}^p x_{aj} log \frac{x_{aj}}{\frac{x_{aj}+x_{bj}}{2}} + 0.5 \sum_{j=1}^p x_{bj} log \frac{x_{bj}}{\frac{x_{aj}+x_{bj}}{2}}}$$

It is a true distance.

#### Hellinger distance

This came up during our own research into a version of correspondence analysis that would accurately model the variance of our count data. Still, it was originally designed for distances between distributions rather than between real data ratios.

The Hellinger distance divides by the square root of the observation as means of normalization:

$$\frac{X}{\sqrt{Var(X)}} \approx \frac{X}{\sqrt{X}} = \sqrt{X}$$

The rationale is that each observation x (rather than the margins of the table) is the best estimate of its expectation E(X), and under the Poisson model also of its variance. Still also these plots are susceptible to differences in library sizes. See Rao (1997), who suggests this method as an alternative to correspondence analysis. The drawback of correspondence analysis is that the estimate for the expected abundances used to standardize departures from independence $X-E$ is based on the taxon sums, and thus on other samples. With the Hellinger distance only, the sample itself is used to estimate the expected abundances, namely by the count itself. It is defined as 

$$d_{hellinger}^2(a,b) = \sum_{j=1}^p \Big(\sqrt{\frac{x_{aj}}{x_{a.}}}-\sqrt{\frac{x_{bj}}{x_{b.}}}\Big)^2 $$

. It does works on the relative abundances. It has been used in publications on microbiome, e.g. Kreisinger _et al._, 2015, Hollister _et al._ 2015, Vital _et al._, 2015.

For a biplot we find the singular value decomposition

$$R^{1/2}(P-Q) = STV^t$$

with $p_{ij} = \sqrt{\frac{x_{ij}}{x_{i.}}}$ and $q_{ij} = \sqrt{\frac{x_{.j}}{x_{..}}}$. To represent the samples we plot the first $k$ columns of $TR^{1/2}S$, and to add the taxa we can add the first $k$ columns of $\Delta^{-1}V$ with $\delta_{ij} = \Big(\sqrt{(P-Q)^tR(P-Q)}\Big)_{ij}$ if $i=j$ and 0 otherwise.

#### Transformations

##### Log transform

In Callahan _et al._ (2016), which serves as a manual for the popular _phyloseq_ package, it is suggested to first log-transform the counts(after adding pseudocount 1) as a variance stabilizing transformation, prior to applying methods like PCoA

##### Variance stabilizng transformation

The _DESeq_ packages provides a variance stabilizing transformation (VST), based on the estimated mean-variance trend. The point is to get transformed values whose variance is independent of the mean.

##### Regularized log-transform

Apart from the VST, the _DESeq2_ packages also offers the _rlog_ transformation. It results in a $log_2$ transformed expected value of the count, based on an intercept and a sample-wise coefficient that is shrunk based on the mean variance trend. This transformation should account for differences in library sizes. See _DESeq2_ manual.

#### Related material

   * See Warton _et al._ (2012): Distance-based multivariate analyses confound
location and dispersion effects
   * See technical report bio BioRankings: "Dealing with High-Dimensional Data". Pairwise distances all become similar in high dimensions.
   
#### Non-metric multidimensional scaling

Non-metric multidimensional scaling (NMDS) can be considered the little brother of MDS, since it also represents distance matrices in lower dimensions. Still there are some important differences: 

 - NMDS is fitted numerically instead of through singular value decomposition
 - The number of axes is chosen beforehand. There is no ordering in the axes, the solution depends on the number of axes chosen
 - It makes no assumption on the distribution of the data, in contrast to PCA which assumes linear relationships and CCA which assumes unimodal ones.
 
 The ordination is fitted iteratively, by minimizing the stress between the original distances and the distances of the actual ordination. See the tutorial by Steven M. Holland for more information.
 
### Double principal coordinates analysis

Double principal coordinates analysis (DPCoA) intends to include dissimilarities between species into the analysis. These could be morphological differences, but in the microbiome case these will be phylogentic distances. 

We have given a n-by-p data matrix __X__ and a p-by-p symmetric dissimilarity matrix between the species $\Sigma$.

1) Use PCoA to find the optimal _n-1_th dimensional representation of the the dissimilarity matrix $\Sigma$
2) Add the samples points at the barycentres (weighted averages) of the species points. In this representations Euclidean distances between samples on the plots equal the square roots of their Rao dissimilarity.
3) Use PCA to find a lower dimensional representation of the samples.

It is similar to Unifrac in the sense that it uses phylogenetic information. It is also reminiscent of PCoA, only that here distances between _species_ rather than samples are used. Also the dimension reduction is more complex.

### PCA

Principal components analysis (PCA) is perhaps the most widely used of all dimension reduction methods. Out os a number of possibly correlated continuous variables, it creates linear combinations of these variables that are orthogonal to each other, and for which each linear combinations has maximal variance, conditional on the previous ones.

PCA is performed as an eigenvalue decomposition of the variance covariance matrix

$$Y^tY = W \Lambda W^t$$

with $W^TW = I$. The variables (columns) of __Y__ are presumed to be centered and scaled. This dimensionality reduction optimally represents Euclidean distances between samples in lower dimensions. Its use with dummies of categorical variables is problematic, since they are inherently correlated and affect the ordination of the PCA. This analysis seems usefull for explorative visualization of continuous covariate data, rather than counts.

#### PCA on ranks

In the phyloseq manual it is proposed to perform PCA on ranked abundance data. Thereby low ranks are set equal to 1 to avoid creating artificially large differences between lowly abundant taxa.

### Correspondence analysis

This is the classical, statistical way of analysing contingency tables of count data. For discussion see the beginning of this document.

### CoDa

In recent years there have been many publications on analysing microbiome data as compositional. Gloor and Reid (2016) and Gloor _et al._ propose a method to extend this philosophy to biplots. In a frist step, zero counts are replaced by a non-zero pseudocount, either 0.5 or through a Bayesian method. Next the data are normalized to realtive abundances, divided by the geometric mean and log-transformed. This is called the central logratio transform (clr). PCA is applied to these transformed data (through eigendecomposition of the variance-covariance matrix) and sample scores mutliplied by eigenvalues are plotted. Row scores can be added as weighted sample means.

Theoretical concerns I have about this method are the following:

 - Heteroscedasticity of count data is not addressed. Authors claim this should be handled "downstream".
 - Negative correlations due to compositionality might be swamped by true biological correlations
 - The library sizes are not always of merely technical nature, and very often measures of biomass or cell count are including. This questions the compositional nature of these data

### Bayesian Nonparametric Ordination 

(Ren _et al._ (2016))

# Method evaluation

After implementing the method and proving its concept, it is time to compare it with existing methodologies. We need to define a way to obtain datasets, and a way to evaluate the ordinations in a high-throughput way. For the simulation we will need multiple repetitions to prove the superiority of our method, so we need an automatic evaluation of the clustering accuracy.

## Overall vs. within distance

A first intuitive approach would be to compare the within cluster distances on the plot to the distances to the origin. Clusters are defined as samples generated with the same taxon distribution, either parametrically or non-parametrically. Say we have $m = 1,...,l$ groups with (multidimensional) means $\mathbf{M}_m$.  The within distance is then defined as $d_{m, within} = \frac{1}{n_m}\sum_{i=1}^{n_m} d_{euclidean}(\mathbf{M}_m, Y_i)$, with $d_{euclidean}$ the euclidean distance. The overall distance of a group then equals $d_{m, overall} = \frac{1}{n_m}\sum_{i=1}^{n_m} d_{euclidean}(\mathbf{0}, Y_i)$ with $\mathbf{0}$ the origin. This calculation can be done for each dimension separately or in the multidimensional space.

We could perhaps look for the distance based method that maximizes $\frac{d_{m, overall}}{d_{m, within}}$? This is related to Ward's minimum variance method, but with scaling to the overall distance. This takes differences in the scales of solutions of different methods into account. We then use the geometric mean of these ratios over the different clusters as statistic:

$$\Big(\prod_{m=1}^l\frac{d_{m, overall}}{d_{m, within}}\Big)^{1/l}$$

This approach can only be applied to the row scores (samples), since there the importance parameters are multiplied and thus distances are represented correctly. This approach is reminiscent of Ward clustering \cite{Ward1963} and a pseudo F-statistic \cite{Anderson2001}.

## Silhouette

See "Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis", Rousseeuw, 1987. The silhouette can be calculated with any distance measure, but we will just use the euclidean distance again. For each point $i$, calculate the distance to each other point in the ordination, and average these distances within the cluster. Call $a(i)$ the average distance to its own cluster and $b(i)$ the smallest of the average distances to the other cluster. The the silhouette of observation i $s(i)$ is defined as:

$$s(i) = \frac{b(i) - a(i)}{max\big(a(i), b(i)\big)}$$

The silhouette can take values between +1 for optimal separation and -1 for wrong classification. If a poin i close to the centroid of its own cluster but very far from all the others the methods has performed well and is rewarded with a high silhouette. We can average the silhouttes over the ordination or jus look at their distribution to compare the ordination methods.

## Correlation with library sizes and abundances

The motivating problem to develop the whole method was to find an approach that would not show correlation between the row scores and the library sizes, which is a technical artefact.

Pearson correlations of row scores with library sizes could be another criterion to evaluate the quality of the biplot. Equivalently correlations with average relative abundance can be investigated.

## Contribution of taxa to the separation of the clusters

For the methods that do yield taxon scores, we can also verify if the correct taxa contribute to the separation of the clusters. A useful measure in this case seems the relative contribution of the taxa with signal in the direction of the cluster centroid, compared to the non-signal taxa. 

With $\mathbf{H}_l$ the vector from the origin to the sample centroid of group $l$, $\mathbf{s}_{l,sig}$ the $n_{l,sig}$-by-k matrix with scores of the $n_{l,sig}$ taxa in group l with signal and $\mathbf{s}_{l,noSig}$ the $n_{l,noSig}$-by-k matrix with scores of the $n_{l,noSig}$ taxa without signal, the quantity we are looking for is:

$$ \frac{n_{l,noSig} \mathbf{s}_{l,sig}\mathbf{H}_l \mathbb{1}}{n_{l,sig} \mathbf{s}_{l,noSig}\mathbf{H}_l \mathbb{1}}$$

with $\mathbb{1}$ a column matrix of one of the appropriate dimension. However, this will very often lead to negative and/or very large ratios. Therefore we look at the average ratio of the median signal of the signal taxa to the 75th percentile of the signal of the non-signal taxa. The rationale is that half of the non-signal taxa are expected to contribute in the direction of the cluster, and taking the median of this half is in practice the 75th percentile. The final statistic is thus


$$\frac{1}{m}\sum_{l=1}^m \frac{\underset{j}{\mathrm{median}} (\mathbf{s}_{lj,sig}\mathbf{H}_l )}{\underset{j}{\hat{F}^{-1}_{0.75}}(\mathbf{s}_{lj,noSig}\mathbf{H}_l )}$$
